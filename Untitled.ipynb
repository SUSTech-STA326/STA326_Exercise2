{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95e855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84054063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a request header (to prevent anti-scraping)\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0a0f1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2403.00762', '2403.00758', '2403.00752', '2403.00745', '2403.00743', '2403.00742', '2403.00737', '2403.00729', '2403.00725', '2403.00724', '2403.00720', '2403.00717', '2403.00715', '2403.00712', '2403.00704', '2403.00696', '2403.00691', '2403.00690', '2403.00689', '2403.00686', '2403.00685', '2403.00682', '2403.00680', '2403.00675', '2403.00674', '2403.00673', '2403.00669', '2403.00668', '2403.00665', '2403.00663', '2403.00662', '2403.00646', '2403.00645', '2403.00644', '2403.00643', '2403.00642', '2403.00641', '2403.00633', '2403.00632', '2403.00631', '2403.00628', '2403.00625', '2403.00623', '2403.00622', '2403.00621', '2403.00613', '2403.00611', '2403.00607', '2403.00606', '2403.00598', '2403.00594', '2403.00592', '2403.00591', '2403.00590', '2403.00587', '2403.00586', '2403.00585', '2403.00584', '2403.00582', '2403.00579', '2403.00578', '2403.00574', '2403.00573', '2403.00571', '2403.00570', '2403.00567', '2403.00566', '2403.00565', '2403.00564', '2403.00563', '2403.00561', '2403.00558', '2403.00556', '2403.00554', '2403.00553', '2403.00550', '2403.00546', '2403.00543', '2403.00542', '2403.00540', '2403.00539', '2403.00536', '2403.00529', '2403.00528', '2403.00527', '2403.00526', '2403.00522', '2403.00520', '2403.00517', '2403.00515', '2403.00514', '2403.00510', '2403.00509', '2403.00506', '2403.00504', '2403.00499', '2403.00497', '2403.00491', '2403.00489', '2403.00486'] 100\n"
     ]
    }
   ],
   "source": [
    "url = 'https://arxiv.org/list/cs/pastweek?skip=0&show=100'\n",
    "# 发送请求获取网页内容\n",
    "response = requests.get(url, headers = headers, timeout=180)\n",
    "\n",
    "web_content = response.text\n",
    "\n",
    "# 使用BeautifulSoup解析网页内容\n",
    "soup = BeautifulSoup(web_content, 'html.parser')\n",
    "    \n",
    "# 创建一个名为abstract_address的list储存abstract的具体网址\n",
    "abstract_address_list = []\n",
    "\n",
    "all_abstract_address = soup.find_all(\"a\", attrs={\"title\": \"Abstract\"})\n",
    "    \n",
    "for address in all_abstract_address:\n",
    "    address = address.text\n",
    "          \n",
    "    match = re.search(r'(\\d+\\.\\d+)', address)\n",
    "    if match:\n",
    "        abstract_address_list.append(match.group(1))\n",
    "            \n",
    "print(abstract_address_list, len(abstract_address_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ff461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok 1\n",
    "abstracts_list = []\n",
    "for address in abstract_address_list:\n",
    "    print(address)\n",
    "    url = f\"https://arxiv.org/abs/{address}\"\n",
    "    # response = requests.get(url, headers = headers, timeout=600)\n",
    "    response = requests.get(url, headers = headers, timeout=600)\n",
    "    web_content = response.text\n",
    "    # 使用BeautifulSoup解析网页内容\n",
    "    soup = BeautifulSoup(web_content, 'html.parser')\n",
    "    abstracts = soup.find_all(\"blockquote\", attrs={\"class\":\"abstract mathjax\"})\n",
    "    for abstract in abstracts:\n",
    "        abstract = abstract.text\n",
    "        abstract = abstract.replace('Abstract:', '').strip()\n",
    "        abstracts_list.append(abstract)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "print(abstracts_list, len(abstracts_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f088776f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2403.00762\n",
      "2403.00758\n",
      "2403.00752\n",
      "2403.00745\n",
      "2403.00743\n",
      "2403.00742\n",
      "2403.00737\n",
      "2403.00729\n",
      "2403.00725\n",
      "2403.00724\n",
      "2403.00720\n",
      "2403.00717\n",
      "2403.00715\n",
      "2403.00712\n",
      "2403.00704\n",
      "2403.00696\n",
      "2403.00691\n",
      "2403.00690\n",
      "2403.00689\n",
      "2403.00686\n",
      "2403.00685\n",
      "2403.00682\n",
      "2403.00680\n",
      "2403.00675\n",
      "2403.00674\n",
      "2403.00673\n",
      "2403.00669\n",
      "2403.00668\n",
      "2403.00665\n",
      "2403.00663\n",
      "2403.00662\n",
      "2403.00646\n",
      "2403.00645\n",
      "2403.00644\n",
      "2403.00643\n",
      "2403.00642\n",
      "2403.00641\n",
      "2403.00633\n",
      "2403.00632\n",
      "2403.00631\n",
      "2403.00628\n",
      "2403.00625\n",
      "2403.00623\n",
      "2403.00622\n",
      "2403.00621\n",
      "2403.00613\n",
      "2403.00611\n",
      "2403.00607\n",
      "2403.00606\n",
      "2403.00598\n",
      "2403.00594\n",
      "2403.00592\n",
      "2403.00591\n",
      "2403.00590\n",
      "2403.00587\n",
      "2403.00586\n",
      "2403.00585\n",
      "2403.00584\n",
      "2403.00582\n",
      "2403.00579\n",
      "2403.00578\n",
      "2403.00574\n",
      "2403.00573\n",
      "2403.00571\n",
      "2403.00570\n",
      "2403.00567\n",
      "2403.00566\n",
      "2403.00565\n",
      "2403.00564\n",
      "2403.00563\n",
      "2403.00561\n",
      "2403.00558\n",
      "2403.00556\n",
      "2403.00554\n",
      "2403.00553\n",
      "2403.00550\n",
      "2403.00546\n",
      "2403.00543\n",
      "2403.00542\n",
      "2403.00540\n",
      "2403.00539\n",
      "2403.00536\n",
      "2403.00529\n",
      "2403.00528\n",
      "2403.00527\n",
      "2403.00526\n",
      "2403.00522\n",
      "2403.00520\n",
      "2403.00517\n",
      "2403.00515\n",
      "2403.00514\n",
      "2403.00510\n",
      "2403.00509\n",
      "2403.00506\n",
      "2403.00504\n",
      "2403.00499\n",
      "2403.00497\n",
      "2403.00491\n",
      "2403.00489\n",
      "2403.00486\n",
      "[\"In this work, for the first time, we demonstrate that Mamba-based point cloud methods can outperform point-based methods. Mamba exhibits strong global modeling capabilities and linear computational complexity, making it highly attractive for point cloud analysis. To enable more effective processing of 3-D point cloud data by Mamba, we propose a novel Consistent Traverse Serialization to convert point clouds into 1-D point sequences while ensuring that neighboring points in the sequence are also spatially adjacent. Consistent Traverse Serialization yields six variants by permuting the order of x, y, and z coordinates, and the synergistic use of these variants aids Mamba in comprehensively observing point cloud data. Furthermore, to assist Mamba in handling point sequences with different orders more effectively, we introduce point prompts to inform Mamba of the sequence's arrangement rules. Finally, we propose positional encoding based on spatial coordinate mapping to inject positional information into point cloud sequences better. Based on these improvements, we construct a point cloud network named Point Cloud Mamba, which combines local and global modeling. Point Cloud Mamba surpasses the SOTA point-based method PointNeXt and achieves new SOTA performance on the ScanObjectNN, ModelNet40, and ShapeNetPart datasets.\", 'While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the \"reversal curse\". It is a typical example that the model knows \"A\\'s father is B\", but is unable to reason \"B\\'s child is A\". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models\\' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may disrupt complete phrases or entities, thereby posing challenges for the model to comprehend and learn from training data. To address this issue, we propose Semantic-aware Permutation Training (SPT), which addresses this issue by segmenting the training sentences into semantic units (i.e., entities or phrases) with an assistant language model and permuting these units before feeding into the model. Extensive experiments demonstrate that SPT effectively mitigates the reversal curse since the performance on reversed questions approximates that on the forward ones, and significantly advances the performance of existing works.', 'Low-latency video streaming over 5G has become rapidly popular over the last few years due to its increased usage in hosting virtual events, online education, webinars, and all-hands meetings. Our work aims to address the absence of studies that reveal the real-world behavior of low-latency video streaming. To that end, we provide an experimental methodology and measurements, collected in a US metropolitan area over a commercial 5G network, that correlates application-level QoE and lower-layer metrics on the devices, such as RSRP, RSRQ, handover records, etc., under both static and mobility scenarios. We find that RAN-side information, which is readily available on every cellular device, has the potential to enhance throughput estimation modules of video streaming clients, ultimately making low-latency streaming more resilient against network perturbations and handover events.', 'Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.', 'The solution of a sparse system of linear equations is ubiquitous in scientific applications. Iterative methods, such as the Preconditioned Conjugate Gradient method (PCG), are normally chosen over direct methods due to memory and computational complexity constraints. However, the efficiency of these methods depends on the preconditioner utilized. The development of the preconditioner normally requires some insight into the sparse linear system and the desired trade-off of generating the preconditioner and the reduction in the number of iterations. Incomplete factorization methods tend to be black box methods to generate these preconditioners but may fail for a number of reasons. These reasons include numerical issues that require searching for adequate scaling, shifting, and fill-in while utilizing a difficult to parallelize algorithm. With a move towards heterogeneous computing, many sparse applications find GPUs that are optimized for dense tensor applications like training neural networks being underutilized. In this work, we demonstrate that a simple artificial neural network trained either at compile time or in parallel to the running application on a GPU can provide an incomplete sparse Cholesky factorization that can be used as a preconditioner. This generated preconditioner is as good or better in terms of reduction of iterations than the one found using multiple preconditioning techniques such as scaling and shifting. Moreover, the generated method also works and never fails to produce a preconditioner that does not reduce the iteration count.', \"Hundreds of millions of people now interact with language models, with uses ranging from serving as a writing aid to informing hiring decisions. Yet these language models are known to perpetuate systematic racial prejudices, making their judgments biased in problematic ways about groups like African Americans. While prior research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice: we extend research showing that Americans hold raciolinguistic stereotypes about speakers of African American English and find that language models have the same prejudice, exhibiting covert stereotypes that are more negative than any human stereotypes about African Americans ever experimentally recorded, although closest to the ones from before the civil rights movement. By contrast, the language models' overt stereotypes about African Americans are much more positive. We demonstrate that dialect prejudice has the potential for harmful consequences by asking language models to make hypothetical decisions about people, based only on how they speak. Language models are more likely to suggest that speakers of African American English be assigned less prestigious jobs, be convicted of crimes, and be sentenced to death. Finally, we show that existing methods for alleviating racial bias in language models such as human feedback training do not mitigate the dialect prejudice, but can exacerbate the discrepancy between covert and overt stereotypes, by teaching language models to superficially conceal the racism that they maintain on a deeper level. Our findings have far-reaching implications for the fair and safe employment of language technology.\", \"Satisfiability solving has been used to tackle a range of long-standing open math problems in recent years. We add another success by solving a geometry problem that originated a century ago. In the 1930s, Esther Klein's exploration of unavoidable shapes in planar point sets in general position showed that every set of five points includes four points in convex position. For a long time, it was open if an empty hexagon, i.e., six points in convex position without a point inside, can be avoided. In 2006, Gerken and Nicolás independently proved that the answer is no. We establish the exact bound: Every 30-point set in the plane in general position contains an empty hexagon. Our key contributions include an effective, compact encoding and a search-space partitioning strategy enabling linear-time speedups even when using thousands of cores.\", 'Spatial relationships between objects represent key scene information for humans to understand and interact with the world. To study the capability of current computer vision systems to recognize physically grounded spatial relations, we start by proposing precise relation definitions that permit consistently annotating a benchmark dataset. Despite the apparent simplicity of this task relative to others in the recognition literature, we observe that existing approaches perform poorly on this benchmark. We propose new approaches exploiting the long-range attention capabilities of transformers for this task, and evaluating key design principles. We identify a simple \"RelatiViT\" architecture and demonstrate that it outperforms all current approaches. To our knowledge, this is the first method to convincingly outperform naive baselines on spatial relation prediction in in-the-wild settings. The code and datasets are available in \\\\url{this https URL}.', 'The robustness of human social networks against epidemic propagation relies on the propensity for physical contact adaptation. During the early phase of infection, asymptomatic carriers exhibit the same activity level as susceptible individuals, which presents challenges for incorporating control measures in epidemic projection models. This paper focuses on modeling and cost-efficient activity control of susceptible and carrier individuals in the context of the susceptible-carrier-infected-removed (SCIR) epidemic model over a two-layer contact network. In this model, individuals switch from a static contact layer to create new links in a temporal layer based on state-dependent activation rates. We derive conditions for the infection to die out or persist in a homogeneous network. Considering the significant costs associated with reducing the activity of susceptible and carrier individuals, we formulate an optimization problem to minimize the disease decay rate while constrained by a limited budget. We propose the use of successive geometric programming (SGP) approximation for this optimization task. Through simulation experiments on Poisson random graphs, we assess the impact of different parameters on disease prevalence. The results demonstrate that our SGP framework achieves a cost reduction of nearly 33% compared to conventional methods based on degree and closeness centrality.', 'The goal of few-shot relation extraction is to predict relations between name entities in a sentence when only a few labeled instances are available for training. Existing few-shot relation extraction methods focus on uni-modal information such as text only. This reduces performance when there are no clear contexts between the name entities described in text. We propose a multi-modal few-shot relation extraction model (MFS-HVE) that leverages both textual and visual semantic information to learn a multi-modal representation jointly. The MFS-HVE includes semantic feature extractors and multi-modal fusion components. The MFS-HVE semantic feature extractors are developed to extract both textual and visual features. The visual features include global image features and local object features within the image. The MFS-HVE multi-modal fusion unit integrates information from various modalities using image-guided attention, object-guided attention, and hybrid feature attention to fully capture the semantic interaction between visual regions of images and relevant texts. Extensive experiments conducted on two public datasets demonstrate that semantic visual information significantly improves the performance of few-shot relation prediction.', 'Implicit-depth neural networks have grown as powerful alternatives to traditional networks in various applications in recent years. However, these models often lack guarantees of existence and uniqueness, raising stability, performance, and reproducibility issues. In this paper, we present a new analysis of the existence and uniqueness of fixed points for implicit-depth neural networks based on the concept of subhomogeneous operators and the nonlinear Perron-Frobenius theory. Compared to previous similar analyses, our theory allows for weaker assumptions on the parameter matrices, thus yielding a more flexible framework for well-defined implicit networks. We illustrate the performance of the resulting subhomogeneous networks on feed-forward, convolutional, and graph neural network examples.', 'This paper investigates new data exploration experiences that enable blind users to interact with statistical data visualizations$-$bar plots, heat maps, box plots, and scatter plots$-$leveraging multimodal data representations. In addition to sonification and textual descriptions that are commonly employed by existing accessible visualizations, our MAIDR (multimodal access and interactive data representation) system incorporates two additional modalities (braille and review) that offer complementary benefits. It also provides blind users with the autonomy and control to interactively access and understand data visualizations. In a user study involving 11 blind participants, we found the MAIDR system facilitated the accurate interpretation of statistical visualizations. Participants exhibited a range of strategies in combining multiple modalities, influenced by their past interactions and experiences with data visualizations. This work accentuates the overlooked potential of combining refreshable tactile representation with other modalities and elevates the discussion on the importance of user autonomy when designing accessible data visualizations.', \"Follow-The-Regularized-Leader (FTRL) is known as an effective and versatile approach in online learning, where appropriate choice of the learning rate is crucial for smaller regret. To this end, we formulate the problem of adjusting FTRL's learning rate as a sequential decision-making problem and introduce the framework of competitive analysis. We establish a lower bound for the competitive ratio and propose update rules for learning rate that achieves an upper bound within a constant factor of this lower bound. Specifically, we illustrate that the optimal competitive ratio is characterized by the (approximate) monotonicity of components of the penalty term, showing that a constant competitive ratio is achievable if the components of the penalty term form a monotonically non-increasing sequence, and derive a tight competitive ratio when penalty terms are $\\\\xi$-approximately monotone non-increasing. Our proposed update rule, referred to as \\\\textit{stability-penalty matching}, also facilitates constructing the Best-Of-Both-Worlds (BOBW) algorithms for stochastic and adversarial environments. In these environments our result contributes to achieve tighter regret bound and broaden the applicability of algorithms for various settings such as multi-armed bandits, graph bandits, linear bandits, and contextual bandits.\", 'Despite the growing demand for accurate surface normal estimation models, existing methods use general-purpose dense prediction models, adopting the same inductive biases as other tasks. In this paper, we discuss the inductive biases needed for surface normal estimation and propose to (1) utilize the per-pixel ray direction and (2) encode the relationship between neighboring surface normals by learning their relative rotation. The proposed method can generate crisp - yet, piecewise smooth - predictions for challenging in-the-wild images of arbitrary resolution and aspect ratio. Compared to a recent ViT-based state-of-the-art model, our method shows a stronger generalization ability, despite being trained on an orders of magnitude smaller dataset. The code is available atthis https URL.', 'Like the notion of computation via (strong) monads serves to classify various flavours of impurity, including exceptions, non-determinism, probability, local and global store, the notion of guardedness classifies well-behavedness of cycles in various settings. In its most general form, the guardedness discipline applies to general symmetric monoidal categories and further specializes to Cartesian and co-Cartesian categories, where it governs guarded recursion and guarded iteration respectively. Here, even more specifically, we deal with the semantics of call-by-value guarded iteration. It was shown by Levy, Power and Thielecke that call-by-value languages can be generally interpreted in Freyd categories, but in order to represent effectful function spaces, such a category must canonically arise from a strong monad. We generalize this fact by showing that representing guarded effectful function spaces calls for certain parametrized monads (in the sense of Uustalu). This provides a description of guardedness as an intrinsic categorical property of programs, complementing the existing description of guardedness as a predicate on a category.', 'Self-consistency has emerged as a powerful method for improving the accuracy of short answers generated by large language models. As previously defined, it only concerns the accuracy of a final answer parsed from generated text. In this work, we extend the idea to open response generation, by integrating voting into the decoding method. Each output sentence is selected from among multiple samples, conditioning on the previous selections, based on a simple token overlap score. We compare this \"Sample & Select\" method to greedy decoding, beam search, nucleus sampling, and the recently introduced hallucination avoiding decoders of DoLA, P-CRR, and S-CRR. We show that Sample & Select improves factuality by a 30% relative margin against these decoders in NLI-based evaluation on the subsets of CNN/DM and XSum used in the FRANK benchmark, while maintaining comparable ROUGE-1 F1 scores against reference summaries. We collect human verifications of the generated summaries, confirming the factual superiority of our method.', \"Information retrieval is an ever-evolving and crucial research domain. The substantial demand for high-quality human motion data especially in online acquirement has led to a surge in human motion research works. Prior works have mainly concentrated on dual-modality learning, such as text and motion tasks, but three-modality learning has been rarely explored. Intuitively, an extra introduced modality can enrich a model's application scenario, and more importantly, an adequate choice of the extra modality can also act as an intermediary and enhance the alignment between the other two disparate modalities. In this work, we introduce LAVIMO (LAnguage-VIdeo-MOtion alignment), a novel framework for three-modality learning integrating human-centric videos as an additional modality, thereby effectively bridging the gap between text and motion. Moreover, our approach leverages a specially designed attention mechanism to foster enhanced alignment and synergistic effects among text, video, and motion modalities. Empirically, our results on the HumanML3D and KIT-ML datasets show that LAVIMO achieves state-of-the-art performance in various motion-related cross-modal retrieval tasks, including text-to-motion, motion-to-text, video-to-motion and motion-to-video.\", \"Large Language Models (LLMs) have shown great success as high-level planners for zero-shot game-playing agents. However, these agents are primarily evaluated on Minecraft, where long-term planning is relatively straightforward. In contrast, agents tested in dynamic robot environments face limitations due to simplistic environments with only a few objects and interactions. To fill this gap in the literature, we present NetPlay, the first LLM-powered zero-shot agent for the challenging roguelike NetHack. NetHack is a particularly challenging environment due to its diverse set of items and monsters, complex interactions, and many ways to die.NetPlay uses an architecture designed for dynamic robot environments, modified for NetHack. Like previous approaches, it prompts the LLM to choose from predefined skills and tracks past interactions to enhance decision-making. Given NetHack's unpredictable nature, NetPlay detects important game events to interrupt running skills, enabling it to react to unforeseen circumstances. While NetPlay demonstrates considerable flexibility and proficiency in interacting with NetHack's mechanics, it struggles with ambiguous task descriptions and a lack of explicit feedback. Our findings demonstrate that NetPlay performs best with detailed context information, indicating the necessity for dynamic methods in supplying context information for complex games such as NetHack.\", \"Hydra is a system which utilizes computer vision to perform near real time data quality management, initially developed for Hall-D in 2019. Since then, it has been deployed across all experimental halls at Jefferson Lab, with the CLAS12 collaboration in Hall-B being the first outside of GlueX to fully utilize Hydra. The system comprises back end processes that manage the models, their inferences, and the data flow. The front-end components, accessible via web pages, allow detector experts and shift crews to view and interact with the system. This talk will give an overview of the Hydra system as well as highlight significant developments in Hydra's feature set, acute challenges with operating Hydra in all halls, and lessons learned along the way.\", 'How should text dataset sizes be compared across languages? Even for content-matched (parallel) corpora, UTF-8 encoded text can require a dramatically different number of bytes for different languages. In our work, we define the byte premium between two languages as the ratio of bytes used to encode content-matched text in those languages. We compute byte premiums for 1155 languages, and we use linear regressions to estimate byte premiums for other languages. We release a tool to obtain byte premiums for any two languages, enabling comparisons of dataset sizes across languages for more equitable multilingual model development and data practices.', 'Defeasible reasoning is a kind of reasoning where some generalisations may not be valid in all circumstances, that is general conclusions may fail in some cases. Various formalisms have been developed to model this kind of reasoning, which is characteristic of common-sense contexts. However, it is not easy for a modeller to choose among these systems the one that better fits its domain from an ontological point of view. In this paper we first propose a framework based on the notions of exceptionality and defeasibility in order to be able to compare formalisms and reveal their ontological commitments. Then, we apply this framework to compare four systems, showing the differences that may occur from an ontological perspective.', 'This paper deals with the equation $-\\\\Delta u+\\\\mu u=f$ on high-dimensional spaces $\\\\mathbb{R}^m$, where the right-hand side $f(x)=F(Tx)$ is composed of a separable function $F$ with an integrable Fourier transform on a space of a dimension $n>m$ and a linear mapping given by a matrix $T$ of full rank and $\\\\mu\\\\geq 0$ is a constant. For example, the right-hand side can explicitly depend on differences $x_i-x_j$ of components of $x$. Following our publication [Numer. Math. (2020) 146:219--238], we show that the solution of this equation can be expanded into sums of functions of the same structure and develop in this framework an equally simple and fast iterative method for its computation. The method is based on the observation that in almost all cases and for large problem classes the expression $\\\\|T^ty\\\\|^2$ deviates on the unit sphere $\\\\|y\\\\|=1$ the less from its mean value the higher the dimension $m$ is, a concentration of measure effect. The higher the dimension $m$, the faster the iteration converges.', 'Item Response Theory (IRT) models aim to assess latent abilities of $n$ examinees along with latent difficulty characteristics of $m$ test items from categorical data that indicates the quality of their corresponding answers. Classical psychometric assessments are based on a relatively small number of examinees and items, say a class of $200$ students solving an exam comprising $10$ problems. More recent global large scale assessments such as PISA, or internet studies, may lead to significantly increased numbers of participants. Additionally, in the context of Machine Learning where algorithms take the role of examinees and data analysis problems take the role of items, both $n$ and $m$ may become very large, challenging the efficiency and scalability of computations. To learn the latent variables in IRT models from large data, we leverage the similarity of these models to logistic regression, which can be approximated accurately using small weighted subsets called coresets. We develop coresets for their use in alternating IRT training algorithms, facilitating scalable learning from large data.', 'Reinforcement learning provides a mathematical framework for learning-based control, whose success largely depends on the amount of data it can utilize. The efficient utilization of historical trajectories obtained from previous policies is essential for expediting policy optimization. Empirical evidence has shown that policy gradient methods based on importance sampling work well. However, existing literature often neglect the interdependence between trajectories from different iterations, and the good empirical performance lacks a rigorous theoretical justification. In this paper, we study a variant of the natural policy gradient method with reusing historical trajectories via importance sampling. We show that the bias of the proposed estimator of the gradient is asymptotically negligible, the resultant algorithm is convergent, and reusing past trajectories helps improve the convergence rate. We further apply the proposed estimator to popular policy optimization algorithms such as trust region policy optimization. Our theoretical results are verified on classical benchmarks.', 'Cell-free massive multiple-input multiple-output (MIMO) is a promising technology for next-generation communication systems. This work proposes a novel partially coherent (PC) transmission framework to cope with the challenge of phase misalignment among the access points (APs), which is important for unlocking the full potential of cell-free massive MIMO technology. With the PC operation, the APs are only required to be phase-aligned within clusters. Each cluster transmits the same data stream towards each user equipment (UE), while different clusters send different data streams. We first propose a novel algorithm to group APs into clusters such that the distance between two APs is always smaller than a reference distance ensuring the phase alignment of these APs. Then, we propose new algorithms that optimize the combining at UEs and precoding at APs to maximize the downlink sum data rates. We also propose a novel algorithm for data stream allocation to further improve the sum data rate of the PC operation. Numerical results show that the PC operation using the proposed framework with a sufficiently small reference distance can offer a sum rate close to the sum rate of the ideal fully coherent (FC) operation that requires network-wide phase alignment. This demonstrates the potential of PC operation in practical deployments of cell-free massive MIMO networks.', 'Deep reinforcement learning (DRL) algorithms require substantial samples and computational resources to achieve higher performance, which restricts their practical application and poses challenges for further development. Given the constraint of limited resources, it is essential to leverage existing computational work (e.g., learned policies, samples) to enhance sample efficiency and reduce the computational resource consumption of DRL algorithms. Previous works to leverage existing computational work require intrusive modifications to existing algorithms and models, designed specifically for specific algorithms, lacking flexibility and universality. In this paper, we present the Snapshot Reinforcement Learning (SnapshotRL) framework, which enhances sample efficiency by simply altering environments, without making any modifications to algorithms and models. By allowing student agents to choose states in teacher trajectories as the initial state to sample, SnapshotRL can effectively utilize teacher trajectories to assist student agents in training, allowing student agents to explore a larger state space at the early training phase. We propose a simple and effective SnapshotRL baseline algorithm, S3RL, which integrates well with existing DRL algorithms. Our experiments demonstrate that integrating S3RL with TD3, SAC, and PPO algorithms on the MuJoCo benchmark significantly improves sample efficiency and average return, without extra samples and additional computational resources.', 'Additive manufacturing (AM) has already proved itself to be the potential alternative to widely-used subtractive manufacturing due to its extraordinary capacity of manufacturing highly customized products with minimum material wastage. Nevertheless, it is still not being considered as the primary choice for the industry due to some of its major inherent challenges, including complex and dynamic process interactions, which are sometimes difficult to fully understand even with traditional machine learning because of the involvement of high-dimensional data such as images, point clouds, and voxels. However, the recent emergence of deep learning (DL) is showing great promise in overcoming many of these challenges as DL can automatically capture complex relationships from high-dimensional data without hand-crafted feature extraction. Therefore, the volume of research in the intersection of AM and DL is exponentially growing each year which makes it difficult for the researchers to keep track of the trend and future potential directions. Furthermore, to the best of our knowledge, there is no comprehensive review paper in this research track summarizing the recent studies. Therefore, this paper reviews the recent studies that apply DL for making the AM process better with a high-level summary of their contributions and limitations. Finally, it summarizes the current challenges and recommends some of the promising opportunities in this domain for further investigation with a special focus on generalizing DL models for wide-range of geometry types, managing uncertainties both in AM data and DL models, overcoming limited and noisy AM data issues by incorporating generative models, and unveiling the potential of interpretable DL for AM.', 'The spectrum crunch challenge poses a vital threat to the progress of cellular networks and recently prompted the inclusion of millimeter wave (mmWave) and Upper 6GHz (U6G) in the 3GPP standards. These two bands promise to unlock a large portion of untapped spectrum, but the harsh propagation due to the increased carrier frequency might negatively impact the performance of urban Radio Access Network (RAN) deployments. Within the span of a year, two co-located 5G networks operating in these frequency bands were deployed at Politecnico di Milano, Milan, Italy, entirely dedicated to the dense urban performance assessment of the two systems. This paper presents an in-depth analysis of the measurement campaigns conducted on them, with the U6G campaign representing the first of its kind. A benchmark is provided by ray-tracing simulations. The results suggest that networks operating in these frequency bands provide good indoor and outdoor coverage and throughput in urban scenarios, even when deployed in the macro base station setup common to lower frequencies. In addition, a comparative performance analysis of these two key technologies is provided, offering insights on their relative strengths, weaknesses and improvement margins and informing on which bands is better suited for urban macro coverage.', 'In this article, the use of channel state information (CSI) for indoor positioning is studied. In the considered model, a server equipped with several antennas sends pilot signals to users, while each user uses the received pilot signals to estimate channel states for user positioning. To this end, we formulate the positioning problem as an optimization problem aiming to minimize the gap between the estimated positions and the ground truth positions of users. To solve this problem, we design a complex-valued neural network (CVNN) model based federated learning (FL) algorithm. Compared to standard real-valued centralized machine learning (ML) methods, our proposed algorithm has two main advantages. First, our proposed algorithm can directly process complex-valued CSI data without data transformation. Second, our proposed algorithm is a distributed ML method that does not require users to send their CSI data to the server. Since the output of our proposed algorithm is complex-valued which consists of the real and imaginary parts, we study the use of the CVNN to implement two learning tasks. First, the proposed algorithm directly outputs the estimated positions of a user. Here, the real and imaginary parts of an output neuron represent the 2D coordinates of the user. Second, the proposed method can output two CSI features (i.e., line-of-sight/non-line-of-sight transmission link classification and time of arrival (TOA) prediction) which can be used in traditional positioning algorithms. Simulation results demonstrate that our designed CVNN based FL can reduce the mean positioning error between the estimated position and the actual position by up to 36\\\\%, compared to a RVNN based FL which requires to transform CSI data into real-valued data.', 'Colorectal cancer is the third most aggressive cancer worldwide. Polyps, as the main biomarker of the disease, are detected, localized, and characterized through colonoscopy procedures. Nonetheless, during the examination, up to 25% of polyps are missed, because of challenging conditions (camera movements, lighting changes), and the close similarity of polyps and intestinal folds. Besides, there is a remarked subjectivity and expert dependency to observe and detect abnormal regions along the intestinal tract. Currently, publicly available polyp datasets have allowed significant advances in computational strategies dedicated to characterizing non-parametric polyp shapes. These computational strategies have achieved remarkable scores of up to 90% in segmentation tasks. Nonetheless, these strategies operate on cropped and expert-selected frames that always observe polyps. In consequence, these computational approximations are far from clinical scenarios and real applications, where colonoscopies are redundant on intestinal background with high textural variability. In fact, the polyps typically represent less than 1% of total observations in a complete colonoscopy record. This work introduces COLON: the largest COlonoscopy LONg sequence dataset with around of 30 thousand polyp labeled frames and 400 thousand background frames. The dataset was collected from a total of 30 complete colonoscopies with polyps at different stages, variations in preparation procedures, and some cases the observation of surgical instrumentation. Additionally, 10 full intestinal background video control colonoscopies were integrated in order to achieve a robust polyp-background frame differentiation. The COLON dataset is open to the scientific community to bring new scenarios to propose computational tools dedicated to polyp detection and segmentation over long sequences, being closer to real colonoscopy scenarios.', \"Explanations are pervasive in our lives. Mostly, they occur in dialogical form where an {\\\\em explainer} discusses a concept or phenomenon of interest with an {\\\\em explainee}. Leaving the explainee with a clear understanding is not straightforward due to the knowledge gap between the two participants. Previous research looked at the interaction of explanation moves, dialogue acts, and topics in successful dialogues with expert explainers. However, daily-life explanations often fail, raising the question of what makes a dialogue successful. In this work, we study explanation dialogues in terms of the interactions between the explainer and explainee and how they correlate with the quality of explanations in terms of a successful understanding on the explainee's side. In particular, we first construct a corpus of 399 dialogues from the Reddit forum {\\\\em Explain Like I am Five} and annotate it for interaction flows and explanation quality. We then analyze the interaction flows, comparing them to those appearing in expert dialogues. Finally, we encode the interaction flows using two language models that can handle long inputs, and we provide empirical evidence for the effectiveness boost gained through the encoding in predicting the success of explanation dialogues.\", 'This work primarily focuses on an operator inference methodology aimed at constructing low-dimensional dynamical models based on a priori hypotheses about their structure, often informed by established physics or expert insights. Stability is a fundamental attribute of dynamical systems, yet it is not always assured in models derived through inference. Our main objective is to develop a method that facilitates the inference of quadratic control dynamical systems with inherent stability guarantees. To this aim, we investigate the stability characteristics of control systems with energy-preserving nonlinearities, thereby identifying conditions under which such systems are bounded-input bounded-state stable. These insights are subsequently applied to the learning process, yielding inferred models that are inherently stable by design. The efficacy of our proposed framework is demonstrated through a couple of numerical examples.', 'This paper investigates the robust cooperative output regulation problem for a class of heterogeneous uncertain linear multi-agent systems with an unknown exosystem via event-triggered control (ETC). By utilizing the internal model approach and the adaptive control technique, a distributed adaptive internal model is constructed for each agent. Then, based on this internal model, a fully distributed ETC strategy composed of a distributed event-triggered adaptive output feedback control law and a distributed dynamic event-triggering mechanism is proposed, in which each agent updates its control input at its own triggering time instants. It is shown that under the proposed ETC strategy, the robust cooperative output regulation problem can be solved without requiring either the global information associated with the communication topology or the bounds of the uncertain or unknown parameters in each agent and the exosystem. A numerical example is provided to illustrate the effectiveness of the proposed control strategy.', 'Diffusion models trained on large-scale datasets have achieved remarkable progress in image synthesis. However, due to the randomness in the diffusion process, they often struggle with handling diverse low-level tasks that require details preservation. To overcome this limitation, we present a new Diff-Plugin framework to enable a single pre-trained diffusion model to generate high-fidelity results across a variety of low-level tasks. Specifically, we first propose a lightweight Task-Plugin module with a dual branch design to provide task-specific priors, guiding the diffusion process in preserving image content. We then propose a Plugin-Selector that can automatically select different Task-Plugins based on the text instruction, allowing users to edit images by indicating multiple low-level tasks with natural language. We conduct extensive experiments on 8 low-level vision tasks. The results demonstrate the superiority of Diff-Plugin over existing methods, particularly in real-world scenarios. Our ablations further validate that Diff-Plugin is stable, schedulable, and supports robust training across different dataset sizes.', \"We study symmetric tensor decompositions, i.e., decompositions of the form $T = \\\\sum_{i=1}^r u_i^{\\\\otimes 3}$ where $T$ is a symmetric tensor of order 3 and $u_i \\\\in \\\\mathbb{C}^n$.In order to obtain efficient decomposition algorithms, it is necessary to require additional properties from $u_i$. In this paper we assume that the $u_i$ are linearly independent. This implies $r \\\\leq n$,that is, the decomposition of T is undercomplete.We give a randomized algorithm for the following problem in the exact arithmetic model of computation: Let $T$ be an order-3 symmetric tensor that has an undercomplete decomposition.Then given some $T'$ close to $T$, an accuracy parameter $\\\\varepsilon$, and an upper bound B on the condition number of the tensor, output vectors $u'_i$ such that $||u_i - u'_i|| \\\\leq \\\\varepsilon$ (up to permutation and multiplication by cube roots of unity) with high probability. The main novel features of our algorithm are:1) We provide the first algorithm for this problem that runs in linear time in the size of the input tensor. More specifically, it requires $O(n^3)$ arithmetic operations for all accuracy parameters $\\\\varepsilon =$ 1/poly(n) and B = poly(n).2) Our algorithm is robust, that is, it can handle inverse-quasi-polynomial noise (in $n$,B,$\\\\frac{1}{\\\\varepsilon}$) in the input tensor.3) We present a smoothed analysis of the condition number of the tensor decomposition problem. This guarantees that the condition number is low with high probability and further shows that our algorithm runs in linear time, except for some rare badly conditioned inputs.Our main algorithm is a reduction to the complete case ($r=n$) treated in our previous work [Koiran,Saha,CIAC 2023]. For efficiency reasons we cannot use this algorithm as a blackbox. Instead, we show that it can be run on an implicitly represented tensor obtained from the input tensor by a change of basis.\", 'Uniformity plays a crucial role in the assessment of learned representations, contributing to a deeper comprehension of self-supervised learning. The seminal work by \\\\citet{Wang2020UnderstandingCR} introduced a uniformity metric that quantitatively measures the collapse degree of learned representations. Directly optimizing this metric together with alignment proves to be effective in preventing constant collapse. However, we present both theoretical and empirical evidence revealing that this metric lacks sensitivity to dimensional collapse, highlighting its limitations. To address this limitation and design a more effective uniformity metric, this paper identifies five fundamental properties, some of which the existing uniformity metric fails to meet. We subsequently introduce a novel uniformity metric that satisfies all of these desiderata and exhibits sensitivity to dimensional collapse. When applied as an auxiliary loss in various established self-supervised methods, our proposed uniformity metric consistently enhances their performance in downstream tasks.Our code was released atthis https URL.', 'As Multi-Robot Systems (MRS) become more affordable and computing capabilities grow, they provide significant advantages for complex applications such as environmental monitoring, underwater inspections, or space exploration. However, accounting for potential communication loss or the unavailability of communication infrastructures in these application domains remains an open problem. Much of the applicable MRS research assumes that the system can sustain communication through proximity regulations and formation control or by devising a framework for separating and adhering to a predetermined plan for extended periods of disconnection. The latter technique enables an MRS to be more efficient, but breakdowns and environmental uncertainties can have a domino effect throughout the system, particularly when the mission goal is intricate or time-sensitive. To deal with this problem, our proposed framework has two main phases: i) a centralized planner to allocate mission tasks by rewarding intermittent rendezvous between robots to mitigate the effects of the unforeseen events during mission execution, and ii) a decentralized replanning scheme leveraging epistemic planning to formalize belief propagation and a Monte Carlo tree search for policy optimization given distributed rational belief updates. The proposed framework outperforms a baseline heuristic and is validated using simulations and experiments with aerial vehicles.', 'Observability is important to ensure the reliability of microservice applications. These applications are often prone to failures, since they have many independent services deployed on heterogeneous environments. When employed \"correctly\", observability can help developers identify and troubleshoot faults quickly. However, instrumenting and configuring the observability of a microservice application is not trivial but tool-dependent and tied to costs. Architects need to understand observability-related trade-offs in order to weigh between different observability design alternatives. Still, these architectural design decisions are not supported by systematic methods and typically just rely on \"professional intuition\". In this paper, we argue for a systematic method to arrive at informed and continuously assessable observability design decisions. Specifically, we focus on fault observability of cloud-native microservice applications, and turn this into a testable and quantifiable property. Towards our goal, we first model the scale and scope of observability design decisions across the cloud-native stack. Then, we propose observability metrics which can be determined for any microservice application through so-called observability experiments. We present a proof-of-concept implementation of our experiment tool OXN. OXN is able to inject arbitrary faults into an application, similar to Chaos Engineering, but also possesses the unique capability to modify the observability configuration, allowing for the assessment of design decisions that were previously left unexplored. We demonstrate our approach using a popular open source microservice application and show the trade-offs involved in different observability design decisions.', \"Human emotions are essentially molded by lived experiences, from which we construct personalised meaning. The engagement in such meaning-making process has been practiced as an intervention in various psychotherapies to promote wellness. Nevertheless, to support recollecting and recounting lived experiences in everyday life remains under explored in HCI. It also remains unknown how technologies such as generative AI models can facilitate the meaning making process, and ultimately support affective mindfulness. In this paper we present Metamorpheus, an affective interface that engages users in a creative visual storytelling of emotional experiences during dreams. Metamorpheus arranges the storyline based on a dream's emotional arc, and provokes self-reflection through the creation of metaphorical images and text depictions. The system provides metaphor suggestions, and generates visual metaphors and text depictions using generative AI models, while users can apply generations to recolour and re-arrange the interface to be visually affective. Our experience-centred evaluation manifests that, by interacting with Metamorpheus, users can recall their dreams in vivid detail, through which they relive and reflect upon their experiences in a meaningful way.\", \"Optimization is a critical tool for addressing a broad range of human and technical problems. However, the paradox of advanced optimization techniques is that they have maximum utility for problems in which the relationship between the structure of the problem and the ultimate solution is the most obscure. The existence of solution with limited insight contrasts with techniques that have been developed for a broad range of engineering problems where integral transform techniques yield solutions and insight in tandem. Here, we present a ``Pareto-Laplace'' integral transform framework that can be applied to problems typically studied via optimization. We show that the framework admits related geometric, statistical, and physical representations that provide new forms of insight into relationships between objectives and outcomes. We argue that some known approaches are special cases of this framework, and point to a broad range of problems for further application.\", 'Learned Image Compression (LIC) has shown remarkable progress in recent years. Existing works commonly employ CNN-based or self-attention-based modules as transform methods for compression. However, there is no prior research on neural transform that focuses on specific regions. In response, we introduce the class-agnostic segmentation masks (i.e. semantic masks without category labels) for extracting region-adaptive contextual information. Our proposed module, Region-Adaptive Transform, applies adaptive convolutions on different regions guided by the masks. Additionally, we introduce a plug-and-play module named Scale Affine Layer to incorporate rich contexts from various regions. While there have been prior image compression efforts that involve segmentation masks as additional intermediate inputs, our approach differs significantly from them. Our advantages lie in that, to avoid extra bitrate overhead, we treat these masks as privilege information, which is accessible during the model training stage but not required during the inference phase. To the best of our knowledge, we are the first to employ class-agnostic masks as privilege information and achieve superior performance in pixel-fidelity metrics, such as Peak Signal to Noise Ratio (PSNR). The experimental results demonstrate our improvement compared to previously well-performing methods, with about 8.2% bitrate saving compared to VTM-17.0. The code will be released atthis https URL.', 'Fine-tuning pre-trained models is a widely employed technique in numerous real-world applications. However, fine-tuning these models on new tasks can lead to unfair outcomes. This is due to the absence of generalization guarantees for fairness properties, regardless of whether the original pre-trained model was developed with fairness considerations. To tackle this issue, we introduce an efficient and robust fine-tuning framework specifically designed to mitigate biases in new tasks. Our empirical analysis shows that the parameters in the pre-trained model that affect predictions for different demographic groups are different, so based on this observation, we employ a transfer learning strategy that neutralizes the importance of these influential weights, determined using Fisher information across demographic groups. Additionally, we integrate this weight importance neutralization strategy with a matrix factorization technique, which provides a low-rank approximation of the weight matrix using fewer parameters, reducing the computational demands. Experiments on multiple pre-trained models and new tasks demonstrate the effectiveness of our method.', 'We establish a theoretical framework of the particle relaxation method for uniform particle generation of Smoothed Particle Hydrodynamics. We achieve this by reformulating the particle relaxation as an optimization problem. The objective function is an integral difference between discrete particle-based and smoothed-analytical volume fractions. The analysis demonstrates that the particle relaxation method in the domain interior is essentially equivalent to employing a gradient descent approach to solve this optimization problem, and we can extend such an equivalence to the bounded domain by introducing a proper boundary term. Additionally, each periodic particle distribution has a spatially uniform particle volume, denoted as characteristic volume. The relaxed particle distribution has the largest characteristic volume, and the kernel cut-off radius determines this volume. This insight enables us to control the relaxed particle distribution by selecting the target kernel cut-off radius for a given kernel function.', 'In this paper, we propose a low-latency decoding solution of shortened polar codes based on their automorphism groups. The automorphism group of shortened polar codes, designed according to two existing shortening patterns, are shown to be limited but non-empty, making the Automorphism Ensemble (AE) decoding of shortened polar codes possible. Extensive simulation results for shortened polar codes under AE are provided and are compared to the SC-List (SCL) algorithm. The block-error rate of shortened polar codes under AE matches or beats SCL while lowering the decoding latency.', 'The use of one-bit analog-to-digital converter (ADC) has been considered as a viable alternative to high resolution counterparts in realizing and commercializing massive multiple-input multiple-output (MIMO) systems. However, the issue of discarding the amplitude information by one-bit quantizers has to be compensated. Thus, carefully tailored methods need to be developed for one-bit channel estimation and data detection as the conventional ones cannot be used. To address these issues, the problems of one-bit channel estimation and data detection for MIMO orthogonal frequency division multiplexing (OFDM) system that operates over uncorrelated frequency selective channels are investigated here. We first develop channel estimators that exploit Gaussian discriminant analysis (GDA) classifier and approximated versions of it as the so-called weak classifiers in an adaptive boosting (AdaBoost) approach. Particularly, the combination of the approximated GDA classifiers with AdaBoost offers the benefit of scalability with the linear order of computations, which is critical in massive MIMO-OFDM systems. We then take advantage of the same idea for proposing the data detectors. Numerical results validate the efficiency of the proposed channel estimators and data detectors compared to other methods. They show comparable/better performance to that of the state-of-the-art methods, but require dramatically lower computational complexities and run times.', \"We present a simultaneous sensor-based inspection and footprint coverage (SIFC) planning and control design with applications to autonomous robotic crack mapping and filling. The main challenge of the SIFC problem lies in the coupling of complete sensing (for mapping) and robotic footprint (for filling) coverage tasks. Initially, we assume known target information (e.g., crack) and employ classic cell decomposition methods to achieve complete sensing coverage of the workspace and complete robotic footprint coverage using the least-cost route. Subsequently, we generalize the algorithm to handle unknown target information, allowing the robot to scan and incrementally construct the target graph online while conducting robotic footprint coverage. The online polynomial-time SIFC planning algorithm minimizes the total robot traveling distance, guarantees complete sensing coverage of the entire workspace, and achieves near-optimal robotic footprint coverage, as demonstrated through empirical experiments. For the demonstrated application, we design coordinated nozzle motion control with the planned robot trajectory to efficiently fill all cracks within the robot's footprint. Experimental results are presented to illustrate the algorithm's design, performance, and comparisons. The SIFC algorithm offers a high-efficiency motion planning solution for various robotic applications requiring simultaneous sensing and actuation coverage.\", 'This paper investigates the problems of interference prediction and sensing for efficient spectrum access and link adaptation. The considered approach for interference prediction relies on a parametric model. However, we assume that the number of observations available to learn theses parameters is limited. This implies that they should be treated as random variables rather than fixed values. We show how this can impact the spectrum access and link adaptation strategies. We also introduce the notion of \"interferer-coherence time\" to establish the number of independent interferer state realizations experienced by a codeword. We explain how it can be computed taking into account the model uncertainty and how this impacts the link adaptation.', \"We study a two-player discounted zero-sum stochastic game model for dynamic operational planning in military campaigns. At each stage, the players manage multiple commanders who order military actions on objectives that have an open line of control. When a battle over the control of an objective occurs, its stochastic outcome depends on the actions and the enabling support provided by the control of other objectives. Each player aims to maximize the cumulative number of objectives they control, weighted by their criticality. To solve this large-scale stochastic game, we derive properties of its Markov perfect equilibria by leveraging the logistics and military operational command and control structure. We show the consequential isotonicity of the optimal value function with respect to the partially ordered state space, which in turn leads to a significant reduction of the state and action spaces. We also accelerate Shapley's value iteration algorithm by eliminating dominated actions and investigating pure equilibria of the matrix game solved at each iteration. We demonstrate the computational value of our equilibrium results on a case study that reflects representative operational-level military campaigns with geopolitical implications. Our analysis reveals a complex interplay between the game's parameters and dynamics in equilibrium, resulting in new military insights for campaign analysts.\", 'Convolutional neural networks (CNNs) have long been the paradigm of choice for robust medical image processing (MIP). Therefore, it is crucial to effectively and efficiently deploy CNNs on devices with different computing capabilities to support computer-aided diagnosis. Many methods employ factorized convolutional layers to alleviate the burden of limited computational resources at the expense of expressiveness. To this end, given weak medical image-driven CNN model optimization, a Singular value equalization generalizer-induced Factorized Convolution (SFConv) is proposed to improve the expressive power of factorized convolutions in MIP models. We first decompose the weight matrix of convolutional filters into two low-rank matrices to achieve model reduction. Then minimize the KL divergence between the two low-rank weight matrices and the uniform distribution, thereby reducing the number of singular value directions with significant variance. Extensive experiments on fundus and OCTA datasets demonstrate that our SFConv yields competitive expressiveness over vanilla convolutions while reducing complexity.', 'We consider many-to-one matching problems, where one side corresponds to applicants who have preferences and the other side to houses who do not have preferences. We consider two different types of this market: one, where the applicants have capacities, and one where the houses do. First, we answer an open question by Manlove and Sng (2006) (partly solved Paluch (2014) for preferences with ties), that is, we show that deciding if a popular matching exists in the house allocation problem, where agents have capacities is NP-hard for previously studied versions of popularity. Then, we consider the other version, where the houses have capacities. We study how to optimally increase the capacities of the houses to obtain a matching satisfying multiple optimality criteria, like popularity, Pareto-optimality and perfectness. We consider two common optimality criteria, one aiming to minimize the sum of capacity increases of all houses and the other aiming to minimize the maximum capacity increase of any school. We obtain a complete picture in terms of computational complexity and some algorithms.', 'We consider minimizers of the N-particle interaction potential energy and briefly review numerical methods used to calculate them. We consider simple pair potentials which are attractive at short distances and repulsive at long distances, focusing on examples which are sums of two powers. The range of powers we look at includes the well-known case of the Lennard-Jones potential, but we are also interested in less singular potentials which are relevant in collective behavior models. We report on results using the software GMIN developed by Wales and collaborators for problems in chemistry. For all cases, this algorithm gives good candidates for the minimizers for relatively low values of the particle number N. This is well-known for potentials similar to Lennard-Jones, but not for the range which is of interest in collective behavior. Standard minimization procedures have been used in the literature in this range, but they are likely to yield stationary states which are not minimizers. We illustrate numerically some properties of the minimizers in 2D, such as lattice structure, Wulff shapes, and the continuous large-N limit for locally integrable (that is, less singular) potentials.', 'This paper revisits few-shot 3D point cloud semantic segmentation (FS-PCS), with a focus on two significant issues in the state-of-the-art: foreground leakage and sparse point distribution. The former arises from non-uniform point sampling, allowing models to distinguish the density disparities between foreground and background for easier segmentation. The latter results from sampling only 2,048 points, limiting semantic information and deviating from the real-world practice. To address these issues, we introduce a standardized FS-PCS setting, upon which a new benchmark is built. Moreover, we propose a novel FS-PCS model. While previous methods are based on feature optimization by mainly refining support features to enhance prototypes, our method is based on correlation optimization, referred to as Correlation Optimization Segmentation (COSeg). Specifically, we compute Class-specific Multi-prototypical Correlation (CMC) for each query point, representing its correlations to category prototypes. Then, we propose the Hyper Correlation Augmentation (HCA) module to enhance CMC. Furthermore, tackling the inherent property of few-shot training to incur base susceptibility for models, we propose to learn non-parametric prototypes for the base classes during training. The learned base prototypes are used to calibrate correlations for the background class through a Base Prototypes Calibration (BPC) module. Experiments on popular datasets demonstrate the superiority of COSeg over existing methods. The code is available at:this https URL', 'Object detection limits its recognizable categories during the training phase, in which it can not cover all objects of interest for users. To satisfy the practical necessity, the incremental learning ability of the detector becomes a critical factor for real-world applications. Unfortunately, neural networks unavoidably meet catastrophic forgetting problem when it is implemented on a new task. To this end, many incremental object detection models preserve the knowledge of previous tasks by replaying samples or distillation from previous models. However, they ignore an important factor that the performance of the model mostly depends on its feature. These models try to rouse the memory of the neural network with previous samples but not to prevent forgetting. To this end, in this paper, we propose an incremental causal object detection (ICOD) model by learning causal features, which can adapt to more tasks. Traditional object detection models, unavoidably depend on the data-bias or data-specific features to get the detection results, which can not adapt to the new task. When the model meets the requirements of incremental learning, the data-bias information is not beneficial to the new task, and the incremental learning may eliminate these features and lead to forgetting. To this end, our ICOD is introduced to learn the causal features, rather than the data-bias features when training the detector. Thus, when the model is implemented to a new task, the causal features of the old task can aid the incremental learning process to alleviate the catastrophic forgetting problem. We conduct our model on several experiments, which shows a causal feature without data-bias can make the model adapt to new tasks better. \\\\keywords{Object detection, incremental learning, causal feature.', \"Today's networks are struggling to scale and satisfy the high number and high variety of co-existing network requirements. While existing congestion control (CC) protocols are designed to handle strict classification of network flows into one or few priorities, a more granular and dynamic congestion control is needed.In this paper we present Hercules, a novel CC protocol based on an online learning approach, which supports unbounded and continues requirements space. We implemented Hercules as a QUIC module and we show, through analytical analysis and real-world experiments, that it provides between $50\\\\%-250\\\\%$ higher QoS for co-existing diverse network flows and outperforms state-of-the-art CC protocols, even under high network congestion.\", \"Existing work has observed that current text-to-image systems do not accurately reflect explicit spatial relations between objects such as 'left of' or 'below'. We hypothesize that this is because explicit spatial relations rarely appear in the image captions used to train these models. We propose an automatic method that, given existing images, generates synthetic captions that contain 14 explicit spatial relations. We introduce the Spatial Relation for Generation (SR4G) dataset, which contains 9.9 millions image-caption pairs for training, and more than 60 thousand captions for evaluation. In order to test generalization we also provide an 'unseen' split, where the set of objects in the train and test captions are disjoint. SR4G is the first dataset that can be used to spatially fine-tune text-to-image systems. We show that fine-tuning two different Stable Diffusion models (denoted as SD$_{SR4G}$) yields up to 9 points improvements in the VISOR metric. The improvement holds in the 'unseen' split, showing that SD$_{SR4G}$ is able to generalize to unseen objects. SD$_{SR4G}$ improves the state-of-the-art with fewer parameters, and avoids complex architectures. Our analysis shows that improvement is consistent for all relations. The dataset and the code will be publicly available.\", 'We present the second version of the Open Assistant Toolkit (OAT-v2), an open-source task-oriented conversational system for composing generative neural models. OAT-v2 is a scalable and flexible assistant platform supporting multiple domains and modalities of user interaction. It splits processing a user utterance into modular system components, including submodules such as action code generation, multimodal content retrieval, and knowledge-augmented response generation. Developed over multiple years of the Alexa TaskBot challenge, OAT-v2 is a proven system that enables scalable and robust experimentation in experimental and real-world deployment. OAT-v2 provides open models and software for research and commercial applications to enable the future of multimodal virtual assistants across diverse applications and types of rich interaction.', \"Elasticity plays an important role in modern cloud computing systems. Elastic computing allows virtual machines (i.e., computing nodes) to be preempted when high-priority jobs arise, and also allows new virtual machines to participate in the computation. In 2018, Yang et al. introduced Coded Storage Elastic Computing (CSEC) to address the elasticity using coding technology, with lower storage and computation load requirements. However, CSEC is limited to certain types of computations (e.g., linear) due to the coded data storage based on linear coding. Then Centralized Uncoded Storage Elastic Computing (CUSEC) with heterogeneous computation speeds was proposed, which directly copies parts of data into the virtual machines. In all existing works in elastic computing, the storage assignment is centralized, meaning that the number and identity of all virtual machines possible used in the whole computation process are known during the storage assignment. In this paper, we consider Decentralized Uncoded Storage Elastic Computing (DUSEC) with heterogeneous computation speeds, where any available virtual machine can join the computation which is not predicted and thus coordination among different virtual machines' storage assignments is not allowed. Under a decentralized storage assignment originally proposed in coded caching by Maddah-Ali and Niesen, we propose a computing scheme with closed-form optimal computation time. We also run experiments over MNIST dataset with Softmax regression model through the Tencent cloud platform, and the experiment results demonstrate that the proposed DUSEC system approaches the state-of-art best storage assignment in the CUSEC system in computation time.\", \"We present a novel framework for user representation in large-scale recommender systems, aiming at effectively representing diverse user taste in a generalized manner. Our approach employs a two-stage methodology combining representation learning and transfer learning. The representation learning model uses an autoencoder that compresses various user features into a representation space. In the second stage, downstream task-specific models leverage user representations via transfer learning instead of curating user features individually. We further augment this methodology on the representation's input features to increase flexibility and enable reaction to user events, including new user experiences, in Near-Real Time. Additionally, we propose a novel solution to manage deployment of this framework in production models, allowing downstream models to work independently. We validate the performance of our framework through rigorous offline and online experiments within a large-scale system, showcasing its remarkable efficacy across multiple evaluation tasks. Finally, we show how the proposed framework can significantly reduce infrastructure costs compared to alternative approaches.\", 'Despite the importance of trust in human-AI interactions, researchers must adopt questionnaires from other disciplines that lack validation in the AI context. Motivated by the need for reliable and valid measures, we investigated the psychometric quality of two trust questionnaires, the Trust between People and Automation scale (TPA) by Jian et al. (2000) and the Trust Scale for the AI Context (TAI) by Hoffman et al. (2023). In a pre-registered online experiment (N = 1485), participants observed interactions with trustworthy and untrustworthy AI (autonomous vehicle and chatbot). Results support the psychometric quality of the TAI while revealing opportunities to improve the TPA, which we outline in our recommendations for using the two questionnaires. Furthermore, our findings provide additional empirical evidence of trust and distrust as two distinct constructs that may coexist independently. Building on our findings, we highlight the opportunities and added value of measuring both trust and distrust in human-AI research and advocate for further work on both constructs.', 'Modern transformer-based Large Language Models (LLMs) are constructed with a series of decoder blocks. Each block comprises three key components: (1) QKV generation, (2) multi-head attention, and (3) feed-forward networks. In batched processing, QKV generation and feed-forward networks involve compute-intensive matrix-matrix multiplications (GEMM), while multi-head attention requires bandwidth-heavy matrix-vector multiplications (GEMV). Machine learning accelerators like TPUs or NPUs are proficient in handling GEMM but are less efficient for GEMV computations. Conversely, Processing-in-Memory (PIM) technology is tailored for efficient GEMV computation, while it lacks the computational power to effectively handle GEMM. Inspired by this insight, we propose NeuPIMs, a heterogeneous accelerator-based system that jointly exploits a conventional GEMM-focused NPU and GEMV-optimized PIM devices. The main challenge in efficiently integrating NPU and PIM lies in enabling concurrent operations on both platforms, each addressing a specific kernel type. First, existing PIMs typically operate in a \"blocked\" mode, allowing only either NPU or PIM to be active at any given time. Second, the inherent dependencies between GEMM and GEMV in LLMs restrict their parallel processing. To tackle these challenges, NeuPIMs is equipped with dual row buffers in each bank, facilitating the simultaneous management of memory read/write operations and PIM commands. Further, NeuPIMs employs a runtime sub-batch interleaving technique to maximize concurrent execution, leveraging batch parallelism to allow two independent sub-batches to be pipelined within a single NeuPIMs node. Our evaluation demonstrates that compared to an NPU-only approach and a naïve NPU-PIM integrated system, NeuPIMs achieves 2.3$\\\\times$ and 1.6$\\\\times$ throughput improvement, respectively.', 'In this work we analyze the effectiveness of the Sparse Identification of Nonlinear Dynamics (SINDy) technique on three benchmark datasets for nonlinear identification, to provide a better understanding of its suitability when tackling real dynamical systems. While SINDy can be an appealing strategy for pursuing physics-based learning, our analysis highlights difficulties in dealing with unobserved states and non-smooth dynamics. Due to the ubiquity of these features in real systems in general, and control applications in particular, we complement our analysis with hands-on approaches to tackle these issues in order to exploit SINDy also in these challenging contexts.', 'Despite an extensive body of literature on deep learning optimization, our current understanding of what makes an optimization algorithm effective is fragmented. In particular, we do not understand well whether enhanced optimization translates to improved generalizability. Current research overlooks the inherent stochastic nature of stochastic gradient descent (SGD) and its variants, resulting in a lack of comprehensive benchmarking and insight into their statistical performance. This paper aims to address this gap by adopting a novel approach. Rather than solely evaluating the endpoint of individual optimization trajectories, we draw from an ensemble of trajectories to estimate the stationary distribution of stochastic optimizers. Our investigation encompasses a wide array of techniques, including SGD and its variants, flat-minima optimizers, and new algorithms we propose under the Basin Hopping framework. Through our evaluation, which encompasses synthetic functions with known minima and real-world problems in computer vision and natural language processing, we emphasize fair benchmarking under a statistical framework, comparing stationary distributions and establishing statistical significance. Our study uncovers several key findings regarding the relationship between training loss and hold-out accuracy, as well as the comparable performance of SGD, noise-enabled variants, and novel optimizers utilizing the BH framework. Notably, these algorithms demonstrate performance on par with flat-minima optimizers like SAM, albeit with half the gradient evaluations. We anticipate that our work will catalyze further exploration in deep learning optimization, encouraging a shift away from single-model approaches towards methodologies that acknowledge and leverage the stochastic nature of optimizers.', 'The increasing use of digital technologies and mobile-based registration procedures highlights the vital role of personal identity documents (IDs) in verifying users and safeguarding sensitive information. However, the rise in counterfeit ID production poses a significant challenge, necessitating the development of reliable and efficient automated verification methods. This paper introduces IDTrust, a deep-learning framework for assessing the quality of IDs. IDTrust is a system that enhances the quality of identification documents by using a deep learning-based approach. This method eliminates the need for relying on original document patterns for quality checks and pre-processing steps for alignment. As a result, it offers significant improvements in terms of dataset applicability. By utilizing a bandpass filtering-based method, the system aims to effectively detect and differentiate ID quality. Comprehensive experiments on the MIDV-2020 and L3i-ID datasets identify optimal parameters, significantly improving discrimination performance and effectively distinguishing between original and scanned ID documents.', 'The morphology of nanostructured materials exhibiting a polydisperse porous space, such as aerogels, is very open porous and fine grained. Therefore, a simulation of the deformation of a large aerogel structure resolving the nanostructure would be extremely expensive. Thus, multi-scale or homogenization approaches have to be considered. Here, a computational scale bridging approach based on the FE$^2$ method is suggested, where the macroscopic scale is discretized using finite elements while the microstructure of the open-porous material is resolved as a network of Euler-Bernoulli beams. Here, the beam frame based RVEs (representative volume elements) have pores whose size distribution follows the measured values for a specific material. This is a well-known approach to model aerogel structures. For the computational homogenization, an approach to average the first Piola-Kirchhoff stresses in a beam frame by neglecting rotational moments is suggested. To further overcome the computationally most expensive part in the homogenization method, that is, solving the RVEs and averaging their stress fields, a surrogate model is introduced based on neural networks. The networks input is the localized deformation gradient on the macroscopic scale and its output is the averaged stress for the specific material. It is trained on data generated by the beam frame based approach. The effiency and robustness of both homogenization approaches is shown numerically, the approximation properties of the surrogate model is verified for different macroscopic problems and discretizations. Different (Quasi-)Newton solvers are considered on the macroscopic scale and compared with respect to their convergence properties.', 'We present a comprehensive experimental study on image-level conditioning for diffusion models using cluster assignments. We elucidate how individual components regarding image clustering impact image synthesis across three datasets. By combining recent advancements from image clustering and diffusion models, we show that, given the optimal cluster granularity with respect to image synthesis (visual groups), cluster-conditioning can achieve state-of-the-art FID (i.e. 1.67, 2.17 on CIFAR10 and CIFAR100 respectively), while attaining a strong training sample efficiency. Finally, we propose a novel method to derive an upper cluster bound that reduces the search space of the visual groups using solely feature-based clustering. Unlike existing approaches, we find no significant connection between clustering and cluster-conditional image generation. The code and cluster assignments will be released.', 'Cross-domain few-shot learning (CDFSL) aims to acquire knowledge from limited training data in the target domain by leveraging prior knowledge transferred from source domains with abundant training samples. CDFSL faces challenges in transferring knowledge across dissimilar domains and fine-tuning models with limited training data. To address these challenges, we initially extend the analysis of loss landscapes from the parameter space to the representation space, which allows us to simultaneously interpret the transferring and fine-tuning difficulties of CDFSL models. We observe that sharp minima in the loss landscapes of the representation space result in representations that are hard to transfer and fine-tune. Moreover, existing flatness-based methods have limited generalization ability due to their short-range flatness. To enhance the transferability and facilitate fine-tuning, we introduce a simple yet effective approach to achieve long-range flattening of the minima in the loss landscape. This approach considers representations that are differently normalized as minima in the loss landscape and flattens the high-loss region in the middle by randomly sampling interpolated representations. We implement this method as a new normalization layer that replaces the original one in both CNNs and ViTs. This layer is simple and lightweight, introducing only a minimal number of additional parameters. Experimental results on 8 datasets demonstrate that our approach outperforms state-of-the-art methods in terms of average accuracy. Moreover, our method achieves performance improvements of up to 9\\\\% compared to the current best approaches on individual datasets. Our code will be released.', 'Automated phenotyping of plants for breeding and plant studies promises to provide quantitative metrics on plant traits at a previously unattainable observation frequency. Developers of tools for performing high-throughput phenotyping are, however, constrained by the availability of relevant datasets on which to perform validation. To this end, we present a spatio-temporal dataset of 3D point clouds of strawberry plants for two varieties, totalling 84 individual point clouds. We focus on the end use of such tools - the extraction of biologically relevant phenotypes - and demonstrate a phenotyping pipeline on the dataset. This comprises of the steps, including; segmentation, skeletonisation and tracking, and we detail how each stage facilitates the extraction of different phenotypes or provision of data insights. We particularly note that assessment is focused on the validation of phenotypes, extracted from the representations acquired at each step of the pipeline, rather than singularly focusing on assessing the representation itself. Therefore, where possible, we provide \\\\textit{in silico} ground truth baselines for the phenotypes extracted at each step and introduce methodology for the quantitative assessment of skeletonisation and the length trait extracted thereof. This dataset contributes to the corpus of freely available agricultural/horticultural spatio-temporal data for the development of next-generation phenotyping tools, increasing the number of plant varieties available for research in this field and providing a basis for genuine comparison of new phenotyping methodology.', 'Unmanned aerial vehicles are becoming common and have many productive uses. However, their increased prevalence raises safety concerns -- how can we protect restricted airspace? Knowing the type of unmanned aerial vehicle can go a long way in determining any potential risks it carries. For instance, fixed-wing craft can carry more weight over longer distances, thus potentially posing a more significant threat. This paper presents a machine learning model for classifying unmanned aerial vehicles as quadrotor, hexarotor, or fixed-wing. Our approach effectively applies a Long-Short Term Memory (LSTM) neural network for the purpose of time series classification. We performed experiments to test the effects of changing the timestamp sampling method and addressing the imbalance in the class distribution. Through these experiments, we identified the top-performing sampling and class imbalance fixing methods. Averaging the macro f-scores across 10 folds of data, we found that the majority quadrotor class was predicted well (98.16%), and, despite an extreme class imbalance, the model could also predicted a majority of fixed-wing flights correctly (73.15%). Hexarotor instances were often misclassified as quadrotors due to the similarity of multirotors in general (42.15%). However, results remained relatively stable across certain methods, which prompted us to analyze and report on their tradeoffs. The supplemental material for this paper, including the code and data for running all the experiments and generating the results tables, is available atthis https URL.', 'Sample efficiency remains a crucial challenge in applying Reinforcement Learning (RL) to real-world tasks. While recent algorithms have made significant strides in improving sample efficiency, none have achieved consistently superior performance across diverse domains. In this paper, we introduce EfficientZero V2, a general framework designed for sample-efficient RL algorithms. We have expanded the performance of EfficientZero to multiple domains, encompassing both continuous and discrete actions, as well as visual and low-dimensional inputs. With a series of improvements we propose, EfficientZero V2 outperforms the current state-of-the-art (SOTA) by a significant margin in diverse tasks under the limited data setting. EfficientZero V2 exhibits a notable advancement over the prevailing general algorithm, DreamerV3, achieving superior outcomes in 50 of 66 evaluated tasks across diverse benchmarks, such as Atari 100k, Proprio Control, and Vision Control.', \"Feature selection is a crucial task in settings where data is high-dimensional or acquiring the full set of features is costly. Recent developments in neural network-based embedded feature selection show promising results across a wide range of applications. Concrete Autoencoders (CAEs), considered state-of-the-art in embedded feature selection, may struggle to achieve stable joint optimization, hurting their training time and generalization. In this work, we identify that this instability is correlated with the CAE learning duplicate selections. To remedy this, we propose a simple and effective improvement: Indirectly Parameterized CAEs (IP-CAEs). IP-CAEs learn an embedding and a mapping from it to the Gumbel-Softmax distributions' parameters. Despite being simple to implement, IP-CAE exhibits significant and consistent improvements over CAE in both generalization and training time across several datasets for reconstruction and classification. Unlike CAE, IP-CAE effectively leverages non-linear relationships and does not require retraining the jointly optimized decoder. Furthermore, our approach is, in principle, generalizable to Gumbel-Softmax distributions beyond feature selection.\", 'Face images contain a wide variety of attribute information. In this paper, we propose a generalized framework for joint estimation of ordinal and nominal attributes based on information sharing. We tackle the correlation problem between heterogeneous attributes using hard parameter sharing of shallow features, and trade-off multiple loss functions by considering homoskedastic uncertainty for each attribute estimation task. This leads to optimal estimation of multiple attributes of the face and reduces the training cost of multitask learning. Experimental results on benchmarks with multiple face attributes show that the proposed approach has superior performance compared to state of the art. Finally, we discuss the bias issues arising from the proposed approach in face attribute estimation and validate its feasibility on edge systems.', 'In this paper, a set of tools is introduced that simplifies the synthesis and rapid-prototyping of single-loop rational kinematic chains. It allows the user to perform rational motion interpolation of up to four given poses and yields the design parameters of a linkage that can execute this motion. The package also provides a visualization of the output and performs a self-collision analysis with the possibility to adapt the design parameters. The results can be imported into CAD-systems for fast 3D printing.', 'The conditional mutual information quantifies the conditional dependence of two random variables. It has numerous applications; it forms, for example, part of the definition of transfer entropy, a common measure of the causal relationship between time series. It does, however, require a lot of data to estimate accurately and suffers the curse of dimensionality, limiting its application in machine learning and data science. However, the Kozachenko-Leonenko approach can address this problem: it is possible, in this approach to define a nearest-neighbour estimator which depends only on the distance between data points and not on the dimension of the data. Furthermore, the bias can be calculated analytically for this estimator. Here this estimator is described and is tested on simulated data.', 'This paper presents a distributed solution for the problem of collaborative collision avoidance for autonomous inland waterway ships. A two-layer collision avoidance framework that considers inland waterway traffic regulations is proposed to increase navigational safety for autonomous ships. Our approach allows for modifying traffic rules without changing the collision avoidance algorithm, and is based on a novel formulation of model predictive control (MPC) for collision avoidance of ships. This MPC formulation is designed for inland waterway traffic and can handle complex scenarios. The alternating direction method of multipliers is used as a scheme for exchanging and negotiating intentions among ships. Simulation results show that the proposed algorithm can comply with traffic rules. Furthermore, the proposed algorithm can safely deviate from traffic rules when necessary to increase efficiency in complex scenarios.', 'The diversity across outputs generated by large language models shapes the perception of their quality and utility. Prompt leaks, templated answer structure, and canned responses across different interactions are readily noticed by people, but there is no standard score to measure this aspect of model behavior. In this work we empirically investigate diversity scores on English texts. We find that computationally efficient compression algorithms capture information similar to what is measured by slow to compute $n$-gram overlap homogeneity scores. Further, a combination of measures -- compression ratios, self-repetition of long $n$-grams and Self-BLEU and BERTScore -- are sufficient to report, as they have low mutual correlation with each other. The applicability of scores extends beyond analysis of generative models; for example, we highlight applications on instruction-tuning datasets and human-produced texts. We release a diversity score package to facilitate research and invite consistency across reports.', 'Imitation learning field requires expert data to train agents in a task. Most often, this learning approach suffers from the absence of available data, which results in techniques being tested on its dataset. Creating datasets is a cumbersome process requiring researchers to train expert agents from scratch, record their interactions and test each benchmark method with newly created data. Moreover, creating new datasets for each new technique results in a lack of consistency in the evaluation process since each dataset can drastically vary in state and action distribution. In response, this work aims to address these issues by creating Imitation Learning Datasets, a toolkit that allows for: (i) curated expert policies with multithreaded support for faster dataset creation; (ii) readily available datasets and techniques with precise measurements; and (iii) sharing implementations of common imitation learning techniques. Demonstration link:this https URL', \"Vehicular Adhoc networks (VANETs) are composed of vehicles connected with wireless links to exchange data. VANETs have become the backbone of the Intelligent Transportation Systems (ITS) in smart cities and enable many essential services like roadside safety, traffic management, platooning, etc with vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communications. In any form of research testing and evaluation plays a crucial role. However, in VANETs, real-world experiments require high investment, and heavy resources and can cause many practical difficulties. Therefore, simulations have become critical and the primary way of evaluating VANETs' applications. Furthermore, the upfront challenge is the realistic capture of the networking mechanism of VANETs, which varies from situation to situation. Several factors may contribute to the successful achievement of a random realistic networking behavior. However, the biggest dependency is a powerful tool for the implementation, which could probably take into account all the configuration parameters, loss factors, mobility schemes, and other key features of a VANET, yet give out practical performance metrics with a good trade-off between investment of resources and the results. Hence, the aim of this research is to evaluate some simulators in the scope of VANETs with respect to resource utilization, packet delivery, and computational time.\", 'In this paper, we revisit techniques for uncertainty estimation within deep neural networks and consolidate a suite of techniques to enhance their reliability. Our investigation reveals that an integrated application of diverse techniques--spanning model regularization, classifier and optimization--substantially improves the accuracy of uncertainty predictions in image classification tasks. The synergistic effect of these techniques culminates in our novel SURE approach. We rigorously evaluate SURE against the benchmark of failure prediction, a critical testbed for uncertainty estimation efficacy. Our results showcase a consistently better performance than models that individually deploy each technique, across various datasets and model architectures. When applied to real-world challenges, such as data corruption, label noise, and long-tailed class distribution, SURE exhibits remarkable robustness, delivering results that are superior or on par with current state-of-the-art specialized methods. Particularly on Animal-10N and Food-101N for learning with noisy labels, SURE achieves state-of-the-art performance without any task-specific adjustments. This work not only sets a new benchmark for robust uncertainty estimation but also paves the way for its application in diverse, real-world scenarios where reliability is paramount. Our code is available at \\\\url{this https URL}.', 'Machine learning (ML) algorithms are predictively competitive algorithms with many human-impact applications. However, the issue of long execution time remains unsolved in the literature for high-dimensional spaces. This study proposes combining ML algorithms with an efficient methodology known as the barycentric correction procedure (BCP) to address this issue. This study uses synthetic data and an educational dataset from a private university to show the benefits of the proposed method. It was found that this combination provides significant benefits related to time in synthetic and real data without losing accuracy when the number of instances and dimensions increases. Additionally, for high-dimensional spaces, it was proved that BCP and linear support vector classification (LinearSVC), after an estimated feature map for the gaussian radial basis function (RBF) kernel, were unfeasible in terms of computational time and accuracy.', 'Thompson sampling (TS) serves as a solution for addressing the exploitation-exploration dilemma in Bayesian optimization (BO). While it prioritizes exploration by randomly generating and maximizing sample paths of Gaussian process (GP) posteriors, TS weakly manages its exploitation by gathering information about the true objective function after each exploration is performed. In this study, we incorporate the epsilon-greedy ($\\\\varepsilon$-greedy) policy, a well-established selection strategy in reinforcement learning, into TS to improve its exploitation. We first delineate two extremes of TS applied for BO, namely the generic TS and a sample-average TS. The former and latter promote exploration and exploitation, respectively. We then use $\\\\varepsilon$-greedy policy to randomly switch between the two extremes. A small value of $\\\\varepsilon \\\\in (0,1)$ prioritizes exploitation, and vice versa. We empirically show that $\\\\varepsilon$-greedy TS with an appropriate $\\\\varepsilon$ is better than one of its two extremes and competes with the other.', \"Python has emerged as one of the most popular programming languages, extensively utilized in domains such as machine learning, data analysis, and web applications. Python's dynamic nature and extensive usage make it an attractive candidate for dynamic program analysis. However, unlike for other popular languages, there currently is no comprehensive benchmark suite of executable Python projects, which hinders the development of dynamic analyses. This work addresses this gap by presenting DyPyBench, the first benchmark of Python projects that is large scale, diverse, ready to run (i.e., with fully configured and prepared test suites), and ready to analyze (by integrating with the DynaPyt dynamic analysis framework). The benchmark encompasses 50 popular opensource projects from various application domains, with a total of 681k lines of Python code, and 30k test cases. DyPyBench enables various applications in testing and dynamic analysis, of which we explore three in this work: (i) Gathering dynamic call graphs and empirically comparing them to statically computed call graphs, which exposes and quantifies limitations of existing call graph construction techniques for Python. (ii) Using DyPyBench to build a training data set for LExecutor, a neural model that learns to predict values that otherwise would be missing at runtime. (iii) Using dynamically gathered execution traces to mine API usage specifications, which establishes a baseline for future work on specification mining for Python. We envision DyPyBench to provide a basis for other dynamic analyses and for studying the runtime behavior of Python code.\", 'An important goal in algorithm design is determining the best running time for solving a problem (approximately). For some problems, we know the optimal running time, assuming certain conditional lower bounds. In this work, we study the $d$-dimensional geometric knapsack problem where we are far from this level of understanding. We are given a set of weighted d-dimensional geometric items like squares, rectangles, or hypercubes and a knapsack which is a square or a (hyper-)cube. We want to select a subset of items that fit non-overlappingly inside the knapsack, maximizing the total profit of the packed items. We make a significant step towards determining the best running time for solving these problems approximately by presenting approximation algorithms with near-linear running times for any constant dimension d and any constant parameter $\\\\epsilon$.For (hyper)-cubes, we present a $(1+\\\\epsilon)$-approximation algorithm whose running time drastically improves upon the known $(1+\\\\epsilon)$-approximation algorithm which has a running time where the exponent of n depends exponentially on $1/\\\\epsilon$ and $d$. Moreover, we present a $(2+\\\\epsilon)$-approximation algorithm for rectangles in the setting without rotations and a $(17/9+\\\\epsilon)$-approximation algorithm if we allow rotations by 90 degrees. The best known polynomial time algorithms for these settings have approximation ratios of $17/9+\\\\epsilon$ and $1.5+\\\\epsilon$, respectively, and running times in which the exponent of n depends exponentially on $1/\\\\epsilon$. We also give dynamic algorithms with polylogarithmic query and update times and the same approximation guarantees as the algorithms above. Key to our results is a new family of structured packings which we call easily guessable packings. They are flexible enough to guarantee profitable solutions and structured enough so that we can compute these solutions quickly.', 'Achieving nuanced and accurate emulation of human voice has been a longstanding goal in artificial intelligence. Although significant progress has been made in recent years, the mainstream of speech synthesis models still relies on supervised speaker modeling and explicit reference utterances. However, there are many aspects of human voice, such as emotion, intonation, and speaking style, for which it is hard to obtain accurate labels. In this paper, we propose VoxGenesis, a novel unsupervised speech synthesis framework that can discover a latent speaker manifold and meaningful voice editing directions without supervision. VoxGenesis is conceptually simple. Instead of mapping speech features to waveforms deterministically, VoxGenesis transforms a Gaussian distribution into speech distributions conditioned and aligned by semantic tokens. This forces the model to learn a speaker distribution disentangled from the semantic content. During the inference, sampling from the Gaussian distribution enables the creation of novel speakers with distinct characteristics. More importantly, the exploration of latent space uncovers human-interpretable directions associated with specific speaker characteristics such as gender attributes, pitch, tone, and emotion, allowing for voice editing by manipulating the latent codes along these identified directions. We conduct extensive experiments to evaluate the proposed VoxGenesis using both subjective and objective metrics, finding that it produces significantly more diverse and realistic speakers with distinct characteristics than the previous approaches. We also show that latent space manipulation produces consistent and human-identifiable effects that are not detrimental to the speech quality, which was not possible with previous approaches. Audio samples of VoxGenesis can be found at: \\\\url{this https URL}.', 'Language Models (LMs) such as BERT, have been shown to perform well on the task of identifying Named Entities (NE) in text. A BERT LM is typically used as a classifier to classify individual tokens in the input text, or to classify spans of tokens, as belonging to one of a set of possible NE categories.In this paper, we hypothesise that decoder-only Large Language Models (LLMs) can also be used generatively to extract both the NE, as well as potentially recover the correct surface form of the NE, where any spelling errors that were present in the input text get automatically corrected.We fine-tune two BERT LMs as baselines, as well as eight open-source LLMs, on the task of producing NEs from text that was obtained by applying Optical Character Recognition (OCR) to images of Japanese shop receipts; in this work, we do not attempt to find or evaluate the location of NEs in the text.We show that the best fine-tuned LLM performs as well as, or slightly better than, the best fine-tuned BERT LM, although the differences are not significant. However, the best LLM is also shown to correct OCR errors in some cases, as initially hypothesised.', 'In recent years, the global unemployment rate has remained persistently high. Compounding this issue, the ageing population in China often encounters additional challenges in finding employment due to prevalent age discrimination in daily life. However, with the advent of social media, there has been a rise in the popularity of short videos and live-streams for recruiting ageing workers. To better understand the motivations of ageing job seekers to engage with these video-based recruitment methods and to explore the extent to which such platforms can empower them, we conducted an interview-based study with ageing job seekers who have had exposure to these short recruitment videos and live-streaming channels. Our findings reveal that these platforms can provide a job-seeking choice that is particularly friendly to ageing job seekers, effectively improving their disadvantaged situation.', 'Data-oriented applications, their users, and even the law require data of high quality. Research has broken down the rather vague notion of data quality into various dimensions, such as accuracy, consistency, and reputation, to name but a few. To achieve the goal of high data quality, many tools and techniques exist to clean and otherwise improve data. Yet, systematic research on actually assessing data quality in all of its dimensions is largely absent, and with it the ability to gauge the success of any data cleaning effort. It is our vision to establish a systematic and comprehensive framework for the (numeric) assessment of data quality for a given dataset and its intended use. Such a framework must cover the various facets that influence data quality, as well as the many types of data quality dimensions. In particular, we identify five facets that serve as a foundation of data quality assessment. For each facet, we outline the challenges and opportunities that arise when trying to actually assign quality scores to data and create a data quality profile for it, along with a wide range of technologies needed for this purpose.', 'Large language models are built on top of a transformer-based architecture to process textual inputs. For example, the LLaMA stands out among many open-source implementations. Can the same transformer be used to process 2D images? In this paper, we answer this question by unveiling a LLaMA-like vision transformer in plain and pyramid forms, termed VisionLLaMA, which is tailored for this purpose. VisionLLaMA is a unified and generic modelling framework for solving most vision tasks. We extensively evaluate its effectiveness using typical pre-training paradigms in a good portion of downstream tasks of image perception and especially image generation. In many cases, VisionLLaMA have exhibited substantial gains over the previous state-of-the-art vision transformers. We believe that VisionLLaMA can serve as a strong new baseline model for vision generation and understanding. Our code will be released atthis https URL.', 'While interest in conversational recommender systems has been on the rise, operational systems suitable for serving as research platforms for comprehensive studies are currently lacking. This paper introduces an enhanced version of the IAI MovieBot conversational movie recommender system, aiming to evolve it into a robust and adaptable platform for conducting user-facing experiments. The key highlights of this enhancement include the addition of trainable neural components for natural language understanding and dialogue policy, transparent and explainable modeling of user preferences, along with improvements in the user interface and research infrastructure.', 'The electrification of public transport vehicles offers the potential to relieve city centers of pollutant and noise emissions. Furthermore, electric buses have lower life-cycle greenhouse gas (GHG) emissions than diesel buses, particularly when operated with sustainably produced electricity. However, the heating, ventilation, and air-conditioning (HVAC) system can consume a significant amount of energy, thus limiting the achievable driving range. In this paper, we address the HVAC system in an electric city bus by analyzing the trade-off between the energy consumption and the thermal comfort of the passengers. We do this by developing a dynamic thermal model for the bus cabin, which we simplify by considering it to be in steady state. We introduce a method that is able to quickly optimize the steady-state HVAC system inputs for a large number of samples representative of a year-round operation. A comparison between the results from the steady-state optimization approach and a dynamic simulation reveal small deviations in both the HVAC system power demand and achieved thermal comfort. Thus, the approximation of the system performance with a steady-state model is justified. We present two case studies to demonstrate the practical relevance of the approach. First, we show how the method can be used to compare different system designs based on a year-round performance evaluation. Second, we show how the method can be used to generate accurate setpoints for online controllers. In conclusion, this study shows that a steady-state analysis of the HVAC systems of an electric city bus is a valuable approach to evaluate and optimize its performance.', 'Function-as-a-Service (FaaS) is a promising edge computing execution model but requires secure sandboxing mechanisms to isolate workloads from multiple tenants on constrained infrastructure. Although Docker containers are lightweight and popular in open-source FaaS platforms, they are generally considered insufficient for executing untrusted code and providing sandbox isolation. Commercial cloud FaaS platforms thus rely on Linux microVMs or hardened container runtimes, which are secure but come with a higher resource footprint.Unikernels combine application code and limited operating system primitives into a single purpose appliance, reducing the footprint of an application and its sandbox while providing full Linux compatibility. In this paper, we study the suitability of unikernels as an edge FaaS execution environment using the Nanos and OSv unikernel tool chains. We compare performance along several metrics such as cold start overhead and idle footprint against sandboxes such as Firecracker Linux microVMs, Docker containers, and secure gVisor containers. We find that unikernels exhibit desirable cold start performance, yet lag behind Linux microVMs in stability. Nevertheless, we show that unikernels are a promising candidate for further research on Linux-compatible FaaS isolation.', 'Recent advancements in off-policy Reinforcement Learning (RL) have significantly improved sample efficiency, primarily due to the incorporation of various forms of regularization that enable more gradient update steps than traditional agents. However, many of these techniques have been tested in limited settings, often on tasks from single simulation benchmarks and against well-known algorithms rather than a range of regularization approaches. This limits our understanding of the specific mechanisms driving RL improvements. To address this, we implemented over 60 different off-policy agents, each integrating established regularization techniques from recent state-of-the-art algorithms. We tested these agents across 14 diverse tasks from 2 simulation benchmarks. Our findings reveal that while the effectiveness of a specific regularization setup varies with the task, certain combinations consistently demonstrate robust and superior performance. Notably, a simple Soft Actor-Critic agent, appropriately regularized, reliably solves dog tasks, which were previously solved mainly through model-based approaches.', 'Probing the memorization of large language models holds significant importance. Previous works have established metrics for quantifying memorization, explored various influencing factors, such as data duplication, model size, and prompt length, and evaluated memorization by comparing model outputs with training corpora. However, the training corpora are of enormous scale and its pre-processing is time-consuming. To explore memorization without accessing training data, we propose a novel approach, named ROME, wherein memorization is explored by comparing disparities across memorized and non-memorized. Specifically, models firstly categorize the selected samples into memorized and non-memorized groups, and then comparing the demonstrations in the two groups from the insights of text, probability, and hidden state. Experimental findings show the disparities in factors including word length, part-of-speech, word frequency, mean and variance, just to name a few.', 'In this work, we develop a pipeline for historical-psychological text analysis in classical Chinese. Humans have produced texts in various languages for thousands of years; however, most of the computational literature is focused on contemporary languages and corpora. The emerging field of historical psychology relies on computational techniques to extract aspects of psychology from historical corpora using new methods developed in natural language processing (NLP). The present pipeline, called Contextualized Construct Representations (CCR), combines expert knowledge in psychometrics (i.e., psychological surveys) with text representations generated via transformer-based language models to measure psychological constructs such as traditionalism, norm strength, and collectivism in classical Chinese corpora. Considering the scarcity of available data, we propose an indirect supervised contrastive learning approach and build the first Chinese historical psychology corpus (C-HI-PSY) to fine-tune pre-trained models. We evaluate the pipeline to demonstrate its superior performance compared with other approaches. The CCR method outperforms word-embedding-based approaches across all of our tasks and exceeds prompting with GPT-4 in most tasks. Finally, we benchmark the pipeline against objective, external data to further verify its validity.', \"The Potsdam Textbook Corpus (PoTeC) is a naturalistic eye-tracking-while-reading corpus containing data from 75 participants reading 12 scientific texts. PoTeC is the first naturalistic eye-tracking-while-reading corpus that contains eye-movements from domain-experts as well as novices in a within-participant manipulation: It is based on a 2x2x2 fully-crossed factorial design which includes the participants' level of study and the participants' discipline of study as between-subject factors and the text domain as a within-subject factor. The participants' reading comprehension was assessed by a series of text comprehension questions and their domain knowledge was tested by text-independent background questions for each of the texts. The materials are annotated for a variety of linguistic features at different levels. We envision PoTeC to be used for a wide range of studies including but not limited to analyses of expert and non-expert reading strategies. The corpus and all the accompanying data at all stages of the preprocessing pipeline and all code used to preprocess the data are made available via GitHub:this https URL.\", 'Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned representations, learning invariant representations such as contrastive methods, or equivariant representations such as masked image modelling.', 'Recent advances in LLMs have sparked a debate on whether they understand text. In this position paper, we argue that opponents in this debate hold different definitions for understanding, and particularly differ in their view on the role of consciousness. To substantiate this claim, we propose a thought experiment involving an open-source chatbot $Z$ which excels on every possible benchmark, seemingly without subjective experience. We ask whether $Z$ is capable of understanding, and show that different schools of thought within seminal AI research seem to answer this question differently, uncovering their terminological disagreement. Moving forward, we propose two distinct working definitions for understanding which explicitly acknowledge the question of consciousness, and draw connections with a rich literature in philosophy, psychology and neuroscience.', 'A recent paper describes a framework for studying the computational complexity of graph problems on monotone classes, that is those omitting a set of graphs as a subgraph. If the problems lie in the framework, and many do, then the computational complexity can be described for all monotone classes defined by a finite set of omitted subgraphs. It is known that certain homomorphism problems, e.g. $C_5$-Colouring, do not sit in the framework. By contrast, we show that the more general problem of Graph Homomorphism does sit in the framework.The original framework had examples where hard versus easy were NP-complete versus P, or at least quadratic versus almost linear. We give the first example of a problem in the framework such that hardness is in the polynomial hierarchy above NP. Considering a variant of the colouring game as studied by Bodlaender, we show that with the restriction of bounded alternation, the list version of this problem is contained in the framework. The hard cases are $\\\\Pi_{2k}^\\\\mathrm{P}$-complete and the easy cases are in P.The cases in P comprise those classes for which the pathwidth is bounded. Bodlaender explains that Sequential $3$-Colouring Construction Game is in P on classes with bounded vertex separation number, which coincides with bounded pathwidth on unordered graphs. However, these graphs are ordered with a playing order for the two players, which corresponds to a prefix pattern in a quantified formula. We prove that Sequential $3$-Colouring Construction Game is Pspace-complete on some class of bounded pathwidth, using a celebrated result of Atserias and Oliva.We consider several locally constrained variants of the homomorphism problem. Like $C_5$-Colouring, none of these is in the framework. However, when we consider the bounded-degree restrictions, we prove that each of these problems is in our framework.', 'Branching and weak probabilistic bisimilarities are two well-known notions capturing behavioral equivalence between nondeterministic probabilistic systems. For probabilistic systems, divergence is of major concern. Recently several divergence-sensitive refinements of branching and weak probabilistic bisimilarities have been proposed in the literature. Both the definitions of these equivalences and the techniques to investigate them differ significantly. This paper presents a comprehensive comparative study on divergence-sensitive behavioral equivalence relations that refine the branching and weak probabilistic bisimilarities. Additionally, these equivalence relations are shown to have efficient checking algorithms. The techniques of this paper might be of independent interest in a more general setting.', 'Despite the growth of physically assistive robotics (PAR) research over the last decade, nearly half of PAR user studies do not involve participants with the target disabilities. There are several reasons for this -- recruitment challenges, small sample sizes, and transportation logistics -- all influenced by systemic barriers that people with disabilities face. However, it is well-established that working with end-users results in technology that better addresses their needs and integrates with their lived circumstances. In this paper, we reflect on multiple approaches we have taken to working with people with motor impairments across the design, development, and evaluation of three PAR projects: (a) assistive feeding with a robot arm; (b) assistive teleoperation with a mobile manipulator; and (c) shared control with a robot arm. We discuss these approaches to working with users along three dimensions -- individual- vs. community-level insight, logistic burden on end-users vs. researchers, and benefit to researchers vs. community -- and share recommendations for how other PAR researchers can incorporate users into their work.', \"Stereo matching methods based on iterative optimization, like RAFT-Stereo and IGEV-Stereo, have evolved into a cornerstone in the field of stereo matching. However, these methods struggle to simultaneously capture high-frequency information in edges and low-frequency information in smooth regions due to the fixed receptive field. As a result, they tend to lose details, blur edges, and produce false matches in textureless areas. In this paper, we propose Selective Recurrent Unit (SRU), a novel iterative update operator for stereo matching. The SRU module can adaptively fuse hidden disparity information at multiple frequencies for edge and smooth regions. To perform adaptive fusion, we introduce a new Contextual Spatial Attention (CSA) module to generate attention maps as fusion weights. The SRU empowers the network to aggregate hidden disparity information across multiple frequencies, mitigating the risk of vital hidden disparity information loss during iterative processes. To verify SRU's universality, we apply it to representative iterative stereo matching methods, collectively referred to as Selective-Stereo. Our Selective-Stereo ranks $1^{st}$ on KITTI 2012, KITTI 2015, ETH3D, and Middlebury leaderboards among all published methods. Code is available atthis https URL.\"]\n"
     ]
    }
   ],
   "source": [
    "# ok 2\n",
    "abstracts_list = []\n",
    "session = requests.Session()  # 使用会话来保持持久连接\n",
    "\n",
    "for address in abstract_address_list:\n",
    "    print(address)\n",
    "    url = f\"https://arxiv.org/abs/{address}\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=600)\n",
    "        response.raise_for_status()  # 如果请求返回了一个错误状态，将会抛出异常\n",
    "        web_content = response.text\n",
    "        soup = BeautifulSoup(web_content, 'html.parser')\n",
    "        abstract = soup.find(\"blockquote\", class_=\"abstract mathjax\").get_text(strip=True)\n",
    "        abstract = abstract.replace('Abstract:', '').strip()\n",
    "        abstracts_list.append(abstract)\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error: {e}\")\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        print(f\"Connection Error: {e}\")\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        print(f\"Timeout Error: {e}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Other Error: {e}\")\n",
    "    time.sleep(1)  # 等待1秒再进行下一个请求，减缓爬取速度\n",
    "\n",
    "print(abstracts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "101fcb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "[{'title': ['Point Could Mamba: Point Cloud Learning via State Space Model'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Tao Zhang', 'Xiangtai Li', 'Haobo Yuan', 'Shunping Ji', 'Shuicheng Yan']}, {'title': ['Mitigating Reversal Curse via Semantic-aware Permutation Training'], 'subject': ['Computation and Language (cs.CL)'], \"authors' name\": ['Qingyan Guo', 'Rui Wang', 'Junliang Guo', 'Xu Tan', 'Jiang Bian', 'Yujiu Yang']}, {'title': ['An Experimental Study of Low-Latency Video Streaming over 5G'], 'subject': ['Multimedia (cs.MM)'], \"authors' name\": ['Imran Khan', 'Tuyen X. Tran', 'Matti Hiltunen', 'Theodore Karagioules', 'Dimitrios Koutsonikolas']}, {'title': ['AtP*: An efficient and scalable method for localizing LLM behaviour to  components'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['János Kramár', 'Tom Lieberum', 'Rohin Shah', 'Neel Nanda']}, {'title': ['Neural Acceleration of Incomplete Cholesky Preconditioners'], 'subject': ['Distributed, Parallel, and Cluster Computing (cs.DC)'], \"authors' name\": ['Joshua Dennis Booth', 'Hongyang Sun', 'Trevor Garnett']}, {'title': [\"Dialect prejudice predicts AI decisions about people's character,  employability, and criminality\"], 'subject': ['Computation and Language (cs.CL)'], \"authors' name\": ['Valentin Hofmann', 'Pratyusha Ria Kalluri', 'Dan Jurafsky', 'Sharese King']}, {'title': ['Happy Ending: An Empty Hexagon in Every Set of 30 Points'], 'subject': ['Computational Geometry (cs.CG)'], \"authors' name\": ['Marijn J.H. Heule', 'Manfred Scheucher']}, {'title': ['Can Transformers Capture Spatial Relations between Objects?'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Chuan Wen', 'Dinesh Jayaraman', 'Yang Gao']}, {'title': ['Cost-Effective Activity Control of Asymptomatic Carriers in Layered  Temporal Social Networks'], 'subject': ['Social and Information Networks (cs.SI)'], \"authors' name\": ['Masoumeh Moradian', 'Aresh Dadlani', 'Rasul Kairgeldin', 'Ahmad Khonsari']}, {'title': ['Few-Shot Relation Extraction with Hybrid Visual Evidence'], 'subject': ['Computation and Language (cs.CL)'], \"authors' name\": ['Jiaying Gong', 'Hoda Eldardiry']}, {'title': ['Subhomogeneous Deep Equilibrium Models'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Pietro Sittoni', 'Francesco Tudisco']}, {'title': ['MAIDR: Making Statistical Visualizations Accessible with Multimodal Data  Representation'], 'subject': ['Human-Computer Interaction (cs.HC)'], \"authors' name\": ['JooYoung Seo', 'Yilin Xia', 'Bongshin Lee', 'Sean McCurry', 'Yu Jun Yam']}, {'title': ['Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive  Ratio Analysis and Best-of-Both-Worlds'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Shinji Ito', 'Taira Tsuchiya', 'Junya Honda']}, {'title': ['Rethinking Inductive Biases for Surface Normal Estimation'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Gwangbin Bae', 'Andrew J. Davison']}, {'title': ['Representing Guardedness in Call-by-Value and Guarded Parametrized  Monads'], 'subject': ['Logic in Computer Science (cs.LO)'], \"authors' name\": ['Sergey Goncharov']}, {'title': ['Self-Consistent Decoding for More Factual Open Responses'], 'subject': ['Computation and Language (cs.CL)'], \"authors' name\": ['Christopher Malon', 'Xiaodan Zhu']}, {'title': ['Tri-Modal Motion Retrieval by Learning a Joint Embedding Space'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Kangning Yin', 'Shihao Zou', 'Yuxuan Ge', 'Zheng Tian']}, {'title': ['Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents'], 'subject': ['Artificial Intelligence (cs.AI)'], \"authors' name\": ['Dominik Jeurissen', 'Diego Perez-Liebana', 'Jeremy Gow', 'Duygu Cakmak', 'James Kwan']}, {'title': ['Hydra: Computer Vision for Data Quality Monitoring'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Thomas Britton', 'Torri Jeske', 'David Lawrence', 'Kishansingh Rajput']}, {'title': ['A Bit of a Problem: Measurement Disparities in Dataset Sizes Across  Languages'], 'subject': ['Computation and Language (cs.CL)'], \"authors' name\": ['Catherine Arnett', 'Tyler A. Chang', 'Benjamin K. Bergen']}, {'title': ['Know your exceptions: Towards an Ontology of Exceptions in Knowledge  Representation'], 'subject': ['Artificial Intelligence (cs.AI)'], \"authors' name\": ['Gabriele Sacco', 'Loris Bozzato', 'Oliver Kutz']}, {'title': ['An iterative method for the solution of Laplace-like equations in high  and very high space dimensions'], 'subject': ['Numerical Analysis (math.NA)'], \"authors' name\": ['Harry Yserentant']}, {'title': ['Scalable Learning of Item Response Theory Models'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Susanne Frick', 'Amer Krivošija', 'Alexander Munteanu']}, {'title': ['Reusing Historical Trajectories in Natural Policy Gradient via  Importance Sampling: Convergence and Convergence Rate'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Yifan Lin', 'Yuhao Wang', 'Enlu Zhou']}, {'title': ['Cell-Free Massive MIMO with Multi-Antenna Users and Phase Misalignments:  A Novel Partially Coherent Transmission Framework'], 'subject': ['Information Theory (cs.IT)'], \"authors' name\": ['Unnikrishnan Kunnath Ganesan', 'Tung Thanh Vu', 'Erik G. Larsson']}, {'title': ['Snapshot Reinforcement Learning: Leveraging Prior Trajectories for  Efficiency'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Yanxiao Zhao', 'Yangge Qian', 'Tianyi Wang', 'Jingyang Shan', 'Xiaolin Qin']}, {'title': ['Advancing Additive Manufacturing through Deep Learning: A Comprehensive  Review of Current Progress and Future Challenges'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Amirul Islam Saimon', 'Emmanuel Yangue', 'Xiaowei Yue', 'Zhenyu', 'Kong', 'Chenang Liu']}, {'title': ['Exploring Upper-6GHz and mmWave in Real-World 5G Networks: A Direct  on-Field Comparison'], 'subject': ['Networking and Internet Architecture (cs.NI)'], \"authors' name\": ['Marcello Morini', 'Eugenio Moro', 'Ilario Filippini', 'Antonio Capone', 'Danilo De Donno']}, {'title': ['Complex-Valued Neural Network based Federated Learning for Multi-user  Indoor Positioning Performance Optimization'], 'subject': ['Information Theory (cs.IT)'], \"authors' name\": ['Hanzhi Yu', 'Mingzhe Chen', 'Yuchen Liu']}, {'title': ['COLON: The largest COlonoscopy LONg sequence public database'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Lina Ruiz', 'Franklin Sierra-Jerez', 'Jair Ruiz', 'Fabio Martinez']}, {'title': ['Modeling the Quality of Dialogical Explanations'], 'subject': ['Computation and Language (cs.CL)'], \"authors' name\": ['Milad Alshomary', 'Felix Lange', 'Meisam Booshehri', 'Meghdut Sengupta', 'Philipp Cimiano', 'Henning Wachsmuth']}, {'title': ['Stability-Certified Learning of Control Systems with Quadratic  Nonlinearities'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Igor Pontes Duff', 'Pawan Goyal', 'Peter Benner']}, {'title': ['Event-Triggered Robust Cooperative Output Regulation for a Class of  Linear Multi-Agent Systems with an Unknown Exosystem'], 'subject': ['Systems and Control (eess.SY)'], \"authors' name\": ['Yangyang Qian', 'Lu Liu']}, {'title': ['Diff-Plugin: Revitalizing Details for Diffusion-based Low-level Tasks'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Yuhao Liu', 'Fang Liu', 'Zhanghan Ke', 'Nanxuan Zhao', 'Rynson W.H. Lau']}, {'title': ['Undercomplete Decomposition of Symmetric Tensors in Linear Time, and  Smoothed Analysis of the Condition Number'], 'subject': ['Data Structures and Algorithms (cs.DS)'], \"authors' name\": ['Pascal Koiran', 'Subhayan Saha']}, {'title': ['Rethinking The Uniformity Metric in Self-Supervised Learning'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Xianghong Fang', 'Jian Li', 'Qiang Sun', 'Benyou Wang']}, {'title': ['Robust Online Epistemic Replanning of Multi-Robot Missions'], 'subject': ['Robotics (cs.RO)'], \"authors' name\": ['Lauren Bramblett', 'Branko Miloradovic', 'Patrick Sherman', 'Alessandro V. Papadopoulos', 'Nicola Bezzo']}, {'title': ['Informed and Assessable Observability Design Decisions in Cloud-native  Microservice Applications'], 'subject': ['Software Engineering (cs.SE)'], \"authors' name\": ['Maria C. Borges', 'Joshua Bauer', 'Sebastian Werner', 'Michael Gebauer', 'Stefan Tai']}, {'title': ['Metamorpheus: Interactive, Affective, and Creative Dream Narration  Through Metaphorical Visual Storytelling'], 'subject': ['Human-Computer Interaction (cs.HC)'], \"authors' name\": ['Qian Wan', 'Xin Feng', 'Yining Bei', 'Zhiqi Gao', 'Zhicong Lu']}, {'title': ['Transforming Design Spaces Using Pareto-Laplace Filters'], 'subject': ['Computational Engineering, Finance, and Science (cs.CE)'], \"authors' name\": ['Hazhir Aliahmadi', 'Ruben Perez', 'Greg van Anders']}, {'title': ['Region-Adaptive Transform with Segmentation Prior for Image Compression'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Yuxi Liu', 'Wenhan Yang', 'Huihui Bai', 'Yunchao Wei', 'Yao Zhao']}, {'title': ['Bias Mitigation in Fine-tuning Pre-trained Models for Enhanced Fairness  and Efficiency'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Yixuan Zhang', 'Feng Zhou']}, {'title': ['Analysis of the particle relaxation method for generating uniform  particle distributions in smoothed particle hydrodynamics'], 'subject': ['Numerical Analysis (math.NA)'], \"authors' name\": ['Yu Fan', 'Xiaoliang Li', 'Shuoguo Zhang', 'Xiangyu Hu', 'Nikolaus A. Adams']}, {'title': ['Shortened Polar Codes under Automorphism Ensemble Decoding'], 'subject': ['Information Theory (cs.IT)'], \"authors' name\": ['Charles Pillet', 'Ilshat Sagitov', 'Valerio Bioglio', 'Pascal Giard']}, {'title': ['AdaBoost-Based Efficient Channel Estimation and Data Detection in  One-Bit Massive MIMO'], 'subject': ['Information Theory (cs.IT)'], \"authors' name\": ['Majdoddin Esfandiari', 'Sergiy A. Vorobyov', 'Robert W. Heath Jr']}, {'title': ['Complete and Near-Optimal Robotic Crack Coverage and Filling in Civil  Infrastructure'], 'subject': ['Robotics (cs.RO)'], \"authors' name\": ['Vishnu Veeraraghavan', 'Kyle Hunte', 'Jingang Yi', 'Kaiyan Yu']}, {'title': ['Probabilistic positioning via ray tracing with noisy angle of arrival  measurements'], 'subject': ['Information Theory (cs.IT)'], \"authors' name\": ['Vincent Corlay', 'Viet-Hoa Nguyen', 'Nicolas Gresset']}, {'title': ['Dynamic Operational Planning in Warfare: A Stochastic Game Approach to  Military Campaigns'], 'subject': ['Computer Science and Game Theory (cs.GT)'], \"authors' name\": ['Joseph E. McCarthy', 'Mathieu Dahan', 'Chelsea C. White III']}, {'title': ['Flattening Singular Values of Factorized Convolution for Medical Images'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Zexin Feng', 'Na Zeng', 'Jiansheng Fang', 'Xingyue Wang', 'Xiaoxi Lu', 'Heng Meng', 'Jiang Liu']}, {'title': ['Popularity and Perfectness in One-sided Matching Markets with Capacities'], 'subject': ['Computer Science and Game Theory (cs.GT)'], \"authors' name\": ['Gergely Csáji']}, {'title': ['Discrete minimizers of the interaction energy in collective behavior: a  brief numerical and analytic review'], 'subject': ['Numerical Analysis (math.NA)'], \"authors' name\": ['José A. Cañizo', 'Alejandro Ramos-Lora']}, {'title': ['Rethinking Few-shot 3D Point Cloud Semantic Segmentation'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Zhaochong An', 'Guolei Sun', 'Yun Liu', 'Fayao Liu', 'Zongwei Wu', 'Dan Wang', 'Luc Van Gool', 'Serge Belongie']}, {'title': ['Learning Causal Features for Incremental Object Detection'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Zhenwei He', 'Lei Zhang']}, {'title': ['Hercules: Heterogeneous Requirements Congestion Control Protocol'], 'subject': ['Networking and Internet Architecture (cs.NI)'], \"authors' name\": ['Neta Rozen-Schiff', 'Itzcak Pechtalt', 'Amit Navon', 'Leon Bruckman']}, {'title': ['Improving Explicit Spatial Relationships in Text-to-Image Generation  through an Automatically Derived Dataset'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Ander Salaberria', 'Gorka Azkune', 'Oier Lopez de Lacalle', 'Aitor Soroa', 'Eneko Agirre', 'Frank Keller']}, {'title': ['Open Assistant Toolkit -- version 2'], 'subject': ['Information Retrieval (cs.IR)'], \"authors' name\": ['Sophie Fischer', 'Federico Rossetto', 'Carlos Gemmell', 'Andrew Ramsay', 'Iain Mackie', 'Philip Zubel', 'Niklas Tecklenburg', 'Jeffrey Dalton']}, {'title': ['Decentralized Uncoded Storage Elastic Computing with Heterogeneous  Computation Speeds'], 'subject': ['Information Theory (cs.IT)'], \"authors' name\": ['Wenbo Huang', 'Xudong You', 'Kai Wan', 'Robert Caiming Qiu', 'Mingyue Ji']}, {'title': ['Generalized User Representations for Transfer Learning'], 'subject': ['Information Retrieval (cs.IR)'], \"authors' name\": ['Ghazal Fazelnia', 'Sanket Gupta', 'Claire Keum', 'Mark Koh', 'Ian Anderson', 'Mounia Lalmas']}, {'title': ['To Trust or Distrust Trust Measures: Validating Questionnaires for Trust  in AI'], 'subject': ['Human-Computer Interaction (cs.HC)'], \"authors' name\": ['Nicolas Scharowski', 'Sebastian A. C. Perrig', 'Lena Fanya Aeschbach', 'Nick von Felten', 'Klaus Opwis', 'Philipp Wintersberger', 'Florian Brühlmann']}, {'title': ['NeuPIMs: A NPU-PIM Heterogeneous Acceleration for Batched Inference of  Large Language Model'], 'subject': ['Hardware Architecture (cs.AR)'], \"authors' name\": ['Guseul Heo', 'Sangyeop Lee', 'Jaehong Cho', 'Hyunmin Choi', 'Sanghyeon Lee', 'Hyungkyu Ham', 'Gwangsun Kim', 'Divya Mahajan', 'Jongse Park']}, {'title': ['SINDy vs Hard Nonlinearities and Hidden Dynamics: a Benchmarking Study'], 'subject': ['Systems and Control (eess.SY)'], \"authors' name\": ['Aurelio Raffa Ugolini', 'Valentina Breschi', 'Andrea Manzoni', 'Mara Tanelli']}, {'title': ['Beyond Single-Model Views for Deep Learning: Optimization versus  Generalizability of Stochastic Optimization Algorithms'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Toki Tahmid Inan', 'Mingrui Liu', 'Amarda Shehu']}, {'title': ['IDTrust: Deep Identity Document Quality Detection with Bandpass  Filtering'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Musab Al-Ghadi', 'Joris Voerman', 'Souhail Bakkali', 'Mickaël Coustaty', 'Nicolas Sidere', 'Xavier St-Georges']}, {'title': ['Computational homogenization for aerogel-like polydisperse open-porous  materials using neural network--based surrogate models on the microscale'], 'subject': ['Numerical Analysis (math.NA)'], \"authors' name\": ['Axel Klawonn', 'Martin Lanser', 'Lucas Mager', 'Ameya Rege']}, {'title': ['Rethinking cluster-conditioned diffusion models'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Nikolas Adaloglou', 'Tim Kaiser', 'Felix Michels', 'Markus Kollmann']}, {'title': ['Flatten Long-Range Loss Landscapes for Cross-Domain Few-Shot Learning'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Yixiong Zou', 'Yicong Liu', 'Yiman Hu', 'Yuhua Li', 'Ruixuan Li']}, {'title': [\"Lincoln's Annotated Spatio-Temporal Strawberry Dataset (LAST-Straw)\"], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Katherine Margaret Frances James', 'Karoline Heiwolt', 'Daniel James Sargent', 'Grzegorz Cielniak']}, {'title': ['Predicting UAV Type: An Exploration of Sampling and Data Augmentation  for Time Series Classification'], 'subject': ['Robotics (cs.RO)'], \"authors' name\": ['Tarik Crnovrsanin', 'Calvin Yu', 'Dane Hankamer', 'Cody Dunne']}, {'title': ['EfficientZero V2: Mastering Discrete and Continuous Control with Limited  Data'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Shengjie Wang', 'Shaohuai Liu', 'Weirui Ye', 'Jiacheng You', 'Yang Gao']}, {'title': ['Indirectly Parameterized Concrete Autoencoders'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Alfred Nilsson', 'Klas Wijk', 'Sai bharath chandra Gutha', 'Erik Englesson', 'Alexandra Hotti', 'Carlo Saccardi', 'Oskar Kviman', 'Jens Lagergren', 'Ricardo Vinuesa', 'Hossein Azizpour']}, {'title': ['Multi-Task Learning Using Uncertainty to Weigh Losses for Heterogeneous  Face Attribute Estimation'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Huaqing Yuan', 'Yi He', 'Peng Du', 'Lu Song']}, {'title': ['Rational Linkages: From Poses to 3D-printed Prototypes'], 'subject': ['Robotics (cs.RO)'], \"authors' name\": ['Daniel Huczala', 'Johannes Siegele', 'Daren A. Thimm', 'Martin Pfurner', 'Hans-Peter Schröcker']}, {'title': ['Nearest-Neighbours Estimators for Conditional Mutual Information'], 'subject': ['Information Theory (cs.IT)'], \"authors' name\": ['Jake Witter', 'Conor Houghton']}, {'title': ['Distributed MPC for autonomous ships on inland waterways with  collaborative collision avoidance'], 'subject': ['Systems and Control (eess.SY)'], \"authors' name\": ['Hoang Anh Tran', 'Tor Arne Johansen', 'Rudy R. Negenborn']}, {'title': ['Standardizing the Measurement of Text Diversity: A Tool and a  Comparative Analysis of Scores'], 'subject': ['Computation and Language (cs.CL)'], \"authors' name\": ['Chantal Shaib', 'Joe Barrow', 'Jiuding Sun', 'Alexa F. Siu', 'Byron C. Wallace', 'Ani Nenkova']}, {'title': ['Imitation Learning Datasets: A Toolkit For Creating Datasets, Training  Agents and Benchmarking'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Nathan Gavenski', 'Michael Luck', 'Odinaldo Rodrigues']}, {'title': ['Comparative Study of Simulators for Vehicular Networks'], 'subject': ['Networking and Internet Architecture (cs.NI)'], \"authors' name\": ['Rida Saghir', 'Thenuka Karunathilake', 'Anna Förster']}, {'title': ['SURE: SUrvey REcipes for building reliable and robust deep networks'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Yuting Li', 'Yingyi Chen', 'Xuanlong Yu', 'Dexiong Chen', 'Xi Shen']}, {'title': ['Machine Learning Training Optimization using the Barycentric Correction  Procedure'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Sofia Ramos-Pulido', 'Neil Hernandez-Gress', 'Hector G. Ceballos-Cancino']}, {'title': ['Epsilon-Greedy Thompson Sampling to Bayesian Optimization'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Bach Do', 'Ruda Zhang']}, {'title': ['DyPyBench: A Benchmark of Executable Python Software'], 'subject': ['Software Engineering (cs.SE)'], \"authors' name\": ['Islem Bouzenia', 'Bajaj Piyush Krishan', 'Michael Pradel']}, {'title': ['Approximating the Geometric Knapsack Problem in Near-Linear Time and  Dynamically'], 'subject': ['Data Structures and Algorithms (cs.DS)'], \"authors' name\": ['Moritz Buchem', 'Paul Deuker', 'Andreas Wiese']}, {'title': ['VoxGenesis: Unsupervised Discovery of Latent Speaker Manifold for Speech  Synthesis'], 'subject': ['Sound (cs.SD)'], \"authors' name\": ['Weiwei Lin', 'Chenhang He', 'Man-Wai Mak', 'Jiachen Lian', 'Kong Aik Lee']}, {'title': ['Large Language Models for Simultaneous Named Entity Extraction and  Spelling Correction'], 'subject': ['Computation and Language (cs.CL)'], \"authors' name\": ['Edward Whittaker', 'Ikuo Kitagishi']}, {'title': ['\"There is a Job Prepared for Me Here\": Understanding How Short Video and  Live-streaming Platforms Empower Ageing Job Seekers in China'], 'subject': ['Human-Computer Interaction (cs.HC)'], \"authors' name\": ['PiaoHong Wang', 'Siying Hu', 'Bo Wen', 'Zhicong Lu']}, {'title': ['Data Quality Assessment: Challenges and Opportunities'], 'subject': ['Databases (cs.DB)'], \"authors' name\": ['Sedir Mohammed', 'Hazar Harmouch', 'Felix Naumann', 'Divesh Srivastava']}, {'title': ['VisionLLaMA: A Unified LLaMA Interface for Vision Tasks'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Xiangxiang Chu', 'Jianlin Su', 'Bo Zhang', 'Chunhua Shen']}, {'title': ['IAI MovieBot 2.0: An Enhanced Research Platform with Trainable Neural  Components and Transparent User Modeling'], 'subject': ['Information Retrieval (cs.IR)'], \"authors' name\": ['Nolwenn Bernard', 'Ivica Kostric', 'Krisztian Balog']}, {'title': ['Optimization of the Energy-Comfort Trade-Off of HVAC Systems in Electric  City Buses Based on a Steady-State Model'], 'subject': ['Systems and Control (eess.SY)'], \"authors' name\": ['Fabio Widmer', 'Stijn van Dooren', 'Christopher H. Onder']}, {'title': ['Are Unikernels Ready for Serverless on the Edge?'], 'subject': ['Distributed, Parallel, and Cluster Computing (cs.DC)'], \"authors' name\": ['Felix Moebius', 'Tobias Pfandzelter', 'David Bermbach']}, {'title': ['Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter  Lesson of Reinforcement Learning'], 'subject': ['Machine Learning (cs.LG)'], \"authors' name\": ['Michal Nauman', 'Michał Bortkiewicz', 'Mateusz Ostaszewski', 'Piotr Miłoś', 'Tomasz Trzciński', 'Marek Cygan']}, {'title': ['ROME: Memorization Insights from Text, Probability and Hidden State in  Large Language Models'], 'subject': ['Computation and Language (cs.CL)'], \"authors' name\": ['Bo Li', 'Qinghua Zhao', 'Lijie Wen']}, {'title': ['Surveying the Dead Minds: Historical-Psychological Text Analysis with  Contextualized Construct Representation (CCR) for Classical Chinese'], 'subject': ['Computation and Language (cs.CL)'], \"authors' name\": ['Yuqi Chen', 'Sixuan Li', 'Ying Li', 'Mohammad Atari']}, {'title': ['PoTeC: A German Naturalistic Eye-tracking-while-reading Corpus'], 'subject': ['Computation and Language (cs.CL)'], \"authors' name\": ['Deborah N. Jakobi', 'Thomas Kern', 'David R. Reich', 'Patrick Haller', 'Lena A. Jäger']}, {'title': ['Learning and Leveraging World Models in Visual Representation Learning'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Quentin Garrido', 'Mahmoud Assran', 'Nicolas Ballas', 'Adrien Bardes', 'Laurent Najman', 'Yann LeCun']}, {'title': ['Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of  Machine Cognition'], 'subject': ['Computation and Language (cs.CL)'], \"authors' name\": ['Ariel Goldstein', 'Gabriel Stanovsky']}, {'title': ['Graph Homomorphism, Monotone Classes and Bounded Pathwidth'], 'subject': ['Computational Complexity (cs.CC)'], \"authors' name\": ['Tala Eagling-Vose', 'Barnaby Martin', 'Daniel Paulusma', 'Mark Siggers', 'Siani Smith']}, {'title': ['Analyzing Divergence for Nondeterministic Probabilistic Models'], 'subject': ['Logic in Computer Science (cs.LO)'], \"authors' name\": ['Hao Wu', 'Yuxi Fu', 'Huan Long', 'Xian Xu', 'Wenbo Zhang']}, {'title': ['Multiple Ways of Working with Users to Develop Physically Assistive  Robots'], 'subject': ['Human-Computer Interaction (cs.HC)'], \"authors' name\": ['Amal Nanavati', 'Max Pascher', 'Vinitha Ranganeni', 'Ethan K. Gordon', 'Taylor Kessler Faulkner', 'Siddhartha S. Srinivasa', 'Maya Cakmak', 'Patrícia Alves-Oliveira', 'Jens Gerken']}, {'title': ['Selective-Stereo: Adaptive Frequency Information Selection for Stereo  Matching'], 'subject': ['Computer Vision and Pattern Recognition (cs.CV)'], \"authors' name\": ['Xianqi Wang', 'Gangwei Xu', 'Hao Jia', 'Xin Yang']}] 100\n"
     ]
    }
   ],
   "source": [
    "list = [] # 储存所有paper的信息，里面每个元素均是字典\n",
    "\n",
    "url = f'https://arxiv.org/list/cs/pastweek?skip=0&show=100'\n",
    "# 发送请求获取网页内容\n",
    "response = requests.get(url, headers = headers, timeout=600)\n",
    "    \n",
    "print(response.status_code)\n",
    "    \n",
    "web_content = response.text\n",
    "# 使用BeautifulSoup解析网页内容\n",
    "soup = BeautifulSoup(web_content, 'html.parser')\n",
    "    \n",
    "papers = soup.find_all('div', attrs={\"class\": \"meta\"})\n",
    "    \n",
    "for paper in papers: # 遍历每一篇paper\n",
    "        \n",
    "    titles_list = []\n",
    "    subjects_list = []\n",
    "    authors_list = []\n",
    "    paper_dict = {} # 创建一个字典，储存每个paper的title，subject，authors\n",
    "\n",
    "\n",
    "    titles= paper.find_all('div', class_='list-title mathjax')\n",
    "    subjects = paper.find_all(\"span\", attrs={\"class\": \"primary-subject\"})\n",
    "    # authors = soup.find_all('a', href=True)\n",
    "    authors = paper.find_all('div', class_='list-authors') # 首先找到包含作者名字的div\n",
    "\n",
    "    for title in titles:\n",
    "        title = title.text\n",
    "        title = title.replace('Title:', '').strip()\n",
    "        titles_list.append(title)\n",
    "            \n",
    "    for subject in subjects:\n",
    "        subject = subject.text\n",
    "        subjects_list.append(subject)\n",
    "        \n",
    "    for authors in authors: # 遍历所有的<a>标签，并打印出文本内容\n",
    "        authors = authors.find_all('a', href=True) # 在这个div内找到所有的<a>标签\n",
    "        for author in authors:\n",
    "            author_name = author.get_text(strip=True) # 获取文本内容，并使用strip()方法移除字符串首尾的空格\n",
    "            authors_list.append(author_name)\n",
    "        \n",
    "    paper_dict = {\"title\": titles_list, \"subject\": subjects_list, \"authors' name\": authors_list}\n",
    "    list.append(paper_dict)\n",
    "        \n",
    "    # 每爬取一篇paper休息一秒\n",
    "    time.sleep(1)\n",
    "    print(\"success\")\n",
    "        \n",
    "print(list, len(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bef88311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "# 指定CSV文件名\n",
    "filename = 'papers.csv'\n",
    "\n",
    "# 打开文件，准备写入\n",
    "with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    # 创建CSV写入器\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # 写入标题行\n",
    "    writer.writerow(['Title', \"subject\", 'Author', 'Abstract'])\n",
    "    for i in range(0,100,1):\n",
    "        print(i)\n",
    "        # 写入数据\n",
    "        writer.writerow([list[i][\"title\"], list[i][\"subject\"], list[i][\"authors' name\"], abstracts_list[i]])\n",
    "\n",
    "# 记得要关闭csv，释放线程\n",
    "# 代码未完整\n",
    "# filename.close"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
