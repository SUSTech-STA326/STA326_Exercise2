{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90901508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests  # send request\n",
    "from bs4 import BeautifulSoup  # parse web pages\n",
    "import pandas as pd  # csv\n",
    "from time import sleep  # wait\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import csv\n",
    "from urllib.parse import urljoin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3447e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a00ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://arxiv.org/list/cs/pastweek?skip=0&show=25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf744d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(r.text,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca09da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_list = soup.find_all('span', class_='list-identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fa0419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_info = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b981874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract:Many online content portals allow users to ask questions to supplement their understanding (e.g., of lectures). While information retrieval (IR) systems may provide answers for such user queries, they do not directly assist content creators -- such as lecturers who want to improve their content -- identify segments that _caused_ a user to ask those questions. We introduce the task of backtracing, in which systems retrieve the text segment that most likely caused a user query. We formalize three real-world domains for which backtracing is important in improving content delivery and communication: understanding the cause of (a) student confusion in the Lecture domain, (b) reader curiosity in the News Article domain, and (c) user emotion in the Conversation domain. We evaluate the zero-shot performance of popular information retrieval methods and language modeling methods, including bi-encoder, re-ranking and likelihood-based methods and ChatGPT. While traditional IR systems retrieve semantically relevant information (e.g., details on \"projection matrices\" for a query \"does projecting multiple times still lead to the same point?\"), they often miss the causally relevant context (e.g., the lecturer states \"projecting twice gets me the same answer as one projection\"). Our results show that there is room for improvement on backtracing and it requires new retrieval approaches. We hope our benchmark serves to improve future retrieval systems for backtracing, spawning systems that refine content generation and identify linguistic triggers influencing user queries. Our code and data are open-sourced: this https URL.\n",
      "Abstract:Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 55.3% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in diverse aspects, including space, viewpoint, appearance, and instance. Interestingly, in real robot experiments, DP3 rarely violates safety requirements, in contrast to baseline methods which frequently do, necessitating human intervention. Our extensive evaluation highlights the critical importance of 3D representations in real-world robot learning. Videos, code, and data are available on this https URL .\n",
      "Abstract:This paper introduces BLaIR, a series of pretrained sentence embedding models specialized for recommendation scenarios. BLaIR is trained to learn correlations between item metadata and potential natural language context, which is useful for retrieving and recommending items. To pretrain BLaIR, we collect Amazon Reviews 2023, a new dataset comprising over 570 million reviews and 48 million items from 33 categories, significantly expanding beyond the scope of previous versions. We evaluate the generalization ability of BLaIR across multiple domains and tasks, including a new task named complex product search, referring to retrieving relevant items given long, complex natural language contexts. Leveraging large language models like ChatGPT, we correspondingly construct a semi-synthetic evaluation set, Amazon-C4. Empirical results on the new task, as well as conventional retrieval and recommendation tasks, demonstrate that BLaIR exhibit strong text and item representation capacity. Our datasets, code, and checkpoints are available at: this https URL.\n",
      "Abstract:Value functions are a central component of deep reinforcement learning (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains. These include: single-task RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving state-of-the-art results on these domains. Through careful analysis, we show that the benefits of categorical cross-entropy primarily stem from its ability to mitigate issues inherent to value-based RL, such as noisy targets and non-stationarity. Overall, we argue that a simple shift to training value functions with categorical cross-entropy can yield substantial improvements in the scalability of deep RL at little-to-no cost.\n",
      "Abstract:Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in \"digital twin\" simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel \"inverse distillation\" procedure for bringing real-world demonstrations into simulated environments for efficient fine-tuning, with minimal human intervention and engineering required. We evaluate RialTo across a variety of robotic manipulation problems in the real world, such as robustly stacking dishes on a rack, placing books on a shelf, and six other tasks. RialTo increases (over 67%) in policy robustness without requiring extensive human data collection. Project website and videos at this https URL\n",
      "Abstract:Automatically estimating the performance difficulty of a music piece represents a key process in music education to create tailored curricula according to the individual needs of the students. Given its relevance, the Music Information Retrieval (MIR) field depicts some proof-of-concept works addressing this task that mainly focuses on high-level music abstractions such as machine-readable scores or music sheet images. In this regard, the potential of directly analyzing audio recordings has been generally neglected, which prevents students from exploring diverse music pieces that may not have a formal symbolic-level transcription. This work pioneers in the automatic estimation of performance difficulty of music pieces on audio recordings with two precise contributions: (i) the first audio-based difficulty estimation dataset -- namely, Piano Syllabus (PSyllabus) dataset -- featuring 7,901 piano pieces across 11 difficulty levels from 1,233 composers; and (ii) a recognition framework capable of managing different input representations -- both unimodal and multimodal manners -- directly derived from audio to perform the difficulty estimation task. The comprehensive experimentation comprising different pre-training schemes, input modalities, and multi-task scenarios prove the validity of the proposal and establishes PSyllabus as a reference dataset for audio-based difficulty estimation in the MIR field. The dataset as well as the developed code and trained models are publicly shared to promote further research in the field.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Abstract:Federated learning is a popular framework for collaborative machine learning where multiple clients only share gradient updates on their local data with the server and not the actual data. Unfortunately, it was recently shown that gradient inversion attacks can reconstruct this data from these shared gradients. Existing attacks enable exact reconstruction only for a batch size of $b=1$ in the important honest-but-curious setting, with larger batches permitting only approximate reconstruction. In this work, we propose \\emph{the first algorithm reconstructing whole batches with $b >1$ exactly}. This approach combines mathematical insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected networks and show that it recovers batches of $b \\lesssim 25$ elements exactly while being tractable for large network widths and depths.\n",
      "Abstract:Prior work has found that pretrained language models (LMs) fine-tuned with different random seeds can achieve similar in-domain performance but generalize differently on tests of syntactic generalization. In this work, we show that, even within a single model, we can find multiple subnetworks that perform similarly in-domain, but generalize vastly differently. To better understand these phenomena, we investigate if they can be understood in terms of \"competing subnetworks\": the model initially represents a variety of distinct algorithms, corresponding to different subnetworks, and generalization occurs when it ultimately converges to one. This explanation has been used to account for generalization in simple algorithmic tasks. Instead of finding competing subnetworks, we find that all subnetworks -- whether they generalize or not -- share a set of attention heads, which we refer to as the heuristic core. Further analysis suggests that these attention heads emerge early in training and compute shallow, non-generalizing features. The model learns to generalize by incorporating additional attention heads, which depend on the outputs of the \"heuristic\" heads to compute higher-level features. Overall, our results offer a more detailed picture of the mechanisms for syntactic generalization in pretrained LMs.\n",
      "Abstract:We introduce GUIDE, a novel continual learning approach that directs diffusion models to rehearse samples at risk of being forgotten. Existing generative strategies combat catastrophic forgetting by randomly sampling rehearsal examples from a generative model. Such an approach contradicts buffer-based approaches where sampling strategy plays an important role. We propose to bridge this gap by integrating diffusion models with classifier guidance techniques to produce rehearsal examples specifically targeting information forgotten by a continuously trained model. This approach enables the generation of samples from preceding task distributions, which are more likely to be misclassified in the context of recently encountered classes. Our experimental results show that GUIDE significantly reduces catastrophic forgetting, outperforming conventional random sampling approaches and surpassing recent state-of-the-art methods in continual learning with generative replay.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 2\n",
      "Abstract:The competition complexity of an auction setting is the number of additional bidders needed such that the simple mechanism of selling items separately (with additional bidders) achieves greater revenue than the optimal but complex (randomized, prior-dependent, Bayesian-truthful) optimal mechanism without the additional bidders. Our main result settles the competition complexity of $n$ bidders with additive values over $m < n$ independent items at $\\Theta(\\sqrt{nm})$. The $O(\\sqrt{nm})$ upper bound is due to [BW19], and our main result improves the prior lower bound of $\\Omega(\\ln n)$ to $\\Omega(\\sqrt{nm})$.\n",
      "Our main result follows from an explicit construction of a Bayesian IC auction for $n$ bidders with additive values over $m<n$ independent items drawn from the Equal Revenue curve truncated at $\\sqrt{nm}$ ($\\mathcal{ER}_{\\le \\sqrt{nm}}$), which achieves revenue that exceeds $\\text{SRev}_{n+\\sqrt{nm}}(\\mathcal{ER}_{\\le \\sqrt{nm}}^m)$.\n",
      "Along the way, we show that the competition complexity of $n$ bidders with additive values over $m$ independent items is exactly equal to the minimum $c$ such that $\\text{SRev}_{n+c}(\\mathcal{ER}_{\\le p}^m) \\geq \\text{Rev}_n(\\mathcal{ER}_{\\le p}^m)$ for all $p$ (that is, some truncated Equal Revenue witnesses the worst-case competition complexity). Interestingly, we also show that the untruncated Equal Revenue curve does not witness the worst-case competition complexity when $n > m$: $\\text{SRev}_n(\\mathcal{ER}^m) = nm+O_m(\\ln (n)) \\leq \\text{SRev}_{n+O_m(\\ln (n))}(\\mathcal{ER}^m)$, and therefore our result can only follow by considering all possible truncations.\n",
      "Abstract:This paper explores the complex relationship between demographics and artificial intelligence (AI) advances in Europe and Africa, projecting into the year 2050. The advancement of AI technologies has occurred at diverse rates, with Africa lagging behind Europe. Moreover, the imminent economic consequences of demographic shifts require a more careful examination of immigration patterns, with Africa emerging as a viable labor pool for European countries. However, within these dynamics, questions are raised about the differences in AI proficiency between African immigrants and Europeans by 2050. This paper examines demographic trends and AI developments to unravel insights into the multifaceted challenges and opportunities that lie ahead in the realms of technology, the economy, and society as we look ahead to 2050.\n",
      "Abstract:An open stochastic system à la Willems is a system affected two qualitatively different kinds of uncertainty -- one is probabilistic fluctuation, and the other one is nondeterminism caused by lack of information. We give a formalization of open stochastic systems in the language of category theory. A new construction, which we term copartiality, is needed to model the propagating lack of information (which corresponds to varying sigma-algebras).\n",
      "As a concrete example, we discuss extended Gaussian distributions, which combine Gaussian probability with nondeterminism and correspond precisely to Willems' notion of Gaussian linear systems. We describe them both as measure-theoretic and abstract categorical entities, which enables us to rigorously describe a variety of phenomena like noisy physical laws and uninformative priors in Bayesian statistics. The category of extended Gaussian maps can be seen as a mutual generalization of Gaussian probability and linear relations, which connects the literature on categorical probability with ideas from control theory like signal-flow diagrams.\n",
      "Abstract:For every $n >0$, we show the existence of a CNF tautology over $O(n^2)$ variables of width $O(\\log n)$ such that it has a Polynomial Calculus Resolution refutation over $\\{0,1\\}$ variables of size $O(n^3polylog(n))$ but any Polynomial Calculus refutation over $\\{+1,-1\\}$ variables requires size $2^{\\Omega(n)}$. This shows that Polynomial Calculus sizes over the $\\{0,1\\}$ and $\\{+1,-1\\}$ bases are incomparable (since Tseitin tautologies show a separation in the other direction) and answers an open problem posed by Sokolov [Sok20] and Razborov.\n",
      "Abstract:This paper presents an innovative approach to extreme precipitation nowcasting by employing Transformer-based generative models, namely NowcastingGPT with Extreme Value Loss (EVL) regularization. Leveraging a comprehensive dataset from the Royal Netherlands Meteorological Institute (KNMI), our study focuses on predicting short-term precipitation with high accuracy. We introduce a novel method for computing EVL without assuming fixed extreme representations, addressing the limitations of current models in capturing extreme weather events. We present both qualitative and quantitative analyses, demonstrating the superior performance of the proposed NowcastingGPT-EVL in generating accurate precipitation forecasts, especially when dealing with extreme precipitation events. The code is available at \\url{this https URL}.\n",
      "Abstract:Neural machine translation (MT) models achieve strong results across a variety of settings, but it is widely believed that they are highly sensitive to \"noisy\" inputs, such as spelling errors, abbreviations, and other formatting issues. In this paper, we revisit this insight in light of recent multilingual MT models and large language models (LLMs) applied to machine translation. Somewhat surprisingly, we show through controlled experiments that these models are far more robust to many kinds of noise than previous models, even when they perform similarly on clean data. This is notable because, even though LLMs have more parameters and more complex training processes than past models, none of the open ones we consider use any techniques specifically designed to encourage robustness. Next, we show that similar trends hold for social media translation experiments -- LLMs are more robust to social media text. We include an analysis of the circumstances in which source correction techniques can be used to mitigate the effects of noise. Altogether, we show that robustness to many types of noise has increased.\n",
      "Abstract:This paper explores the transformative potential of computer-assisted textual analysis in enhancing instructional quality through in-depth insights from educational artifacts. We integrate Richard Elmore's Instructional Core Framework to examine how artificial intelligence (AI) and machine learning (ML) methods, particularly natural language processing (NLP), can analyze educational content, teacher discourse, and student responses to foster instructional improvement. Through a comprehensive review and case studies within the Instructional Core Framework, we identify key areas where AI/ML integration offers significant advantages, including teacher coaching, student support, and content development. We unveil patterns that indicate AI/ML not only streamlines administrative tasks but also introduces novel pathways for personalized learning, providing actionable feedback for educators and contributing to a richer understanding of instructional dynamics. This paper emphasizes the importance of aligning AI/ML technologies with pedagogical goals to realize their full potential in educational settings, advocating for a balanced approach that considers ethical considerations, data quality, and the integration of human expertise.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 2\n",
      "Abstract:Motivated by empirical research on bias and opinion formation, we introduce a novel multidimensional nonlinear opinion dynamical model where agents have individual biases, which are fixed, as well as opinions, which evolve. The dimensions are coupled through a normalization step, which is also the source of the nonlinearity, so that the state describes an agent's relative opinion of various options. This can capture, for example, an individual's relative trust in different media. In special cases including where biases are uniform across agents our model achieves consensus, but in general, behaviors are richer and capture multipolar opinion distributions. We examine general fixed points of the system, as well as special cases such as zero biases toward certain options or partitioned decision sets. Lastly, we demonstrate that our model exhibits polarization when biases are spatially correlated across the network, while, as empirical research suggests, a mixed community can mediate biases.\n",
      "Abstract:Lithium-ion battery packs demand effective active equalization systems to enhance their usable capacity and lifetime. Despite numerous topologies and control schemes proposed in the literature, conducting quantitative analyses, comprehensive comparisons, and systematic optimization of their performance remains challenging due to the absence of a unified mathematical model at the pack level. To address this gap, we introduce a novel, hypergraph-based approach to establish the first unified model for various active battery equalization systems. This model reveals the intrinsic relationship between battery cells and equalizers by representing them as the vertices and hyperedges of hypergraphs, respectively. With the developed model, we identify the necessary condition for all equalization systems to achieve balance through controllability analysis, offering valuable insights for selecting the number of equalizers. Moreover, we prove that the battery equalization time is inversely correlated with the second smallest eigenvalue of the hypergraph's Laplacian matrix of each equalization system. This significantly simplifies the selection and optimized design of equalization systems, obviating the need for extensive experiments or simulations to derive the equalization time. Illustrative results demonstrate the efficiency of the proposed model and validate our findings.\n",
      "Abstract:Typologically diverse benchmarks are increasingly created to track the progress achieved in multilingual NLP. Linguistic diversity of these data sets is typically measured as the number of languages or language families included in the sample, but such measures do not consider structural properties of the included languages. In this paper, we propose assessing linguistic diversity of a data set against a reference language sample as a means of maximising linguistic diversity in the long run. We represent languages as sets of features and apply a version of the Jaccard index suitable for comparing sets of measures. In addition to the features extracted from typological data bases, we propose an automatic text-based measure, which can be used as a means of overcoming the well-known problem of data sparsity in manually collected features. Our diversity score is interpretable in terms of linguistic features and can identify the types of languages that are not represented in a data set. Using our method, we analyse a range of popular multilingual data sets (UD, Bible100, mBERT, XTREME, XGLUE, XNLI, XCOPA, TyDiQA, XQuAD). In addition to ranking these data sets, we find, for example, that (poly)synthetic languages are missing in almost all of them.\n",
      "Abstract:In his 2018 paper, Herlihy introduced an atomic protocol for multi-party asset swaps across different blockchains. His model represents an asset swap by a directed graph whose nodes are the participating parties and edges represent asset transfers, and rational behavior of the participants is captured by a preference relation between a protocol's outcomes. Asset transfers between parties are achieved using smart contracts. These smart contracts are quite involved and they require storage and processing of a large number of paths in the swap digraph, limiting practical significance of his protocol. His paper also describes a different protocol that uses only standard hash time-lock contracts (HTLC's), but this simpler protocol applies only to some special types of digraphs. He left open the question whether there is a simple and efficient protocol for cross-chain asset swaps in arbitrary digraphs. Motivated by this open problem, we conducted a comprehensive study of \\emph{HTLC-based protocols}, in which all asset transfers are implemented with HTLCs. Our main contribution is a full characterization of swap digraphs that have such protocols.\n",
      "Abstract:The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. Motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood.\n",
      "Our main contribution is significantly sharper bounds on the approximation parameter degradation of deflation methods for $k$-PCA. For a quadratic form notion of approximation we term ePCA (energy PCA), we show deflation methods suffer no parameter loss. For an alternative well-studied approximation notion we term cPCA (correlation PCA), we tightly characterize the parameter regimes where deflation methods are feasible. Moreover, we show that in all feasible regimes, $k$-cPCA deflation algorithms suffer no asymptotic parameter loss for any constant $k$. We apply our framework to obtain state-of-the-art $k$-PCA algorithms robust to dataset contamination, improving prior work both in sample complexity and approximation quality.\n",
      "Abstract:In this study, we explore advanced strategies for enhancing software quality by detecting and refactoring data clumps, special types of code smells. Our approach transcends the capabilities of integrated development environments, utilizing a novel method that separates the detection of data clumps from the source access. This method facilitates data clump processing. We introduce a command-line interface plugin to support this novel method of processing data clumps. This research highlights the efficacy of modularized algorithms and advocates their integration into continuous workflows, promising enhanced code quality and efficient project management across various programming and integrated development environments.\n",
      "Abstract:Sequential recommendation aims to estimate the dynamic user preferences and sequential dependencies among historical user behaviors. Although Transformer-based models have proven to be effective for sequential recommendation, they suffer from the inference inefficiency problem stemming from the quadratic computational complexity of attention operators, especially for long-range behavior sequences. Inspired by the recent success of state space models (SSMs), we propose Mamba4Rec, which is the first work to explore the potential of selective SSMs for efficient sequential recommendation. Built upon the basic Mamba block which is a selective SSM with an efficient hardware-aware parallel algorithm, we incorporate a series of sequential modeling techniques to further promote the model performance and meanwhile maintain the inference efficiency. Experiments on two public datasets demonstrate that Mamba4Rec is able to well address the effectiveness-efficiency dilemma, and defeat both RNN- and attention-based baselines in terms of both effectiveness and efficiency.\n",
      "Abstract:Accurate electrical load forecasting is of great importance for the efficient operation and control of modern power systems. In this work, a hybrid long short-term memory (LSTM)-based model with online correction is developed for day-ahead electrical load forecasting. Firstly, four types of features are extracted from the original electrical load dataset, including the historical time series, time index features, historical statistical features, and similarity features. Then, a hybrid LSTM-based electrical load forecasting model is designed, where an LSTM neural network block and a fully-connected neural network block are integrated that can model both temporal features (historical time series) and non-temporal features (the rest features). A gradient regularization-based offline training algorithm and an output layer parameter fine-tuning-based online model correction method are developed to enhance the model's capabilities to defend against disturbance and adapt to the latest load data distribution, thus improving the forecasting accuracy. At last, extensive experiments are carried out to validate the effectiveness of the proposed electrical load forecasting strategy with superior accuracy compared with commonly used forecasting models.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 2\n",
      "Abstract:BusyBox, an open-source software bundling over 300 essential Linux commands into a single executable, is ubiquitous in Linux-based embedded devices. Vulnerabilities in BusyBox can have far-reaching consequences, affecting a wide array of devices. This research, driven by the extensive use of BusyBox, delved into its analysis. The study revealed the prevalence of older BusyBox versions in real-world embedded products, prompting us to conduct fuzz testing on BusyBox. Fuzzing, a pivotal software testing method, aims to induce crashes that are subsequently scrutinized to uncover vulnerabilities. Within this study, we introduce two techniques to fortify software testing. The first technique enhances fuzzing by leveraging Large Language Models (LLM) to generate target-specific initial seeds. Our study showed a substantial increase in crashes when using LLM-generated initial seeds, highlighting the potential of LLM to efficiently tackle the typically labor-intensive task of generating target-specific initial seeds. The second technique involves repurposing previously acquired crash data from similar fuzzed targets before initiating fuzzing on a new target. This approach streamlines the time-consuming fuzz testing process by providing crash data directly to the new target before commencing fuzzing. We successfully identified crashes in the latest BusyBox target without conducting traditional fuzzing, emphasizing the effectiveness of LLM and crash reuse techniques in enhancing software testing and improving vulnerability detection in embedded systems. Additionally, manual triaging was performed to identify the nature of crashes in the latest BusyBox.\n",
      "No failed requests to retry.\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# from time import sleep\n",
    "# from requests.exceptions import Timeout, RequestException\n",
    "\n",
    "# # 定义最大重试次数\n",
    "# max_retries = 3\n",
    "\n",
    "# for identifier in identifier_list:\n",
    "#     link_list = identifier.find('a', {'title': 'Abstract'})\n",
    "#     abs_url = link_list['href']\n",
    "#     abs_full_url = urljoin(\"https://arxiv.org\", abs_url)\n",
    "    \n",
    "#     # 初始化重试次数\n",
    "#     retry_count = 0\n",
    "#     while retry_count < max_retries:\n",
    "#         try:\n",
    "#             response = requests.get(abs_full_url, timeout=20)  \n",
    "#             sleep(2)  # 在每次请求之后等待2秒钟\n",
    "#             response.raise_for_status()  # 检查请求是否成功\n",
    "#             break  # 请求成功，跳出循环\n",
    "#         except (Timeout, RequestException) as e:\n",
    "#             print(f\"Request error: {e}\")\n",
    "#             retry_count += 1\n",
    "#             if retry_count == max_retries:\n",
    "#                 print(\"Max retries reached, unable to fetch data\")\n",
    "#                 break  # 达到最大重试次数，跳出循环\n",
    "#             else:\n",
    "#                 print(f\"Retrying... Attempt {retry_count}\")\n",
    "#                 sleep(2)  # 在重试之前等待一段时间\n",
    "    \n",
    "#     if retry_count < max_retries:  # 如果请求成功\n",
    "#         soup1 = BeautifulSoup(response.text, 'html.parser')  # Specify parser explicitly\n",
    "#         abstract = soup1.find('blockquote', class_='abstract mathjax').text.strip()\n",
    "#         title = soup1.find('h1', class_='title mathjax').text.strip()\n",
    "#         authors = soup1.find('div', class_='authors').text.strip().replace(',', ';')\n",
    "#         subjects = soup1.find('span', class_='primary-subject').text.strip()\n",
    "#         paper_info = {\n",
    "#             'Title': title,\n",
    "#             'Subjects': subjects,\n",
    "#             'Authors': authors,\n",
    "#             'Abstract': abstract\n",
    "#         }\n",
    "#         papers_info.append(paper_info)\n",
    "#         print(abstract)  # Print or do whatever you need with the abstract\n",
    "\n",
    "import requests\n",
    "from time import sleep\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "\n",
    "# 定义最大重试次数\n",
    "max_retries = 5\n",
    "# 存储失败的请求\n",
    "failed_requests = []\n",
    "for identifier  in identifier_list :\n",
    "    link_list = identifier.find('a', {'title': 'Abstract'})\n",
    "    abs_url = link_list['href']\n",
    "    abs_full_url = urljoin(\"https://arxiv.org\", abs_url)\n",
    "    \n",
    "    # 初始化重试次数\n",
    "    retry_count = 0\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            response = requests.get(abs_full_url, timeout=20)  # 设置超时时间为10秒\n",
    "            sleep(2)  # 在每次请求之后等待2秒钟\n",
    "            response.raise_for_status()  # 检查请求是否成功\n",
    "            break  # 请求成功，跳出循环\n",
    "        except (Timeout, RequestException) as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == max_retries:\n",
    "                print(\"Max retries reached, unable to fetch data\")\n",
    "                # 将失败的请求记录下来\n",
    "                failed_requests.append(abs_full_url)\n",
    "                break  # 达到最大重试次数，跳出循环\n",
    "            else:\n",
    "                print(f\"Retrying... Attempt {retry_count}\")\n",
    "                sleep(2)  # 在重试之前等待一段时间\n",
    "    \n",
    "    if retry_count < max_retries:  # 如果请求成功\n",
    "        soup1 = BeautifulSoup(response.text, 'html.parser')  # Specify parser explicitly\n",
    "        abstract = soup1.find('blockquote', class_='abstract mathjax').text.strip()\n",
    "        title = soup1.find('h1', class_='title mathjax').text.strip()\n",
    "        authors = soup1.find('div', class_='authors').text.strip().replace(',', ';')\n",
    "        subjects = soup1.find('span', class_='primary-subject').text.strip()\n",
    "        paper_info = {\n",
    "            'Title': title,\n",
    "            'Subjects': subjects,\n",
    "            'Authors': authors,\n",
    "            'Abstract': abstract\n",
    "        }\n",
    "        papers_info.append(paper_info)\n",
    "        print(abstract)  # Print or do whatever you need with the abstract\n",
    "\n",
    "if failed_requests:  # 检查 failed_requests 是否为空\n",
    "    for failed_request in failed_requests:\n",
    "        try:\n",
    "            response = requests.get(failed_request, timeout=20)\n",
    "            sleep(2)  # 在每次请求之后等待2秒钟\n",
    "            response.raise_for_status()  # 检查请求是否成功\n",
    "            break  # 请求成功，跳出循环\n",
    "        except (Timeout, RequestException) as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == max_retries:\n",
    "                print(\"Max retries reached, unable to fetch data\")\n",
    "                # 将失败的请求记录下来\n",
    "                failed_requests.append(abs_full_url)\n",
    "                break  # 达到最大重试次数，跳出循环\n",
    "            else:\n",
    "                print(f\"Retrying... Attempt {retry_count}\")\n",
    "                sleep(2)  # 在重试之前等待一段时间\n",
    "    \n",
    "    if retry_count < max_retries:  # 如果请求成功\n",
    "        soup1 = BeautifulSoup(response.text, 'html.parser')  # Specify parser explicitly\n",
    "        abstract = soup1.find('blockquote', class_='abstract mathjax').text.strip()\n",
    "        title = soup1.find('h1', class_='title mathjax').text.strip()\n",
    "        authors = soup1.find('div', class_='authors').text.strip().replace(',', ';')\n",
    "        subjects = soup1.find('span', class_='primary-subject').text.strip()\n",
    "        paper_info = {\n",
    "            'Title': title,\n",
    "            'Subjects': subjects,\n",
    "            'Authors': authors,\n",
    "            'Abstract': abstract\n",
    "        }\n",
    "        papers_info.append(paper_info)\n",
    "        print(abstract) \n",
    "else:\n",
    "    print(\"No failed requests to retry.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05478db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = requests.get(\"https://arxiv.org/list/cs/pastweek?skip=25&show=25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f0887b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1 = BeautifulSoup(r1.text,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b13bd4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_list1 = soup1.find_all('span', class_='list-identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "343b7bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Abstract:Simulation is an invaluable tool for radio-frequency system designers that enables rapid prototyping of various algorithms for imaging, target detection, classification, and tracking. However, simulating realistic radar scans is a challenging task that requires an accurate model of the scene, radio frequency material properties, and a corresponding radar synthesis function. Rather than specifying these models explicitly, we propose DART - Doppler Aided Radar Tomography, a Neural Radiance Field-inspired method which uses radar-specific physics to create a reflectance and transmittance-based rendering pipeline for range-Doppler images. We then evaluate DART by constructing a custom data collection platform and collecting a novel radar dataset together with accurate position and instantaneous velocity measurements from lidar-based localization. In comparison to state-of-the-art baselines, DART synthesizes superior radar range-Doppler images from novel views across all datasets and additionally can be used to generate high quality tomographic images.\n",
      "Abstract:We reformulate the Lanczos tau method for the discretization of time-delay systems in terms of a pencil of operators, allowing for new insights into this approach. As a first main result, we show that, for the choice of a shifted Legendre basis, this method is equivalent to Padé approximation in the frequency domain. We illustrate that Lanczos tau methods straightforwardly give rise to sparse, self nesting discretizations. Equivalence is also demonstrated with pseudospectral collocation, where the non-zero collocation points are chosen as the zeroes of orthogonal polynomials. The importance of such a choice manifests itself in the approximation of the $H^2$-norm, where, under mild conditions, super-geometric convergence is observed and, for a special case, super convergence is proved; both significantly faster than the algebraic convergence reported in previous work.\n",
      "Abstract:Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.\n",
      "To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled with respective intermediate representations. Next, starting from various base Code-LMs (ranging in size from 1.1B to 7.3B parameters), we carry out continued causal language modelling training on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2) align the IR constructs with respective constructs of various programming languages. Our resulting models, dubbed IRCoder, display sizeable and consistent gains across a wide variety of code generation tasks and metrics, including prompt robustness, multilingual code completion, code understanding, and instruction following.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Abstract:To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it's crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic families and levels of resource availability, ranging from high to mid-resource languages. Through comprehensive experiments, we provide insights into the complexities of multilingual toxicity mitigation, offering valuable insights and paving the way for future research in this increasingly important field. Code and data are available at this https URL.\n",
      "Abstract:This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical agent for multi-task robotic manipulation. HDP factorises a manipulation policy into a hierarchical structure: a high-level task-planning agent which predicts a distant next-best end-effector pose (NBP), and a low-level goal-conditioned diffusion policy which generates optimal motion trajectories. The factorised policy representation allows HDP to tackle both long-horizon task planning while generating fine-grained low-level actions. To generate context-aware motion trajectories while satisfying robot kinematics constraints, we present a novel kinematics-aware goal-conditioned control agent, Robot Kinematics Diffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the end-effector pose and joint position trajectories, and distill the accurate but kinematics-unaware end-effector pose diffuser to the kinematics-aware but less accurate joint position diffuser via differentiable kinematics. Empirically, we show that HDP achieves a significantly higher success rate than the state-of-the-art methods in both simulation and real-world.\n",
      "Abstract:Factual recall from a reference source is crucial for evaluating the performance of Retrieval Augmented Generation (RAG) systems, as it directly probes into the quality of both retrieval and generation. However, it still remains a challenge to perform this evaluation reliably and efficiently. Recent work has focused on fact verification via prompting language model (LM) evaluators, however we demonstrate that these methods are unreliable in the presence of incomplete or inaccurate information. We introduce Facts as a Function (FaaF), a new approach to fact verification that utilizes the function calling abilities of LMs and a framework for RAG factual recall evaluation. FaaF substantially improves the ability of LMs to identify unsupported facts in text with incomplete information whilst improving efficiency and lowering cost by several times, compared to prompt-based approaches.\n",
      "Abstract:In this paper, we design and analyze a Virtual Element discretization for the steady motion of non-Newtonian, incompressible fluids. A specific stabilization, tailored to mimic the monotonicity and boundedness properties of the continuous operator, is introduced and theoretically investigated. The proposed method has several appealing features, including the exact enforcement of the divergence free condition and the possibility of making use of fully general polygonal meshes. A complete well-posedness and convergence analysis of the proposed method is presented under mild assumptions on the non-linear laws, encompassing common examples such as the Carreau--Yasuda model. Numerical experiments validating the theoretical bounds as well as demonstrating the practical capabilities of the proposed formulation are presented.\n",
      "Abstract:In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is released under the CC-BY-SA-4.0 License.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 2\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 3\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 4\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Max retries reached, unable to fetch data\n",
      "Abstract:The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both challenges. LD3M incorporates a novel diffusion process tailored for dataset distillation, which improves the gradient norms for learning synthetic images. By adjusting the number of diffusion steps, LD3M also offers a straightforward way of controlling the trade-off between speed and accuracy. We evaluate our approach in several ImageNet subsets and for high-resolution images (128x128 and 256x256). As a result, LD3M consistently outperforms state-of-the-art distillation techniques by up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 2\n",
      "Abstract:Graph neural networks (GNNs) are the predominant architectures for a variety of learning tasks on graphs. We present a new angle on the expressive power of GNNs by studying how the predictions of a GNN probabilistic classifier evolve as we apply it on larger graphs drawn from some random graph model. We show that the output converges to a constant function, which upper-bounds what these classifiers can express uniformly. This convergence phenomenon applies to a very wide class of GNNs, including state of the art models, with aggregates including mean and the attention-based mechanism of graph transformers. Our results apply to a broad class of random graph models, including the (sparse) Erdős-Rényi model and the stochastic block model. We empirically validate these findings, observing that the convergence phenomenon already manifests itself on graphs of relatively modest size.\n",
      "Abstract:Bladder cancer ranks within the top 10 most diagnosed cancers worldwide and is among the most expensive cancers to treat due to the high recurrence rates which require lifetime follow-ups. The primary tool for diagnosis is cystoscopy, which heavily relies on doctors' expertise and interpretation. Therefore, annually, numerous cases are either undiagnosed or misdiagnosed and treated as urinary infections. To address this, we suggest a deep learning approach for bladder cancer detection and segmentation which combines CNNs with a lightweight positional-encoding-free transformer and dual attention gates that fuse self and spatial attention for feature enhancement. The architecture suggested in this paper is efficient making it suitable for medical scenarios that require real time inference. Experiments have proven that this model addresses the critical need for a balance between computational efficiency and diagnostic accuracy in cystoscopic imaging as despite its small size it rivals large models in performance.\n",
      "Abstract:Contention resolution addresses the challenge of coordinating access by multiple processes to a shared resource such as memory, disk storage, or a communication channel. Originally spurred by challenges in database systems and bus networks, contention resolution has endured as an important abstraction for resource sharing, despite decades of technological change. Here, we survey the literature on resolving worst-case contention, where the number of processes and the time at which each process may start seeking access to the resource is dictated by an adversary. We highlight the evolution of contention resolution, where new concerns -- such as security, quality of service, and energy efficiency -- are motivated by modern systems. These efforts have yielded insights into the limits of randomized and deterministic approaches, as well as the impact of different model assumptions such as global clock synchronization, knowledge of the number of processors, feedback from access attempts, and attacks on the availability of the shared resource.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Abstract:Smartphones are integral to modern life, yet research highlights the cognitive drawbacks associated even with their mere presence. Physically removing them from sight is a solution, but it is sometimes impractical and may increase anxiety due to fear of missing out. In response, we introduce a simple but effective use of augmented reality (AR) head-mounted displays, focusing not on augmenting reality with virtual objects, but on diminishing reality by selectively removing or occluding distracting objects, from the user's field of view. We compared cognitive task performance across four conditions: the smartphone being physically nearby, physically remote, visually removed and visually occluded via AR. Our findings reveal that using AR to visually cancel out smartphones significantly mitigates cognitive distractions caused by their presence. Specifically, the AR interventions had effects similar to physically removing the phone. These results suggest potential for novel AR applications designed to diminish reality, thereby enhancing cognitive performance.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Abstract:Since Labov's (1964) foundational work on the social stratification of language, linguistics has dedicated concerted efforts towards understanding the relationships between socio-demographic factors and language production and perception. Despite the large body of evidence identifying significant relationships between socio-demographic factors and language production, relatively few of these factors have been investigated in the context of NLP technology. While age and gender are well covered, Labov's initial target, socio-economic class, is largely absent. We survey the existing Natural Language Processing (NLP) literature and find that only 20 papers even mention socio-economic status. However, the majority of those papers do not engage with class beyond collecting information of annotator-demographics. Given this research lacuna, we provide a definition of class that can be operationalised by NLP researchers, and argue for including socio-economic class in future language technologies.\n",
      "Abstract:Vertical Federated Learning (VFL) is an emergent distributed machine learning paradigm wherein owners of disjoint features of a common set of entities collaborate to learn a global model without sharing data. In VFL, a host client owns data labels for each entity and learns a final representation based on intermediate local representations from all guest clients. Therefore, the host is a single point of failure and label feedback can be used by malicious guest clients to infer private features. Requiring all participants to remain active and trustworthy throughout the entire training process is generally impractical and altogether infeasible outside of controlled environments. We propose Decoupled VFL (DVFL), a blockwise learning approach to VFL. By training each model on its own objective, DVFL allows for decentralized aggregation and isolation between feature learning and label supervision. With these properties, DVFL is fault tolerant and secure. We implement DVFL to train split neural networks and show that model performance is comparable to VFL on a variety of classification datasets.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Abstract:We propose a method to teach multiple large language models (LLM) to collaborate by interleaving their generations at the token level. We model the decision of which LLM generates the next token as a latent variable. By optimizing the marginal likelihood of a training set under our latent variable model, the base LLM automatically learns when to generate itself and when to call on one of the ``assistant'' language models to generate, all without direct supervision. Token-level collaboration during decoding allows for a fusion of each model's expertise in a manner tailored to the specific task at hand. Our collaborative decoding is especially useful in cross-domain settings where a generalist base LLM learns to invoke domain expert models. On instruction-following, domain-specific QA, and reasoning tasks, we show that the performance of the joint system exceeds that of the individual models. Through qualitative analysis of the learned latent decisions, we show models trained with our method exhibit several interesting collaboration patterns, e.g., template-filling. Our code is available at this https URL.\n",
      "Abstract:The digital age introduced the Digital Ecological Niche (DEN), revolutionizing human interactions. The advent of Digital History (DHy) has marked a methodological shift in historical studies, tracing its roots to Babbage and Lovelace's 19th-century work on \"coding\" as a foundational communication process, fostering a new interaction paradigm between humans and machines, termed \"person2persons2machines.\" This evolution, through digitization and informatization, builds upon ancient coding practices but was significantly advanced by Babbage and Lovelace's contributions to mathematical linguistic systems, laying the groundwork for Computer Science. This field, central to 20th-century mainframe interaction through programming languages and formalization, situates Digital History within a broader historical context. Here, coding and mathematical methodologies empower historians with advanced technologies for historical data preservation and analysis. Nonetheless, the extent to which computation and Turing machines can fully understand and interpret history remains a subject of debate.\n",
      "Abstract:Recent works have argued that high-level semantic concepts are encoded \"linearly\" in the representation space of large language models. In this work, we study the origins of such linear representations. To that end, we introduce a simple latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to show that the next token prediction objective (softmax with cross-entropy) and the implicit bias of gradient descent together promote the linear representation of concepts. Experiments show that linear representations emerge when learning from data matching the latent variable model, confirming that this simple structure already suffices to yield linear representations. We additionally confirm some predictions of the theory using the LLaMA-2 large language model, giving evidence that the simplified model yields generalizable insights.\n",
      "Abstract:Large language models (LLMs) adapted to follow user instructions are now widely deployed as conversational agents. In this work, we examine one increasingly common instruction-following task: providing writing assistance to compose a long-form answer. To evaluate the capabilities of current LLMs on this task, we construct KIWI, a dataset of knowledge-intensive writing instructions in the scientific domain. Given a research question, an initial model-generated answer and a set of relevant papers, an expert annotator iteratively issues instructions for the model to revise and improve its answer. We collect 1,260 interaction turns from 234 interaction sessions with three state-of-the-art LLMs. Each turn includes a user instruction, a model response, and a human evaluation of the model response. Through a detailed analysis of the collected responses, we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits. Further, we find that models struggle to judge whether their outputs successfully followed user instructions, with accuracy at least 10 points short of human agreement. Our findings indicate that KIWI will be a valuable resource to measure progress and improve LLMs' instruction-following capabilities for knowledge intensive writing tasks.\n",
      "Abstract:The increasing significance of sustainability considerations within both public spheres (such as policies and regulations) and private sectors (including voluntary commitments by major multinational corporations) underscores the imperative to harness cutting-edge technological advancements. This is essential to ensure that the momentum of this trend translates into tangible outcomes, thwarting phenomena like greenwashing and upholding high standards of integrity, all while expediting progress through automation. This paper focuses specifically on carbon markets, which, after enduring years of confusion and controversy, may finally be on the brink of converging toward internationally recognized minimum standards. Beginning with an introduction to fundamental concepts pertaining to carbon markets and Distributed Ledger Technologies (DLTs), the paper proceeds to dissect the challenges and opportunities within this burgeoning field. Its primary contribution lies in offering a comprehensive overview of recent developments across various initiatives (such as ICVCM, IETA/WorldBank/CAD Trust, IEEE/ISO) and providing a layered analysis of the entire ecosystem. This framework aids in understanding and prioritising future endeavours. Ultimately, the paper furnishes a set of recommendations aimed at bolstering scalability and fostering widespread adoption of best practices within international markets.\n",
      "Abstract:This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models in solving algorithmic puzzles that necessitate both visual understanding, language understanding, and complex algorithmic reasoning. We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic, combinatorics, graph theory, optimization, search, etc., aiming to evaluate the gap between visual data interpretation and algorithmic problem-solving skills. The dataset is generated automatically from code authored by humans. All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations. It ensures that our dataset can be scaled up arbitrarily in terms of reasoning complexity and dataset size. Our investigation reveals that large language models (LLMs) such as GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We find that their performance is near random in a multi-choice question-answering setup for a significant number of puzzles. The findings emphasize the challenges of integrating visual, language, and algorithmic knowledge for solving complex reasoning problems.\n",
      "Abstract:In recent years, few-shot and zero-shot learning, which learn to predict labels with limited annotated instances, have garnered significant attention. Traditional approaches often treat frequent-shot (freq-shot; labels with abundant instances), few-shot, and zero-shot learning as distinct challenges, optimizing systems for just one of these scenarios. Yet, in real-world settings, label occurrences vary greatly. Some of them might appear thousands of times, while others might only appear sporadically or not at all. For practical deployment, it is crucial that a system can adapt to any label occurrence. We introduce a novel classification challenge: X-shot, reflecting a real-world context where freq-shot, few-shot, and zero-shot labels co-occur without predefined limits. Here, X can span from 0 to positive infinity. The crux of X-shot centers on open-domain generalization and devising a system versatile enough to manage various label scenarios. To solve X-shot, we propose BinBin (Binary INference Based on INstruction following) that leverages the Indirect Supervision from a large collection of NLP tasks via instruction following, bolstered by Weak Supervision provided by large language models. BinBin surpasses previous state-of-the-art techniques on three benchmark datasets across multiple domains. To our knowledge, this is the first work addressing X-shot learning, where X remains variable.\n",
      "Abstract:Pretrained language models (PLMs) have shown remarkable few-shot learning capabilities when provided with properly formatted examples. However, selecting the \"best\" examples remains an open challenge. We propose a complexity-based prompt selection approach for sequence tagging tasks. This approach avoids the training of a dedicated model for selection of examples, and instead uses certain metrics to align the syntactico-semantic complexity of test sentences and examples. We use both sentence- and word-level metrics to match the complexity of examples to the (test) sentence being considered. Our results demonstrate that our approach extracts greater performance from PLMs: it achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute improvement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.\n",
      "Abstract:Recent advancements in drone technology have shown that commercial off-the-shelf Micro Aerial Drones are more effective than large-sized drones for performing flight missions in narrow environments, such as swarming, indoor navigation, and inspection of hazardous locations. Due to their deployments in many civilian and military applications, safe and reliable communication of these drones throughout the mission is critical. The Crazyflie ecosystem is one of the most popular Micro Aerial Drones and has the potential to be deployed worldwide. In this paper, we empirically investigate two interference attacks against the Crazy Real Time Protocol (CRTP) implemented within the Crazyflie drones. In particular, we explore the feasibility of experimenting two attack vectors that can disrupt an ongoing flight mission: the jamming attack, and the hijacking attack. Our experimental results demonstrate the effectiveness of such attacks in both autonomous and non-autonomous flight modes on a Crazyflie 2.1 drone. Finally, we suggest potential shielding strategies that guarantee a safe and secure flight mission. To the best of our knowledge, this is the first work investigating jamming and hijacking attacks against Micro Aerial Drones, both in autonomous and non-autonomous modes.\n",
      "Abstract:Accurate training labels are a key component for multi-class medical image segmentation. Their annotation is costly and time-consuming because it requires domain expertise. This work aims to develop a dual-branch network and automatically improve training labels for multi-class image segmentation. Transfer learning is used to train the network and improve inaccurate weak labels sequentially. The dual-branch network is first trained by weak labels alone to initialize model parameters. After the network is stabilized, the shared encoder is frozen, and strong and weak decoders are fine-tuned by strong and weak labels together. The accuracy of weak labels is iteratively improved in the fine-tuning process. The proposed method was applied to a three-class segmentation of muscle, subcutaneous and visceral adipose tissue on abdominal CT scans. Validation results on 11 patients showed that the accuracy of training labels was statistically significantly improved, with the Dice similarity coefficient of muscle, subcutaneous and visceral adipose tissue increased from 74.2% to 91.5%, 91.2% to 95.6%, and 77.6% to 88.5%, respectively (p<0.05). In comparison with our earlier method, the label accuracy was also significantly improved (p<0.05). These experimental results suggested that the combination of the dual-branch network and transfer learning is an efficient means to improve training labels for multi-class segmentation.\n"
     ]
    }
   ],
   "source": [
    "# # 定义最大重试次数\n",
    "# max_retries = 3\n",
    "\n",
    "# for identifier1 in identifier_list1:\n",
    "#     link_list = identifier1.find('a', {'title': 'Abstract'})\n",
    "#     abs_url = link_list['href']\n",
    "#     abs_full_url = urljoin(\"https://arxiv.org\", abs_url)\n",
    "    \n",
    "#     # 初始化重试次数\n",
    "#     retry_count = 0\n",
    "#     while retry_count < max_retries:\n",
    "#         try:\n",
    "#             response = requests.get(abs_full_url, timeout=20)  \n",
    "#             sleep(2)  # 在每次请求之后等待2秒钟\n",
    "#             response.raise_for_status()  # 检查请求是否成功\n",
    "#             break  # 请求成功，跳出循环\n",
    "#         except (Timeout, RequestException) as e:\n",
    "#             print(f\"Request error: {e}\")\n",
    "#             retry_count += 1\n",
    "#             if retry_count == max_retries:\n",
    "#                 print(\"Max retries reached, unable to fetch data\")\n",
    "#                 break  # 达到最大重试次数，跳出循环\n",
    "#             else:\n",
    "#                 print(f\"Retrying... Attempt {retry_count}\")\n",
    "#                 sleep(2)  # 在重试之前等待一段时间\n",
    "    \n",
    "#     if retry_count < max_retries:  # 如果请求成功\n",
    "#         soup1 = BeautifulSoup(response.text, 'html.parser')  # Specify parser explicitly\n",
    "#         abstract = soup1.find('blockquote', class_='abstract mathjax').text.strip()\n",
    "#         title = soup1.find('h1', class_='title mathjax').text.strip()\n",
    "#         authors = soup1.find('div', class_='authors').text.strip().replace(',', ';')\n",
    "#         subjects = soup1.find('span', class_='primary-subject').text.strip()\n",
    "#         paper_info = {\n",
    "#             'Title': title,\n",
    "#             'Subjects': subjects,\n",
    "#             'Authors': authors,\n",
    "#             'Abstract': abstract\n",
    "#         }\n",
    "#         papers_info.append(paper_info)\n",
    "#         print(abstract)  # Print or do whatever you need with the abstract\n",
    "\n",
    "\n",
    "import requests\n",
    "from time import sleep\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "# 存储失败的请求\n",
    "failed_requests = []\n",
    "# 定义最大重试次数\n",
    "max_retries = 5\n",
    "for identifier1 in identifier_list1:\n",
    "    link_list = identifier1.find('a', {'title': 'Abstract'})\n",
    "    abs_url = link_list['href']\n",
    "    abs_full_url = urljoin(\"https://arxiv.org\", abs_url)\n",
    "    \n",
    "    # 初始化重试次数\n",
    "    retry_count = 0\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            response = requests.get(abs_full_url, timeout=20)  # 设置超时时间为10秒\n",
    "            sleep(2)  # 在每次请求之后等待2秒钟\n",
    "            response.raise_for_status()  # 检查请求是否成功\n",
    "            break  # 请求成功，跳出循环\n",
    "        except (Timeout, RequestException) as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == max_retries:\n",
    "                print(\"Max retries reached, unable to fetch data\")\n",
    "                # 将失败的请求记录下来\n",
    "                failed_requests.append(abs_full_url)\n",
    "                break  # 达到最大重试次数，跳出循环\n",
    "            else:\n",
    "                print(f\"Retrying... Attempt {retry_count}\")\n",
    "                sleep(2)  # 在重试之前等待一段时间\n",
    "    \n",
    "    if retry_count < max_retries:  # 如果请求成功\n",
    "        soup1 = BeautifulSoup(response.text, 'html.parser')  # Specify parser explicitly\n",
    "        abstract = soup1.find('blockquote', class_='abstract mathjax').text.strip()\n",
    "        title = soup1.find('h1', class_='title mathjax').text.strip()\n",
    "        authors = soup1.find('div', class_='authors').text.strip().replace(',', ';')\n",
    "        subjects = soup1.find('span', class_='primary-subject').text.strip()\n",
    "        paper_info = {\n",
    "            'Title': title,\n",
    "            'Subjects': subjects,\n",
    "            'Authors': authors,\n",
    "            'Abstract': abstract\n",
    "        }\n",
    "        papers_info.append(paper_info)\n",
    "        print(abstract)  # Print or do whatever you need with the abstract\n",
    "\n",
    "if failed_requests:  # 检查 failed_requests 是否为空\n",
    "    for failed_request in failed_requests:\n",
    "        try:\n",
    "            response = requests.get(failed_request, timeout=20)\n",
    "            sleep(2)  # 在每次请求之后等待2秒钟\n",
    "            response.raise_for_status()  # 检查请求是否成功\n",
    "            break  # 请求成功，跳出循环\n",
    "        except (Timeout, RequestException) as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == max_retries:\n",
    "                print(\"Max retries reached, unable to fetch data\")\n",
    "                # 将失败的请求记录下来\n",
    "                failed_requests.append(abs_full_url)\n",
    "                break  # 达到最大重试次数，跳出循环\n",
    "            else:\n",
    "                print(f\"Retrying... Attempt {retry_count}\")\n",
    "                sleep(2)  # 在重试之前等待一段时间\n",
    "    \n",
    "    if retry_count < max_retries:  # 如果请求成功\n",
    "        soup1 = BeautifulSoup(response.text, 'html.parser')  # Specify parser explicitly\n",
    "        abstract = soup1.find('blockquote', class_='abstract mathjax').text.strip()\n",
    "        title = soup1.find('h1', class_='title mathjax').text.strip()\n",
    "        authors = soup1.find('div', class_='authors').text.strip().replace(',', ';')\n",
    "        subjects = soup1.find('span', class_='primary-subject').text.strip()\n",
    "        paper_info = {\n",
    "            'Title': title,\n",
    "            'Subjects': subjects,\n",
    "            'Authors': authors,\n",
    "            'Abstract': abstract\n",
    "        }\n",
    "        papers_info.append(paper_info)\n",
    "        print(abstract) \n",
    "else:\n",
    "    print(\"No failed requests to retry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d0c957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = requests.get(\"https://arxiv.org/list/cs/pastweek?skip=50&show=25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2c313ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(r2.text,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8f9e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_list2 = soup2.find_all('span', class_='list-identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0dd9bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract:Emoji have become ubiquitous in written communication, on the Web and beyond. They can emphasize or clarify emotions, add details to conversations, or simply serve decorative purposes. This casual use, however, barely scratches the surface of the expressive power of emoji. To further unleash this power, we present Emojinize, a method for translating arbitrary text phrases into sequences of one or more emoji without requiring human input. By leveraging the power of large language models, Emojinize can choose appropriate emoji by disambiguating based on context (eg, cricket-bat vs bat) and can express complex concepts compositionally by combining multiple emoji (eq, ''Emojinize'' is translated to input-latin-letters right-arrow grinning-face). In a cloze test--based user study, we show that Emojinize's emoji translations increase the human guessability of masked words by 55%, whereas human-picked emoji translations do so by only 29%. These results suggest that emoji provide a sufficiently rich vocabulary to accurately translate a wide variety of words. Moreover, annotating words and phrases with Emojinize's emoji translations opens the door to numerous downstream applications, including children learning how to read, adults learning foreign languages, and text understanding for people with learning disabilities.\n",
      "Abstract:We study the limits and capability of public-data assisted differentially private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data. For complete/labeled public data, we show that any $(\\epsilon,\\delta)$-PA-DP has excess risk $\\tilde{\\Omega}\\big(\\min\\big\\{\\frac{1}{\\sqrt{n_{\\text{pub}}}},\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon} \\big\\} \\big)$, where $d$ is the dimension, ${n_{\\text{pub}}}$ is the number of public samples, ${n_{\\text{priv}}}$ is the number of private samples, and $n={n_{\\text{pub}}}+{n_{\\text{priv}}}$. These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form. Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal. We also study PA-DP supervised learning with \\textit{unlabeled} public samples. In contrast to our previous result, we here show novel methods for leveraging public data in private supervised learning. For generalized linear models (GLM) with unlabeled public data, we show an efficient algorithm which, given $\\tilde{O}({n_{\\text{priv}}}\\epsilon)$ unlabeled public samples, achieves the dimension independent rate $\\tilde{O}\\big(\\frac{1}{\\sqrt{n_{\\text{priv}}}} + \\frac{1}{\\sqrt{n_{\\text{priv}}\\epsilon}}\\big)$. We develop new lower bounds for this setting which shows that this rate cannot be improved with more public samples, and any fewer public samples leads to a worse rate. Finally, we provide extensions of this result to general hypothesis classes with finite fat-shattering dimension with applications to neural networks and non-Euclidean geometries.\n",
      "Abstract:We consider unsupervised domain adaptation (UDA) for semantic segmentation in which the model is trained on a labeled source dataset and adapted to an unlabeled target dataset. Unfortunately, current self-training methods are susceptible to misclassified pseudo-labels resulting from erroneous predictions. Since certain classes are typically associated with less reliable predictions in UDA, reducing the impact of such pseudo-labels without skewing the training towards some classes is notoriously difficult. To this end, we propose an extensive cut-and-paste strategy (ECAP) to leverage reliable pseudo-labels through data augmentation. Specifically, ECAP maintains a memory bank of pseudo-labeled target samples throughout training and cut-and-pastes the most confident ones onto the current training batch. We implement ECAP on top of the recent method MIC and boost its performance on two synthetic-to-real domain adaptation benchmarks. Notably, MIC+ECAP reaches an unprecedented performance of 69.1 mIoU on the Synthia->Cityscapes benchmark. Our code is available at this https URL.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Abstract:As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as opposed to more complex pruning techniques, suggests a high degree of redundancy in the model architecture.\n",
      "Abstract:Score-based diffusion models, while achieving remarkable empirical performance, often suffer from low sampling speed, due to extensive function evaluations needed during the sampling phase. Despite a flurry of recent activities towards speeding up diffusion generative modeling in practice, theoretical underpinnings for acceleration techniques remain severely limited. In this paper, we design novel training-free algorithms to accelerate popular deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers. Our accelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the number of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our accelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the rate $O(1/\\sqrt{T})$ for the DDPM sampler. The design of our algorithms leverages insights from higher-order approximation, and shares similar intuitions as popular high-order ODE solvers like the DPM-Solver-2. Our theory accommodates $\\ell_2$-accurate score estimates, and does not require log-concavity or smoothness on the target distribution.\n",
      "Abstract:Recent advances of locomotion controllers utilizing deep reinforcement learning (RL) have yielded impressive results in terms of achieving rapid and robust locomotion across challenging terrain, such as rugged rocks, non-rigid ground, and slippery surfaces. However, while these controllers primarily address challenges underneath the robot, relatively little research has investigated legged mobility through confined 3D spaces, such as narrow tunnels or irregular voids, which impose all-around constraints. The cyclic gait patterns resulted from existing RL-based methods to learn parameterized locomotion skills characterized by motion parameters, such as velocity and body height, may not be adequate to navigate robots through challenging confined 3D spaces, requiring both agile 3D obstacle avoidance and robust legged locomotion. Instead, we propose to learn locomotion skills end-to-end from goal-oriented navigation in confined 3D spaces. To address the inefficiency of tracking distant navigation goals, we introduce a hierarchical locomotion controller that combines a classical planner tasked with planning waypoints to reach a faraway global goal location, and an RL-based policy trained to follow these waypoints by generating low-level motion commands. This approach allows the policy to explore its own locomotion skills within the entire solution space and facilitates smooth transitions between local goals, enabling long-term navigation towards distant goals. In simulation, our hierarchical approach succeeds at navigating through demanding confined 3D environments, outperforming both pure end-to-end learning approaches and parameterized locomotion skills. We further demonstrate the successful real-world deployment of our simulation-trained controller on a real robot.\n",
      "Abstract:In this paper, we study a defense against poisoned encoders in SSL called distillation, which is a defense used in supervised learning originally. Distillation aims to distill knowledge from a given model (a.k.a the teacher net) and transfer it to another (a.k.a the student net). Now, we use it to distill benign knowledge from poisoned pre-trained encoders and transfer it to a new encoder, resulting in a clean pre-trained encoder. In particular, we conduct an empirical study on the effectiveness and performance of distillation against poisoned encoders. Using two state-of-the-art backdoor attacks against pre-trained image encoders and four commonly used image classification datasets, our experimental results show that distillation can reduce attack success rate from 80.87% to 27.51% while suffering a 6.35% loss in accuracy. Moreover, we investigate the impact of three core components of distillation on performance: teacher net, student net, and distillation loss. By comparing 4 different teacher nets, 3 student nets, and 6 distillation losses, we find that fine-tuned teacher nets, warm-up-training-based student nets, and attention-based distillation loss perform best, respectively.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Abstract:The inverse wave scattering problem seeks to estimate a heterogeneous, inaccessible medium, modeled by unknown variable coefficients in wave equations, from transient recordings of waves generated by probing signals. It is a widely studied inverse problem with important applications, that is typically formulated as a nonlinear least squares data fit optimization. For typical measurement setups and band-limited probing signals, the least squares objective function has spurious local minima far and near the true solution, so Newton-type optimization methods fail. We introduce a different approach, for electromagnetic inverse wave scattering in lossless, anisotropic media. Our reduced order model (ROM) is an algebraic, discrete time dynamical system derived from Maxwell's equations with four important properties: (1) It is data driven, without knowledge of the medium. (2) The data to ROM mapping is nonlinear and yet the ROM can be obtained in a non-iterative fashion. (3) It has a special algebraic structure that captures the causal Wave propagation. (4) The ROM interpolates the data on a uniform time grid. We show how to obtain from the ROM an estimate of the wave field at inaccessible points inside the unknown medium. The use of this wave is twofold: First, it defines a computationally inexpensive imaging function designed to estimate the support of reflective structures in the medium, modeled by jump discontinuities of the matrix valued dielectric permittivity. Second, it gives an objective function for quantitative estimation of the dielectric permittivity, that has better behavior than the least squares data fitting objective function. The methodology introduced in this paper applies to Maxwell's equations in three dimensions. To avoid high computational costs, we limit the study to a cylindrical domain filled with an orthotropic medium, so the problem becomes two dimensional.\n",
      "Abstract:The study analyses polarisation on Finnish social media with data from the platform X, which was known as Twitter during the time of data collection (during the Sipilä and Marin governments, 2015-2023). The users were clustered into three different ideological groups - the Conservative Right, the Moderate Right, and the Liberal Left - based on their retweeting of tweets referring to the different political parties in Finland. Trends in polarisation of several topics encompassing the most recent political crises - immigration, climate change, COVID-19, and security policy - between these ideological groups is analysed using network methods. To what extent the polarisation of each topic aligns with the polarisation of the other topics is also studied. In addition, the sharing of news links is examined in relation to the ideological groups of the users as well as to the sentiment and the virality of the tweets in which news links are shared.\n",
      "Abstract:Feature selection aims to identify the most pattern-discriminative feature subset. In prior literature, filter (e.g., backward elimination) and embedded (e.g., Lasso) methods have hyperparameters (e.g., top-K, score thresholding) and tie to specific models, thus, hard to generalize; wrapper methods search a feature subset in a huge discrete space and is computationally costly. To transform the way of feature selection, we regard a selected feature subset as a selection decision token sequence and reformulate feature selection as a deep sequential generative learning task that distills feature knowledge and generates decision sequences. Our method includes three steps: (1) We develop a deep variational transformer model over a joint of sequential reconstruction, variational, and performance evaluator losses. Our model can distill feature selection knowledge and learn a continuous embedding space to map feature selection decision sequences into embedding vectors associated with utility scores. (2) We leverage the trained feature subset utility evaluator as a gradient provider to guide the identification of the optimal feature subset embedding;(3) We decode the optimal feature subset embedding to autoregressively generate the best feature selection decision sequence with autostop. Extensive experimental results show this generative perspective is effective and generic, without large discrete search space and expert-specific hyperparameters.\n",
      "Abstract:Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure. Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects. However, a broader evaluation of Cobweb as a model of human categorization remains lacking. The current study addresses this gap. It establishes Cobweb's alignment with classical human category learning effects. It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model. These findings set the stage for future research on Cobweb as a comprehensive model of human category learning.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Abstract:This research aims to further understanding in the field of continuous authentication using behavioral biometrics. We are contributing a novel dataset that encompasses the gesture data of 15 users playing Minecraft with a Samsung Tablet, each for a duration of 15 minutes. Utilizing this dataset, we employed machine learning (ML) binary classifiers, being Random Forest (RF), K-Nearest Neighbors (KNN), and Support Vector Classifier (SVC), to determine the authenticity of specific user actions. Our most robust model was SVC, which achieved an average accuracy of approximately 90%, demonstrating that touch dynamics can effectively distinguish users. However, further studies are needed to make it viable option for authentication systems\n",
      "Abstract:We introduce Cluster Edge Modification problems with constraints on the size of the clusters and study their complexity. A graph $G$ is a cluster graph if every connected component of $G$ is a clique. In a typical Cluster Edge Modification problem such as the widely studied Cluster Editing, we are given a graph $G$ and a non-negative integer $k$ as input, and we have to decide if we can turn $G$ into a cluster graph by way of at most $k$ edge modifications -- that is, by adding or deleting edges. In this paper, we study the parameterized complexity of such problems, but with an additional constraint: The size difference between any two connected components of the resulting cluster graph should not exceed a given threshold. Depending on which modifications are permissible -- only adding edges, only deleting edges, both adding and deleting edges -- we have three different computational problems. We show that all three problems, when parameterized by $k$, admit single-exponential time FPT algorithms and polynomial kernels. Our problems may be thought of as the size-constrained or balanced counterparts of the typical Cluster Edge Modification problems, similar to the well-studied size-constrained or balanced counterparts of other clustering problems such as $k$-Means Clustering.\n",
      "Abstract:In the realm of computer security, the importance of efficient and reliable user authentication methods has become increasingly critical. This paper examines the potential of mouse movement dynamics as a consistent metric for continuous authentication. By analyzing user mouse movement patterns in two contrasting gaming scenarios, \"Team Fortress\" and Poly Bridge we investigate the distinctive behavioral patterns inherent in high-intensity and low-intensity UI interactions. The study extends beyond conventional methodologies by employing a range of machine learning models. These models are carefully selected to assess their effectiveness in capturing and interpreting the subtleties of user behavior as reflected in their mouse movements. This multifaceted approach allows for a more nuanced and comprehensive understanding of user interaction patterns. Our findings reveal that mouse movement dynamics can serve as a reliable indicator for continuous user authentication. The diverse machine learning models employed in this study demonstrate competent performance in user verification, marking an improvement over previous methods used in this field. This research contributes to the ongoing efforts to enhance computer security and highlights the potential of leveraging user behavior, specifically mouse dynamics, in developing robust authentication systems.\n",
      "Abstract:In this paper, we propose an approach for identifying linear and nonlinear discrete-time state-space models, possibly under $\\ell_1$- and group-Lasso regularization, based on the L-BFGS-B algorithm. For the identification of linear models, we show that, compared to classical linear subspace methods, the approach often provides better results, is much more general in terms of the loss and regularization terms used, and is also more stable from a numerical point of view. The proposed method not only enriches the existing set of linear system identification tools but can be also applied to identifying a very broad class of parametric nonlinear state-space models, including recurrent neural networks. We illustrate the approach on synthetic and experimental datasets and apply it to solve the challenging industrial robot benchmark for nonlinear multi-input/multi-output system identification proposed by Weigand et al. (2022). A Python implementation of the proposed identification method is available in the package \\texttt{jax-sysid}, available at \\url{this https URL}.\n",
      "Abstract:Floating Car Observers (FCOs) are an innovative method to collect traffic data by deploying sensor-equipped vehicles to detect and locate other vehicles. We demonstrate that even a small penetration rate of FCOs can identify a significant amount of vehicles at a given intersection. This is achieved through the emulation of detection within a microscopic traffic simulation. Additionally, leveraging data from previous moments can enhance the detection of vehicles in the current frame. Our findings indicate that, with a 20-second observation window, it is possible to recover up to 20\\% of vehicles that are not visible by FCOs in the current timestep. To exploit this, we developed a data-driven strategy, utilizing sequences of Bird's Eye View (BEV) representations of detected vehicles and deep learning models. This approach aims to bring currently undetected vehicles into view in the present moment, enhancing the currently detected vehicles. Results of different spatiotemporal architectures show that up to 41\\% of the vehicles can be recovered into the current timestep at their current position. This enhancement enriches the information initially available by the FCO, allowing an improved estimation of traffic states and metrics (e.g. density and queue length) for improved implementation of traffic management strategies.\n",
      "Abstract:In this paper we address the task of summarizing television shows, which touches key areas in AI research: complex reasoning, multiple modalities, and long narratives. We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods. Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode. We also present a new metric, PREFS (\\textbf{P}recision and \\textbf{R}ecall \\textbf{E}valuation of Summary \\textbf{F}act\\textbf{s}), to measure both precision and recall of generated summaries, which we decompose into atomic facts. Tested on the recently released SummScreen3D dataset Papalampidi and Lapata (2023), our method produces higher quality summaries than comparison models, as measured with ROUGE and our new fact-based metric.\n",
      "Abstract:Higher-order patterns reveal sequential multistep state transitions, which are usually superior to origin-destination analysis, which depicts only first-order geospatial movement patterns. Conventional methods for higher-order movement modeling first construct a directed acyclic graph (DAG) of movements, then extract higher-order patterns from the DAG. However, DAG-based methods heavily rely on the identification of movement keypoints that are challenging for sparse movements and fail to consider the temporal variants that are critical for movements in urban environments. To overcome the limitations, we propose HoLens, a novel approach for modeling and visualizing higher-order movement patterns in the context of an urban environment. HoLens mainly makes twofold contributions: first, we design an auto-adaptive movement aggregation algorithm that self-organizes movements hierarchically by considering spatial proximity, contextual information, and temporal variability; second, we develop an interactive visual analytics interface consisting of well-established visualization techniques, including the H-Flow for visualizing the higher-order patterns on the map and the higher-order state sequence chart for representing the higher-order state transitions. Two real-world case studies manifest that the method can adaptively aggregate the data and exhibit the process of how to explore the higher-order patterns by HoLens. We also demonstrate our approach's feasibility, usability, and effectiveness through an expert interview with three domain experts.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Abstract:In recent years, open-source software (OSS) has become increasingly prevalent in developing software products. While OSS documentation is the primary source of information provided by the developers' community about a product, its role in the industry's adoption process has yet to be examined. We conducted semi-structured interviews and an online survey to provide insight into this area. Based on interviews and survey insights, we developed a topic model to collect relevant information from OSS documentation automatically. Additionally, according to our survey responses regarding challenges associated with OSS documentation, we propose a novel information augmentation approach, DocMentor, by combining OSS documentation corpus TF-IDF scores and ChatGPT. Through explaining technical terms and providing examples and references, our approach enhances the documentation context and improves practitioners' understanding. Our tool's effectiveness is assessed by surveying practitioners.\n",
      "Abstract:Large language models (LLMs) need to serve everyone, including a global majority of non-English speakers. However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent research shows that, despite limits in their intended use, people prompt LLMs in many different languages. Therefore, in this paper, we investigate the basic multilingual capabilities of state-of-the-art open LLMs beyond their intended use. For this purpose, we introduce MultiQ, a new silver standard benchmark for basic open-ended question answering with 27.4k test questions across a typologically diverse set of 137 languages. With MultiQ, we evaluate language fidelity, i.e.\\ whether models respond in the prompted language, and question answering accuracy. All LLMs we test respond faithfully and/or accurately for at least some languages beyond their intended use. Most models are more accurate when they respond faithfully. However, differences across models are large, and there is a long tail of languages where models are neither accurate nor faithful. We explore differences in tokenization as a potential explanation for our findings, identifying possible correlations that warrant further investigation.\n",
      "Abstract:Used car pricing is a critical aspect of the automotive industry, influenced by many economic factors and market dynamics. With the recent surge in online marketplaces and increased demand for used cars, accurate pricing would benefit both buyers and sellers by ensuring fair transactions. However, the transition towards automated pricing algorithms using machine learning necessitates the comprehension of model uncertainties, specifically the ability to flag predictions that the model is unsure about. Although recent literature proposes the use of boosting algorithms or nearest neighbor-based approaches for swift and precise price predictions, encapsulating model uncertainties with such algorithms presents a complex challenge. We introduce ProbSAINT, a model that offers a principled approach for uncertainty quantification of its price predictions, along with accurate point predictions that are comparable to state-of-the-art boosting techniques. Furthermore, acknowledging that the business prefers pricing used cars based on the number of days the vehicle was listed for sale, we show how ProbSAINT can be used as a dynamic forecasting model for predicting price probabilities for different expected offer duration. Our experiments further indicate that ProbSAINT is especially accurate on instances where it is highly certain. This proves the applicability of its probabilistic predictions in real-world scenarios where trustworthiness is crucial.\n",
      "Abstract:In order to compute the Fourier transform of a function $f$ on the real line numerically, one samples $f$ on a grid and then takes the discrete Fourier transform. We derive exact error estimates for this procedure in terms of the decay and smoothness of $f$. The analysis provides a new recipe of how to relate the number of samples, the sampling interval, and the grid size.\n",
      "Abstract:Self-reflecting about our performance (e.g., how confident we are) before doing a task is essential for decision making, such as selecting the most suitable tool or choosing the best route to drive. While this form of awareness -- thinking about our performance or metacognitive performance -- is well-known in humans, robots still lack this cognitive ability. This reflective monitoring can enhance their embodied decision power, robustness and safety. Here, we take a step in this direction by introducing a mathematical framework that allows robots to use their control self-confidence to make better-informed decisions. We derive a mathematical closed-form expression for control confidence for dynamic systems (i.e., the posterior inverse covariance of the control action). This control confidence seamlessly integrates within an objective function for decision making, that balances the: i) performance for task completion, ii) control effort, and iii) self-confidence. To evaluate our theoretical account, we framed the decision-making within the tool selection problem, where the agent has to select the best robot arm for a particular control task. The statistical analysis of the numerical simulations with randomized 2DOF arms shows that using control confidence during tool selection improves both real task performance, and the reliability of the tool for performance under unmodelled perturbations (e.g., external forces). Furthermore, our results indicate that control confidence is an early indicator of performance and thus, it can be used as a heuristic for making decisions when computation power is restricted or decision-making is intractable. Overall, we show the advantages of using confidence-aware decision-making and control scheme for dynamic systems.\n",
      "Abstract:We propose a method for autonomous precision drone landing with fiducial markers and a gimbal-mounted, multi-payload camera with wide-angle, zoom, and IR sensors. The method has minimal data requirements; it depends primarily on the direction from the drone to the landing pad, enabling it to switch dynamically between the camera's different sensors and zoom factors, and minimizing auxiliary sensor requirements. It eliminates the need for data such as altitude above ground level, straight-line distance to the landing pad, fiducial marker size, and 6 DoF marker pose (of which the orientation is problematic). We leverage the zoom and wide-angle cameras, as well as visual April Tag fiducial markers to conduct successful precision landings from much longer distances than in previous work (168m horizontal distance, 102m altitude). We use two types of April Tags in the IR spectrum - active and passive - for precision landing both at daytime and nighttime, instead of simple IR beacons used in most previous work. The active IR landing pad is heated; the novel, passive one is unpowered, at ambient temperature, and depends on its high reflectivity and an IR differential between the ground and the sky. Finally, we propose a high-level control policy to manage initial search for the landing pad and subsequent searches if it is lost - not addressed in previous work. The method demonstrates successful landings with the landing skids at least touching the landing pad, achieving an average error of 0.19m. It also demonstrates successful recovery and landing when the landing pad is temporarily obscured.\n",
      "Abstract:Euler diagrams are a tool for the graphical representation of set relations. Due to their simple way of visualizing elements in the sets by geometric containment, they are easily readable by an inexperienced reader. Euler diagrams where the sets are visualized as aligned rectangles are of special interest. In this work, we link the existence of such rectangular Euler diagrams to the order dimension of an associated order relation. For this, we consider Euler diagrams in one and two dimensions. In the one-dimensional case, this correspondence provides us with a polynomial-time algorithm to compute the Euler diagrams, while the two-dimensional case results in an exponential-time algorithm.\n",
      "No failed requests to retry.\n"
     ]
    }
   ],
   "source": [
    "# # 定义最大重试次数\n",
    "# max_retries = 3\n",
    "\n",
    "# for identifier2 in identifier_list2:\n",
    "#     link_list = identifier2.find('a', {'title': 'Abstract'})\n",
    "#     abs_url = link_list['href']\n",
    "#     abs_full_url = urljoin(\"https://arxiv.org\", abs_url)\n",
    "    \n",
    "#     # 初始化重试次数\n",
    "#     retry_count = 0\n",
    "#     while retry_count < max_retries:\n",
    "#         try:\n",
    "#             response = requests.get(abs_full_url, timeout=20)  \n",
    "#             sleep(2)  # 在每次请求之后等待2秒钟\n",
    "#             response.raise_for_status()  # 检查请求是否成功\n",
    "#             break  # 请求成功，跳出循环\n",
    "#         except (Timeout, RequestException) as e:\n",
    "#             print(f\"Request error: {e}\")\n",
    "#             retry_count += 1\n",
    "#             if retry_count == max_retries:\n",
    "#                 print(\"Max retries reached, unable to fetch data\")\n",
    "#                 break  # 达到最大重试次数，跳出循环\n",
    "#             else:\n",
    "#                 print(f\"Retrying... Attempt {retry_count}\")\n",
    "#                 sleep(2)  # 在重试之前等待一段时间\n",
    "    \n",
    "#     if retry_count < max_retries:  # 如果请求成功\n",
    "#         soup1 = BeautifulSoup(response.text, 'html.parser')  # Specify parser explicitly\n",
    "#         abstract = soup1.find('blockquote', class_='abstract mathjax').text.strip()\n",
    "#         title = soup1.find('h1', class_='title mathjax').text.strip()\n",
    "#         authors = soup1.find('div', class_='authors').text.strip().replace(',', ';')\n",
    "#         subjects = soup1.find('span', class_='primary-subject').text.strip()\n",
    "#         paper_info = {\n",
    "#             'Title': title,\n",
    "#             'Subjects': subjects,\n",
    "#             'Authors': authors,\n",
    "#             'Abstract': abstract\n",
    "#         }\n",
    "#         papers_info.append(paper_info)\n",
    "#         print(abstract)  # Print or do whatever you need with the abstract\n",
    "\n",
    "import requests\n",
    "from time import sleep\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "# 存储失败的请求\n",
    "failed_requests = []\n",
    "# 定义最大重试次数\n",
    "max_retries = 5\n",
    "for identifier2 in identifier_list2:\n",
    "    link_list = identifier2.find('a', {'title': 'Abstract'})\n",
    "    abs_url = link_list['href']\n",
    "    abs_full_url = urljoin(\"https://arxiv.org\", abs_url)\n",
    "    \n",
    "    # 初始化重试次数\n",
    "    retry_count = 0\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            response = requests.get(abs_full_url, timeout=20)  # 设置超时时间为10秒\n",
    "            sleep(2)  # 在每次请求之后等待2秒钟\n",
    "            response.raise_for_status()  # 检查请求是否成功\n",
    "            break  # 请求成功，跳出循环\n",
    "        except (Timeout, RequestException) as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == max_retries:\n",
    "                print(\"Max retries reached, unable to fetch data\")\n",
    "                # 将失败的请求记录下来\n",
    "                failed_requests.append(abs_full_url)\n",
    "                break  # 达到最大重试次数，跳出循环\n",
    "            else:\n",
    "                print(f\"Retrying... Attempt {retry_count}\")\n",
    "                sleep(2)  # 在重试之前等待一段时间\n",
    "    \n",
    "    if retry_count < max_retries:  # 如果请求成功\n",
    "        soup1 = BeautifulSoup(response.text, 'html.parser')  # Specify parser explicitly\n",
    "        abstract = soup1.find('blockquote', class_='abstract mathjax').text.strip()\n",
    "        title = soup1.find('h1', class_='title mathjax').text.strip()\n",
    "        authors = soup1.find('div', class_='authors').text.strip().replace(',', ';')\n",
    "        subjects = soup1.find('span', class_='primary-subject').text.strip()\n",
    "        paper_info = {\n",
    "            'Title': title,\n",
    "            'Subjects': subjects,\n",
    "            'Authors': authors,\n",
    "            'Abstract': abstract\n",
    "        }\n",
    "        papers_info.append(paper_info)\n",
    "        print(abstract)  # Print or do whatever you need with the abstract\n",
    "\n",
    "if failed_requests:  # 检查 failed_requests 是否为空\n",
    "    for failed_request in failed_requests:\n",
    "        try:\n",
    "            response = requests.get(failed_request, timeout=20)\n",
    "            sleep(2)  # 在每次请求之后等待2秒钟\n",
    "            response.raise_for_status()  # 检查请求是否成功\n",
    "            break  # 请求成功，跳出循环\n",
    "        except (Timeout, RequestException) as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == max_retries:\n",
    "                print(\"Max retries reached, unable to fetch data\")\n",
    "                # 将失败的请求记录下来\n",
    "                failed_requests.append(abs_full_url)\n",
    "                break  # 达到最大重试次数，跳出循环\n",
    "            else:\n",
    "                print(f\"Retrying... Attempt {retry_count}\")\n",
    "                sleep(2)  # 在重试之前等待一段时间\n",
    "    \n",
    "    if retry_count < max_retries:  # 如果请求成功\n",
    "        soup1 = BeautifulSoup(response.text, 'html.parser')  # Specify parser explicitly\n",
    "        abstract = soup1.find('blockquote', class_='abstract mathjax').text.strip()\n",
    "        title = soup1.find('h1', class_='title mathjax').text.strip()\n",
    "        authors = soup1.find('div', class_='authors').text.strip().replace(',', ';')\n",
    "        subjects = soup1.find('span', class_='primary-subject').text.strip()\n",
    "        paper_info = {\n",
    "            'Title': title,\n",
    "            'Subjects': subjects,\n",
    "            'Authors': authors,\n",
    "            'Abstract': abstract\n",
    "        }\n",
    "        papers_info.append(paper_info)\n",
    "        print(abstract) \n",
    "else:\n",
    "    print(\"No failed requests to retry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19b44671",
   "metadata": {},
   "outputs": [],
   "source": [
    "r3 = requests.get(\"https://arxiv.org/list/cs/pastweek?skip=75&show=25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5012150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3 = BeautifulSoup(r3.text,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e8395cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_list3 = soup3.find_all('span', class_='list-identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbc4dc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract:We introduce a new family of prompt injection attacks, termed Neural Exec. Unlike known attacks that rely on handcrafted strings (e.g., \"Ignore previous instructions and...\"), we show that it is possible to conceptualize the creation of execution triggers as a differentiable search problem and use learning-based methods to autonomously generate them.\n",
      "Our results demonstrate that a motivated adversary can forge triggers that are not only drastically more effective than current handcrafted ones but also exhibit inherent flexibility in shape, properties, and functionality. In this direction, we show that an attacker can design and generate Neural Execs capable of persisting through multi-stage preprocessing pipelines, such as in the case of Retrieval-Augmented Generation (RAG)-based applications. More critically, our findings show that attackers can produce triggers that deviate markedly in form and shape from any known attack, sidestepping existing blacklist-based detection and sanitation approaches.\n",
      "Abstract:Treatment effect estimation (TEE) is the task of determining the impact of various treatments on patient outcomes. Current TEE methods fall short due to reliance on limited labeled data and challenges posed by sparse and high-dimensional observational patient data. To address the challenges, we introduce a novel pre-training and fine-tuning framework, KG-TREAT, which synergizes large-scale observational patient data with biomedical knowledge graphs (KGs) to enhance TEE. Unlike previous approaches, KG-TREAT constructs dual-focus KGs and integrates a deep bi-level attention synergy method for in-depth information fusion, enabling distinct encoding of treatment-covariate and outcome-covariate relationships. KG-TREAT also incorporates two pre-training tasks to ensure a thorough grounding and contextualization of patient data and KGs. Evaluation on four downstream TEE tasks shows KG-TREAT's superiority over existing methods, with an average improvement of 7% in Area under the ROC Curve (AUC) and 9% in Influence Function-based Precision of Estimating Heterogeneous Effects (IF-PEHE). The effectiveness of our estimated treatment effects is further affirmed by alignment with established randomized clinical trial findings.\n",
      "Abstract:Ship detection needs to identify ship locations from remote sensing (RS) scenes. However, due to different imaging payloads, various appearances of ships, and complicated background interference from the bird's eye view, it is difficult to set up a unified paradigm for achieving multi-source ship detection. Therefore, in this article, considering that the large language models (LLMs) emerge the powerful generalization ability, a novel unified visual-language model called Popeye is proposed for multi-source ship detection from RS imagery. First, to bridge the interpretation gap between multi-source images for ship detection, a novel image-instruction-answer way is designed to integrate the various ship detection ways (e.g., horizontal bounding box (HBB), oriented bounding box (OBB)) into a unified labeling paradigm. Then, in view of this, a cross-modal image interpretation method is developed for the proposed Popeye to enhance interactive comprehension ability between visual and language content, which can be easily migrated into any multi-source ship detection task. Subsequently, owing to objective domain differences, a knowledge adaption mechanism is designed to adapt the pre-trained visual-language knowledge from the nature scene into the RS domain for multi-source ship detection. In addition, the segment anything model (SAM) is also seamlessly integrated into the proposed Popeye to achieve pixel-level ship segmentation without additional training costs. Finally, extensive experiments are conducted on the newly constructed instruction dataset named MMShip, and the results indicate that the proposed Popeye outperforms current specialist, open-vocabulary, and other visual-language models for zero-shot multi-source ship detection.\n",
      "Abstract:The growing dependence on Large Language Models (LLMs) for finishing user instructions necessitates a comprehensive understanding of their robustness to complex task completion in real-world situations. To address this critical need, we propose the PowerPoint Task Completion Robustness benchmark (PPTC-R) to measure LLMs' robustness to the user PPT task instruction and software version. Specifically, we construct adversarial user instructions by attacking user instructions at sentence, semantic, and multi-language levels. To assess the robustness of Language Models to software versions, we vary the number of provided APIs to simulate both the newest version and earlier version settings. Subsequently, we test 3 closed-source and 4 open-source LLMs using a benchmark that incorporates these robustness settings, aiming to evaluate how deviations impact LLMs' API calls for task completion. We find that GPT-4 exhibits the highest performance and strong robustness in our benchmark, particularly in the version update and the multilingual settings. However, we find that all LLMs lose their robustness when confronted with multiple challenges (e.g., multi-turn) simultaneously, leading to significant performance drops. We further analyze the robustness behavior and error reasons of LLMs in our benchmark, which provide valuable insights for researchers to understand the LLM's robustness in task completion and develop more robust LLMs and agents. We release the code and data at \\url{this https URL}.\n",
      "Abstract:Due to the recent increase in interest in Financial Technology (FinTech), applications like credit default prediction (CDP) are gaining significant industrial and academic attention. In this regard, CDP plays a crucial role in assessing the creditworthiness of individuals and businesses, enabling lenders to make informed decisions regarding loan approvals and risk management. In this paper, we propose a workflow-based approach to improve CDP, which refers to the task of assessing the probability that a borrower will default on his or her credit obligations. The workflow consists of multiple steps, each designed to leverage the strengths of different techniques featured in machine learning pipelines and, thus best solve the CDP task. We employ a comprehensive and systematic approach starting with data preprocessing using Weight of Evidence encoding, a technique that ensures in a single-shot data scaling by removing outliers, handling missing values, and making data uniform for models working with different data types. Next, we train several families of learning models, introducing ensemble techniques to build more robust models and hyperparameter optimization via multi-objective genetic algorithms to consider both predictive accuracy and financial aspects. Our research aims at contributing to the FinTech industry in providing a tool to move toward more accurate and reliable credit risk assessment, benefiting both lenders and borrowers.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Abstract:Arguably, geodesics are the most important geometric objects on a differentiable manifold. They describe candidates for shortest paths and are guaranteed to be unique shortest paths when the starting velocity stays within the so-called injectivity radius of the manifold. In this work, we investigate the injectivity radius of the Stiefel manifold under the canonical metric. The Stiefel manifold $St(n,p)$ is the set of rectangular matrices of dimension $n$-by-$p$ with orthogonal columns, sometimes also called the space of orthogonal $p$-frames in $\\mathbb{R}^n$. Using a standard curvature argument, Rentmeesters has shown in 2013 that the injectivity radius of the Stiefel manifold is bounded by $\\sqrt{\\frac{4}{5}}\\pi$. It is an open question, whether this bound is sharp. With the definition of the injectivity radius via cut points of geodesics, we gain access to the information of the injectivity radius by investigating geodesics. More precisely, we consider the behavior of special variations of geodesics, called Jacobi fields. By doing so, we are able to present an explicit example of a cut point. In addition, since the theoretical analysis of geodesics for cut points and especially conjugate points as a type of cut points is difficult, we investigate the question of the sharpness of the bound by means of numerical experiments.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 2\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 3\n",
      "Abstract:Neural network models have a number of hyperparameters that must be chosen along with their architecture. This can be a heavy burden on a novice user, choosing which architecture and what values to assign to parameters. In most cases, default hyperparameters and architectures are used. Significant improvements to model accuracy can be achieved through the evaluation of multiple architectures. A process known as Neural Architecture Search (NAS) may be applied to automatically evaluate a large number of such architectures. A system integrating open source tools for Neural Architecture Search (OpenNAS), in the classification of images, has been developed as part of this research. OpenNAS takes any dataset of grayscale, or RBG images, and generates Convolutional Neural Network (CNN) architectures based on a range of metaheuristics using either an AutoKeras, a transfer learning or a Swarm Intelligence (SI) approach. Particle Swarm Optimization (PSO) and Ant Colony Optimization (ACO) are used as the SI algorithms. Furthermore, models developed through such metaheuristics may be combined using stacking ensembles. In the context of this paper, we focus on training and optimizing CNNs using the Swarm Intelligence (SI) components of OpenNAS. Two major types of SI algorithms, namely PSO and ACO, are compared to see which is more effective in generating higher model accuracies. It is shown, with our experimental design, that the PSO algorithm performs better than ACO. The performance improvement of PSO is most notable with a more complex dataset. As a baseline, the performance of fine-tuned pre-trained models is also evaluated.\n",
      "Abstract:We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our method, called Expectile-Regularised Neural Optimal Transport (ENOT). ENOT outperforms previous state-of-the-art approaches on the Wasserstein-2 benchmark tasks by a large margin (up to a 3-fold improvement in quality and up to a 10-fold improvement in runtime).\n",
      "Abstract:Counterfactual explanations (CEs) enhance the interpretability of machine learning models by describing what changes to an input are necessary to change its prediction to a desired class. These explanations are commonly used to guide users' actions, e.g., by describing how a user whose loan application was denied can be approved for a loan in the future. Existing approaches generate CEs by focusing on a single, fixed model, and do not provide any formal guarantees on the CEs' future validity. When models are updated periodically to account for data shift, if the generated CEs are not robust to the shifts, users' actions may no longer have the desired impacts on their predictions. This paper introduces VeriTraCER, an approach that jointly trains a classifier and an explainer to explicitly consider the robustness of the generated CEs to small model shifts. VeriTraCER optimizes over a carefully designed loss function that ensures the verifiable robustness of CEs to local model updates, thus providing deterministic guarantees to CE validity. Our empirical evaluation demonstrates that VeriTraCER generates CEs that (1) are verifiably robust to small model updates and (2) display competitive robustness to state-of-the-art approaches in handling empirical model updates including random initialization, leave-one-out, and distribution shifts.\n",
      "Abstract:Existing causal discovery methods based on combinatorial optimization or search are slow, prohibiting their application on large-scale datasets. In response, more recent methods attempt to address this limitation by formulating causal discovery as structure learning with continuous optimization but such approaches thus far provide no statistical guarantees. In this paper, we show that by efficiently parallelizing existing causal discovery methods, we can in fact scale them to thousands of dimensions, making them practical for substantially larger-scale problems. In particular, we parallelize the LiNGAM method, which is quadratic in the number of variables, obtaining up to a 32-fold speed-up on benchmark datasets when compared with existing sequential implementations. Specifically, we focus on the causal ordering subprocedure in DirectLiNGAM and implement GPU kernels to accelerate it. This allows us to apply DirectLiNGAM to causal inference on large-scale gene expression data with genetic interventions yielding competitive results compared with specialized continuous optimization methods, and Var-LiNGAM for causal discovery on U.S. stock data.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 2\n",
      "Abstract:The field of pharmaceutical development and therapeutic application both face substantial challenges. Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development. Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis. Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\\% in patient-level CRE, and a 5-fold increase in indication-level CRE. Furthermore, DeepCRE has identified six drug candidates that show significantly greater effectiveness than a comparator set of two approved drug in 5/8 colorectal cancer (CRC) organoids. This highlights DeepCRE's ability to identify a collection of drug candidates with superior therapeutic effects, underscoring its potential to revolutionize the field of therapeutic development.\n",
      "Abstract:One common way to speed up the find operation within a set of text files involves a trigram index. This structure is merely a map from a trigram (sequence consisting of three characters) to a set of files which contain it. When searching for a pattern, potential file locations are identified by intersecting the sets related to the trigrams in the pattern. Then, the search proceeds only in these files.\n",
      "However, in a code repository, the trigram index evolves across different versions. Upon checking out a new version, this index is typically built from scratch, which is a time-consuming task, while we want our index to have almost zero-time startup.\n",
      "Thus, we explore the persistent version of a trigram index for full-text and key word patterns search. Our approach just uses the current version of the trigram index and applies only the changes between versions during checkout, significantly enhancing performance. Furthermore, we extend our data structure to accommodate CamelHump search for class and function names.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Abstract:The advent of Large Language Models (LLMs) has led to remarkable progress on a wide range of natural language processing tasks. Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document. Previous research addresses the challenging task of detecting hallucinations in the output (i.e. inconsistency detection) in order to evaluate the faithfulness of the generated summaries. However, these works primarily focus on English and recent multilingual approaches lack German data. This work presents absinth, a manually annotated dataset for hallucination detection in German news summarization and explores the capabilities of novel open-source LLMs on this task in both fine-tuning and in-context learning settings. We open-source and release the absinth dataset to foster further research on hallucination detection in German.\n",
      "Abstract:In human-robot interaction (HRI), we study how humans interact with robots, but also the effects of robot behavior on human perception and well-being. Especially, the influence on humans by tandem robots with one human controlled and one autonomous robot or even semi-autonomous multi-robot systems is not yet fully understood. Here, we focus on a leader-follower scenario and study how emotionally expressive motion patterns of a small, mobile follower robot affect the perception of a human operator controlling the leading robot. We examined three distinct emotional behaviors for the follower compared to a neutral condition: angry, happy and sad. We analyzed how participants maneuvered the leader robot along a set path while experiencing each follower behavior in a randomized order. We identified a significant shift in attention toward the follower with emotionally expressive behaviors compared to the neutral condition. For example, the angry behavior significantly heightened participant stress levels and was considered the least preferred behavior. The happy behavior was the most preferred and associated with increased excitement by the participants. Integrating the proposed behaviors in robots can profoundly influence the human operator's attention, emotional state, and overall experience. These insights are valuable for future HRI tandem robot designs.\n",
      "Abstract:The capabilities of large language models (LLMs) have been progressing at a breathtaking speed, leaving even their own developers grappling with the depth of their potential and risks. While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses, to our knowledge, the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights. To this end, we carry out the first safety evaluation for medical LLMs. Specifically, we set forth a definition of medical safety and alignment for medical artificial intelligence systems, develop a dataset of harmful medical questions to evaluate the medical safety and alignment of an LLM, evaluate both general and medical safety and alignment of medical LLMs, demonstrate fine-tuning as an effective mitigation strategy, and discuss broader, large-scale approaches used by the machine learning community to develop safe and aligned LLMs. We hope that this work casts light on the safety and alignment of medical LLMs and motivates future work to study it and develop additional mitigation strategies, minimizing the risks of harm of LLMs in medicine.\n",
      "Abstract:As virtual reality (VR) becomes more popular for intergenerational collaboration, there is still a significant gap in research regarding understanding the potential for reducing ageism. Our study aims to address this gap by analyzing ageism levels before and after VR escape room collaborative experiences. We recruited 28 participants to collaborate with an older player in a challenging VR escape room game. To ensure consistent and reliable performance data of older players, our experimenters simulated older participants following specific guidelines. After completing the game, we found a significant reduction in ageism among younger participants. Furthermore, we introduce a new game mechanism that encourages intergenerational collaboration. Our research highlights the potential of VR collaborative games as a practical tool for mitigating ageism. It provides valuable insights for designing immersive VR experiences that foster enhanced intergenerational collaboration.\n",
      "Abstract:Active learning is a machine learning paradigm designed to optimize model performance in a setting where labeled data is expensive to acquire. In this work, we propose a novel active learning method called SUPClust that seeks to identify points at the decision boundary between classes. By targeting these points, SUPClust aims to gather information that is most informative for refining the model's prediction of complex decision regions. We demonstrate experimentally that labeling these points leads to strong model performance. This improvement is observed even in scenarios characterized by strong class imbalance.\n",
      "Abstract:In the domain of image layout representation learning, the critical process of translating image layouts into succinct vector forms is increasingly significant across diverse applications, such as image retrieval, manipulation, and generation. Most approaches in this area heavily rely on costly labeled datasets and notably lack in adapting their modeling and learning methods to the specific nuances of photographic image layouts. This shortfall makes the learning process for photographic image layouts suboptimal. In our research, we directly address these challenges. We innovate by defining basic layout primitives that encapsulate various levels of layout information and by mapping these, along with their interconnections, onto a heterogeneous graph structure. This graph is meticulously engineered to capture the intricate layout information within the pixel domain explicitly. Advancing further, we introduce novel pretext tasks coupled with customized loss functions, strategically designed for effective self-supervised learning of these layout graphs. Building on this foundation, we develop an autoencoder-based network architecture skilled in compressing these heterogeneous layout graphs into precise, dimensionally-reduced layout representations. Additionally, we introduce the LODB dataset, which features a broader range of layout categories and richer semantics, serving as a comprehensive benchmark for evaluating the effectiveness of layout representation learning methods. Our extensive experimentation on this dataset demonstrates the superior performance of our approach in the realm of photographic image layout representation learning.\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 1\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 2\n",
      "Request error: HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=20)\n",
      "Retrying... Attempt 3\n",
      "Abstract:Binary neural networks utilize 1-bit quantized weights and activations to reduce both the model's storage demands and computational burden. However, advanced binary architectures still incorporate millions of inefficient and nonhardware-friendly full-precision multiplication operations. A&B BNN is proposed to directly remove part of the multiplication operations in a traditional BNN and replace the rest with an equal number of bit operations, introducing the mask layer and the quantized RPReLU structure based on the normalizer-free network architecture. The mask layer can be removed during inference by leveraging the intrinsic characteristics of BNN with straightforward mathematical transformations to avoid the associated multiplication operations. The quantized RPReLU structure enables more efficient bit operations by constraining its slope to be integer powers of 2. Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFAR-10, CIFAR-100, and ImageNet datasets, respectively, which are competitive with the state-of-the-art. Ablation studies have verified the efficacy of the quantized RPReLU structure, leading to a 1.14% enhancement on the ImageNet compared to using a fixed slope RLeakyReLU. The proposed add&bit-operation-only BNN offers an innovative approach for hardware-friendly network architecture.\n",
      "Abstract:Topic modelling was mostly dominated by Bayesian graphical models during the last decade. With the rise of transformers in Natural Language Processing, however, several successful models that rely on straightforward clustering approaches in transformer-based embedding spaces have emerged and consolidated the notion of topics as clusters of embedding vectors. We propose the Transformer-Representation Neural Topic Model (TNTM), which combines the benefits of topic representations in transformer-based embedding spaces and probabilistic modelling. Therefore, this approach unifies the powerful and versatile notion of topics based on transformer embeddings with fully probabilistic modelling, as in models such as Latent Dirichlet Allocation (LDA). We utilize the variational autoencoder (VAE) framework for improved inference speed and modelling flexibility. Experimental results show that our proposed model achieves results on par with various state-of-the-art approaches in terms of embedding coherence while maintaining almost perfect topic diversity. The corresponding source code is available at this https URL.\n",
      "Abstract:Recent progress in generative compression technology has significantly improved the perceptual quality of compressed data. However, these advancements primarily focus on producing high-frequency details, often overlooking the ability of generative models to capture the prior distribution of image content, thus impeding further bitrate reduction in extreme compression scenarios (<0.05 bpp). Motivated by the capabilities of predictive language models for lossless compression, this paper introduces a novel Unified Image Generation-Compression (UIGC) paradigm, merging the processes of generation and compression. A key feature of the UIGC framework is the adoption of vector-quantized (VQ) image models for tokenization, alongside a multi-stage transformer designed to exploit spatial contextual information for modeling the prior distribution. As such, the dual-purpose framework effectively utilizes the learned prior for entropy estimation and assists in the regeneration of lost tokens. Extensive experiments demonstrate the superiority of the proposed UIGC framework over existing codecs in perceptual quality and human perception, particularly in ultra-low bitrate scenarios (<=0.03 bpp), pioneering a new direction in generative compression.\n",
      "Abstract:As part of human core knowledge, the representation of objects is the building block of mental representation that supports high-level concepts and symbolic reasoning. While humans develop the ability of perceiving objects situated in 3D environments without supervision, models that learn the same set of abilities with similar constraints faced by human infants are lacking. Towards this end, we developed a novel network architecture that simultaneously learns to 1) segment objects from discrete images, 2) infer their 3D locations, and 3) perceive depth, all while using only information directly available to the brain as training data, namely: sequences of images and self-motion. The core idea is treating objects as latent causes of visual input which the brain uses to make efficient predictions of future scenes. This results in object representations being learned as an essential byproduct of learning to predict.\n",
      "Abstract:This study addresses the integration of diversity-based and uncertainty-based sampling strategies in active learning, particularly within the context of self-supervised pre-trained models. We introduce a straightforward heuristic called TCM that mitigates the cold start problem while maintaining strong performance across various data levels. By initially applying TypiClust for diversity sampling and subsequently transitioning to uncertainty sampling with Margin, our approach effectively combines the strengths of both strategies. Our experiments demonstrate that TCM consistently outperforms existing methods across various datasets in both low and high data regimes.\n",
      "Abstract:In environments like offices, the duration of a robot's navigation between two locations may vary over time. For instance, reaching a kitchen may take more time during lunchtime since the corridors are crowded with people heading the same way. In this work, we address the problem of routing in such environments with tasks expressed in Metric Interval Temporal Logic (MITL) - a rich robot task specification language that allows us to capture explicit time requirements. Our objective is to find a strategy that maximizes the temporal robustness of the robot's MITL task. As the first step towards a solution, we define a Mixed-integer linear programming approach to solving the task planning problem over a Varying Weighted Transition System, where navigation durations are deterministic but vary depending on the time of day. Then, we apply this planner to optimize for MITL temporal robustness in Markov Decision Processes, where the navigation durations between physical locations are uncertain, but the time-dependent distribution over possible delays is known. Finally, we develop a receding horizon planner for Markov Decision Processes that preserves guarantees over MITL temporal robustness. We show the scalability of our planning algorithms in simulations of robotic tasks.\n",
      "Abstract:Protein design requires a deep understanding of the inherent complexities of the protein universe. While many efforts lean towards conditional generation or focus on specific families of proteins, the foundational task of unconditional generation remains underexplored and undervalued. Here, we explore this pivotal domain, introducing DiMA, a model that leverages continuous diffusion on embeddings derived from the protein language model, ESM-2, to generate amino acid sequences. DiMA surpasses leading solutions, including autoregressive transformer-based and discrete diffusion models, and we quantitatively illustrate the impact of the design choices that lead to its superior performance. We extensively evaluate the quality, diversity, distribution similarity, and biological relevance of the generated sequences using multiple metrics across various modalities. Our approach consistently produces novel, diverse protein sequences that accurately reflect the inherent structural and functional diversity of the protein space. This work advances the field of protein design and sets the stage for conditional models by providing a robust framework for scalable and high-quality protein sequence generation.\n",
      "No failed requests to retry.\n"
     ]
    }
   ],
   "source": [
    "# # 定义最大重试次数\n",
    "# max_retries = 3\n",
    "\n",
    "# for identifier3 in identifier_list3:\n",
    "#     link_list = identifier3.find('a', {'title': 'Abstract'})\n",
    "#     abs_url = link_list['href']\n",
    "#     abs_full_url = urljoin(\"https://arxiv.org\", abs_url)\n",
    "    \n",
    "#     # 初始化重试次数\n",
    "#     retry_count = 0\n",
    "#     while retry_count < max_retries:\n",
    "#         try:\n",
    "#             response = requests.get(abs_full_url, timeout=20)  \n",
    "#             sleep(2)  # 在每次请求之后等待2秒钟\n",
    "#             response.raise_for_status()  # 检查请求是否成功\n",
    "#             break  # 请求成功，跳出循环\n",
    "#         except (Timeout, RequestException) as e:\n",
    "#             print(f\"Request error: {e}\")\n",
    "#             retry_count += 1\n",
    "#             if retry_count == max_retries:\n",
    "#                 print(\"Max retries reached, unable to fetch data\")\n",
    "#                 break  # 达到最大重试次数，跳出循环\n",
    "#             else:\n",
    "#                 print(f\"Retrying... Attempt {retry_count}\")\n",
    "#                 sleep(2)  # 在重试之前等待一段时间\n",
    "    \n",
    "#     if retry_count < max_retries:  # 如果请求成功\n",
    "#         soup1 = BeautifulSoup(response.text, 'html.parser')  # Specify parser explicitly\n",
    "#         abstract = soup1.find('blockquote', class_='abstract mathjax').text.strip()\n",
    "#         title = soup1.find('h1', class_='title mathjax').text.strip()\n",
    "#         authors = soup1.find('div', class_='authors').text.strip().replace(',', ';')\n",
    "#         subjects = soup1.find('span', class_='primary-subject').text.strip()\n",
    "#         paper_info = {\n",
    "#             'Title': title,\n",
    "#             'Subjects': subjects,\n",
    "#             'Authors': authors,\n",
    "#             'Abstract': abstract\n",
    "#         }\n",
    "#         papers_info.append(paper_info)\n",
    "#         print(abstract)  # Print or do whatever you need with the abstract\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "from time import sleep\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "# 存储失败的请求\n",
    "failed_requests = []\n",
    "# 定义最大重试次数\n",
    "max_retries = 5\n",
    "for identifier3 in identifier_list3:\n",
    "    link_list = identifier3.find('a', {'title': 'Abstract'})\n",
    "    abs_url = link_list['href']\n",
    "    abs_full_url = urljoin(\"https://arxiv.org\", abs_url)\n",
    "    \n",
    "    # 初始化重试次数\n",
    "    retry_count = 0\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            response = requests.get(abs_full_url, timeout=20)  # 设置超时时间为10秒\n",
    "            sleep(2)  # 在每次请求之后等待2秒钟\n",
    "            response.raise_for_status()  # 检查请求是否成功\n",
    "            break  # 请求成功，跳出循环\n",
    "        except (Timeout, RequestException) as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == max_retries:\n",
    "                print(\"Max retries reached, unable to fetch data\")\n",
    "                # 将失败的请求记录下来\n",
    "                failed_requests.append(abs_full_url)\n",
    "                break  # 达到最大重试次数，跳出循环\n",
    "            else:\n",
    "                print(f\"Retrying... Attempt {retry_count}\")\n",
    "                sleep(2)  # 在重试之前等待一段时间\n",
    "    \n",
    "    if retry_count < max_retries:  # 如果请求成功\n",
    "        soup1 = BeautifulSoup(response.text, 'html.parser')  # Specify parser explicitly\n",
    "        abstract = soup1.find('blockquote', class_='abstract mathjax').text.strip()\n",
    "        title = soup1.find('h1', class_='title mathjax').text.strip()\n",
    "        authors = soup1.find('div', class_='authors').text.strip().replace(',', ';')\n",
    "        subjects = soup1.find('span', class_='primary-subject').text.strip()\n",
    "        paper_info = {\n",
    "            'Title': title,\n",
    "            'Subjects': subjects,\n",
    "            'Authors': authors,\n",
    "            'Abstract': abstract\n",
    "        }\n",
    "        papers_info.append(paper_info)\n",
    "        print(abstract)  # Print or do whatever you need with the abstract\n",
    "\n",
    "if failed_requests:  # 检查 failed_requests 是否为空\n",
    "    for failed_request in failed_requests:\n",
    "        try:\n",
    "            response = requests.get(failed_request, timeout=20)\n",
    "            sleep(2)  # 在每次请求之后等待2秒钟\n",
    "            response.raise_for_status()  # 检查请求是否成功\n",
    "            break  # 请求成功，跳出循环\n",
    "        except (Timeout, RequestException) as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == max_retries:\n",
    "                print(\"Max retries reached, unable to fetch data\")\n",
    "                # 将失败的请求记录下来\n",
    "                failed_requests.append(abs_full_url)\n",
    "                break  # 达到最大重试次数，跳出循环\n",
    "            else:\n",
    "                print(f\"Retrying... Attempt {retry_count}\")\n",
    "                sleep(2)  # 在重试之前等待一段时间\n",
    "    \n",
    "    if retry_count < max_retries:  # 如果请求成功\n",
    "        soup1 = BeautifulSoup(response.text, 'html.parser')  # Specify parser explicitly\n",
    "        abstract = soup1.find('blockquote', class_='abstract mathjax').text.strip()\n",
    "        title = soup1.find('h1', class_='title mathjax').text.strip()\n",
    "        authors = soup1.find('div', class_='authors').text.strip().replace(',', ';')\n",
    "        subjects = soup1.find('span', class_='primary-subject').text.strip()\n",
    "        paper_info = {\n",
    "            'Title': title,\n",
    "            'Subjects': subjects,\n",
    "            'Authors': authors,\n",
    "            'Abstract': abstract\n",
    "        }\n",
    "        papers_info.append(paper_info)\n",
    "        print(abstract) \n",
    "else:\n",
    "    print(\"No failed requests to retry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a120cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#papers_info.append(papers_info1)\n",
    "with open('papers_info.csv', mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Title', 'Subjects', 'Authors','Abstract']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # 遍历每篇论文的信息，写入 CSV 文件\n",
    "    for paper in papers_info:\n",
    "        writer.writerow({'Title': paper.get('Title', ''),\n",
    "                         'Subjects': paper.get('Subjects', ''),\n",
    "                         'Authors': paper.get('Authors', ''),\n",
    "                         'Abstract': paper.get('Abstract','') })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ac4de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
