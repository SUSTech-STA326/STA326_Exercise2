{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66e7057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests  # send request\n",
    "from bs4 import BeautifulSoup  # parse web pages\n",
    "import pandas as pd  # csv\n",
    "from time import sleep  # wait\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import csv\n",
    "from urllib.parse import urljoin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c099e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc653b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://arxiv.org/list/cs/pastweek?skip=0&show=25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "626e42b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf9c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_list = soup.find_all('span', class_='list-identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_info1 = []\n",
    "\n",
    "for identifier in identifier_list:\n",
    "    link_list = identifier.find('a', {'title': 'Abstract'})\n",
    "    if link_list:  # Check if the link is found\n",
    "        abs_url = link_list['href']\n",
    "        abs_full_url = urljoin(\"https://arxiv.org\", abs_url)\n",
    "        response = requests.get(abs_full_url)\n",
    "        soup1 = BeautifulSoup(response.text, 'html.parser')  # Specify parser explicitly\n",
    "        abstract = soup1.find('blockquote', class_='abstract mathjax').text.strip()\n",
    "        title = soup1.find('h1', class_='title mathjax').text.strip()\n",
    "        authors = soup1.find('div', class_='authors').text.strip().replace(',', ';')\n",
    "        subjects = soup1.find('span', class_='primary-subject').text.strip()\n",
    "        paper_info1 = {\n",
    "            'Title': title,\n",
    "            'Subjects': subjects,\n",
    "            'Authors': authors,\n",
    "            'Abstract': abstract\n",
    "        }\n",
    "        papers_info1.append(paper_info1)\n",
    "        print(abstract)  # Print or do whatever you need with the abstract\n",
    "    else:\n",
    "        print(\"Abstract not found for this identifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec964d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# papers_info=[]\n",
    "# for paper in paper_list:\n",
    "#     title = paper.find('div', class_='list-title mathjax').text.strip()\n",
    "#     subjects = paper.find('div', class_='list-subjects').text.strip()\n",
    "#     authors = paper.find('div', class_='list-authors').text.strip().replace(',', ';')\n",
    "#     #print(title,subjects,authors)\n",
    "    \n",
    "#     # 创建一个新的 paper_info 字典来保存每个 paper 的信息\n",
    "#     paper_info = {\n",
    "#         'Title': title,\n",
    "#         'Subjects': subjects,\n",
    "#         'Authors': authors,\n",
    "#     }\n",
    "    \n",
    "#     # 将每个 paper_info 字典添加到 papers_info 列表中\n",
    "#     papers_info.append(paper_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b4264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#papers_info.append(papers_info1)\n",
    "with open('papers_info.csv', mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Title', 'Subjects', 'Authors','Abstract']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # 遍历每篇论文的信息，写入 CSV 文件\n",
    "    for paper in papers_info1:\n",
    "        writer.writerow({'Title': paper.get('Title', ''),\n",
    "                         'Subjects': paper.get('Subjects', ''),\n",
    "                         'Authors': paper.get('Authors', ''),\n",
    "                         'Abstract': paper.get('Abstract','') })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30455bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Title': 'Title: Point Could Mamba: Point Cloud Learning via State Space Model', 'Subjects': 'Subjects: Computer Vision and Pattern Recognition (cs.CV)', 'Authors': 'Authors:\\nTao Zhang; \\nXiangtai Li; \\nHaobo Yuan; \\nShunping Ji; \\nShuicheng Yan'}, {'Title': 'Title: Mitigating Reversal Curse via Semantic-aware Permutation Training', 'Subjects': 'Subjects: Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)', 'Authors': 'Authors:\\nQingyan Guo; \\nRui Wang; \\nJunliang Guo; \\nXu Tan; \\nJiang Bian; \\nYujiu Yang'}, {'Title': 'Title: An Experimental Study of Low-Latency Video Streaming over 5G', 'Subjects': 'Subjects: Multimedia (cs.MM); Performance (cs.PF)', 'Authors': 'Authors:\\nImran Khan; \\nTuyen X. Tran; \\nMatti Hiltunen; \\nTheodore Karagioules; \\nDimitrios Koutsonikolas'}, {'Title': 'Title: AtP*: An efficient and scalable method for localizing LLM behaviour to  components', 'Subjects': 'Subjects: Machine Learning (cs.LG); Computation and Language (cs.CL)', 'Authors': 'Authors:\\nJános Kramár; \\nTom Lieberum; \\nRohin Shah; \\nNeel Nanda (Google DeepMind)'}, {'Title': 'Title: Neural Acceleration of Incomplete Cholesky Preconditioners', 'Subjects': 'Subjects: Distributed, Parallel, and Cluster Computing (cs.DC); Numerical Analysis (math.NA)', 'Authors': 'Authors:\\nJoshua Dennis Booth; \\nHongyang Sun; \\nTrevor Garnett'}, {'Title': \"Title: Dialect prejudice predicts AI decisions about people's character,  employability, and criminality\", 'Subjects': 'Subjects: Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)', 'Authors': 'Authors:\\nValentin Hofmann; \\nPratyusha Ria Kalluri; \\nDan Jurafsky; \\nSharese King'}, {'Title': 'Title: Happy Ending: An Empty Hexagon in Every Set of 30 Points', 'Subjects': 'Subjects: Computational Geometry (cs.CG); Logic in Computer Science (cs.LO); Combinatorics (math.CO)', 'Authors': 'Authors:\\nMarijn J.H. Heule; \\nManfred Scheucher'}, {'Title': 'Title: Can Transformers Capture Spatial Relations between Objects?', 'Subjects': 'Subjects: Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)', 'Authors': 'Authors:\\nChuan Wen; \\nDinesh Jayaraman; \\nYang Gao'}, {'Title': 'Title: Cost-Effective Activity Control of Asymptomatic Carriers in Layered  Temporal Social Networks', 'Subjects': 'Subjects: Social and Information Networks (cs.SI); Multiagent Systems (cs.MA)', 'Authors': 'Authors:\\nMasoumeh Moradian; \\nAresh Dadlani; \\nRasul Kairgeldin; \\nAhmad Khonsari'}, {'Title': 'Title: Few-Shot Relation Extraction with Hybrid Visual Evidence', 'Subjects': 'Subjects: Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)', 'Authors': 'Authors:\\nJiaying Gong; \\nHoda Eldardiry'}, {'Title': 'Title: Subhomogeneous Deep Equilibrium Models', 'Subjects': 'Subjects: Machine Learning (cs.LG); Numerical Analysis (math.NA); Optimization and Control (math.OC)', 'Authors': 'Authors:\\nPietro Sittoni; \\nFrancesco Tudisco'}, {'Title': 'Title: MAIDR: Making Statistical Visualizations Accessible with Multimodal Data  Representation', 'Subjects': 'Subjects: Human-Computer Interaction (cs.HC); Graphics (cs.GR)', 'Authors': 'Authors:\\nJooYoung Seo; \\nYilin Xia; \\nBongshin Lee; \\nSean McCurry; \\nYu Jun Yam'}, {'Title': 'Title: Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive  Ratio Analysis and Best-of-Both-Worlds', 'Subjects': 'Subjects: Machine Learning (cs.LG); Machine Learning (stat.ML)', 'Authors': 'Authors:\\nShinji Ito; \\nTaira Tsuchiya; \\nJunya Honda'}, {'Title': 'Title: Rethinking Inductive Biases for Surface Normal Estimation', 'Subjects': 'Subjects: Computer Vision and Pattern Recognition (cs.CV)', 'Authors': 'Authors:\\nGwangbin Bae; \\nAndrew J. Davison'}, {'Title': 'Title: Representing Guardedness in Call-by-Value and Guarded Parametrized  Monads', 'Subjects': 'Subjects: Logic in Computer Science (cs.LO)', 'Authors': 'Authors:\\nSergey Goncharov'}, {'Title': 'Title: Self-Consistent Decoding for More Factual Open Responses', 'Subjects': 'Subjects: Computation and Language (cs.CL)', 'Authors': 'Authors:\\nChristopher Malon; \\nXiaodan Zhu'}, {'Title': 'Title: Tri-Modal Motion Retrieval by Learning a Joint Embedding Space', 'Subjects': 'Subjects: Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)', 'Authors': 'Authors:\\nKangning Yin; \\nShihao Zou; \\nYuxuan Ge; \\nZheng Tian'}, {'Title': 'Title: Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents', 'Subjects': 'Subjects: Artificial Intelligence (cs.AI)', 'Authors': 'Authors:\\nDominik Jeurissen; \\nDiego Perez-Liebana; \\nJeremy Gow; \\nDuygu Cakmak; \\nJames Kwan'}, {'Title': 'Title: Hydra: Computer Vision for Data Quality Monitoring', 'Subjects': 'Subjects: Computer Vision and Pattern Recognition (cs.CV); Nuclear Experiment (nucl-ex); Instrumentation and Detectors (physics.ins-det)', 'Authors': 'Authors:\\nThomas Britton; \\nTorri Jeske; \\nDavid Lawrence; \\nKishansingh Rajput'}, {'Title': 'Title: A Bit of a Problem: Measurement Disparities in Dataset Sizes Across  Languages', 'Subjects': 'Subjects: Computation and Language (cs.CL)', 'Authors': 'Authors:\\nCatherine Arnett; \\nTyler A. Chang; \\nBenjamin K. Bergen'}, {'Title': 'Title: Know your exceptions: Towards an Ontology of Exceptions in Knowledge  Representation', 'Subjects': 'Subjects: Artificial Intelligence (cs.AI)', 'Authors': 'Authors:\\nGabriele Sacco; \\nLoris Bozzato; \\nOliver Kutz'}, {'Title': 'Title: An iterative method for the solution of Laplace-like equations in high  and very high space dimensions', 'Subjects': 'Subjects: Numerical Analysis (math.NA)', 'Authors': 'Authors:\\nHarry Yserentant'}, {'Title': 'Title: Scalable Learning of Item Response Theory Models', 'Subjects': 'Subjects: Machine Learning (cs.LG); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)', 'Authors': 'Authors:\\nSusanne Frick; \\nAmer Krivošija; \\nAlexander Munteanu'}, {'Title': 'Title: Reusing Historical Trajectories in Natural Policy Gradient via  Importance Sampling: Convergence and Convergence Rate', 'Subjects': 'Subjects: Machine Learning (cs.LG); Optimization and Control (math.OC)', 'Authors': 'Authors:\\nYifan Lin; \\nYuhao Wang; \\nEnlu Zhou'}, {'Title': 'Title: Cell-Free Massive MIMO with Multi-Antenna Users and Phase Misalignments:  A Novel Partially Coherent Transmission Framework', 'Subjects': 'Subjects: Information Theory (cs.IT); Signal Processing (eess.SP)', 'Authors': 'Authors:\\nUnnikrishnan Kunnath Ganesan; \\nTung Thanh Vu; \\nErik G. Larsson'}, {'Abstract': \"Abstract:In this work, for the first time, we demonstrate that Mamba-based point cloud methods can outperform point-based methods. Mamba exhibits strong global modeling capabilities and linear computational complexity, making it highly attractive for point cloud analysis. To enable more effective processing of 3-D point cloud data by Mamba, we propose a novel Consistent Traverse Serialization to convert point clouds into 1-D point sequences while ensuring that neighboring points in the sequence are also spatially adjacent. Consistent Traverse Serialization yields six variants by permuting the order of x, y, and z coordinates, and the synergistic use of these variants aids Mamba in comprehensively observing point cloud data. Furthermore, to assist Mamba in handling point sequences with different orders more effectively, we introduce point prompts to inform Mamba of the sequence's arrangement rules. Finally, we propose positional encoding based on spatial coordinate mapping to inject positional information into point cloud sequences better. Based on these improvements, we construct a point cloud network named Point Cloud Mamba, which combines local and global modeling. Point Cloud Mamba surpasses the SOTA point-based method PointNeXt and achieves new SOTA performance on the ScanObjectNN, ModelNet40, and ShapeNetPart datasets.\"}, {'Abstract': 'Abstract:While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the \"reversal curse\". It is a typical example that the model knows \"A\\'s father is B\", but is unable to reason \"B\\'s child is A\". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models\\' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may disrupt complete phrases or entities, thereby posing challenges for the model to comprehend and learn from training data. To address this issue, we propose Semantic-aware Permutation Training (SPT), which addresses this issue by segmenting the training sentences into semantic units (i.e., entities or phrases) with an assistant language model and permuting these units before feeding into the model. Extensive experiments demonstrate that SPT effectively mitigates the reversal curse since the performance on reversed questions approximates that on the forward ones, and significantly advances the performance of existing works.'}, {'Abstract': 'Abstract:Low-latency video streaming over 5G has become rapidly popular over the last few years due to its increased usage in hosting virtual events, online education, webinars, and all-hands meetings. Our work aims to address the absence of studies that reveal the real-world behavior of low-latency video streaming. To that end, we provide an experimental methodology and measurements, collected in a US metropolitan area over a commercial 5G network, that correlates application-level QoE and lower-layer metrics on the devices, such as RSRP, RSRQ, handover records, etc., under both static and mobility scenarios. We find that RAN-side information, which is readily available on every cellular device, has the potential to enhance throughput estimation modules of video streaming clients, ultimately making low-latency streaming more resilient against network perturbations and handover events.'}, {'Abstract': 'Abstract:Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.'}, {'Abstract': 'Abstract:The solution of a sparse system of linear equations is ubiquitous in scientific applications. Iterative methods, such as the Preconditioned Conjugate Gradient method (PCG), are normally chosen over direct methods due to memory and computational complexity constraints. However, the efficiency of these methods depends on the preconditioner utilized. The development of the preconditioner normally requires some insight into the sparse linear system and the desired trade-off of generating the preconditioner and the reduction in the number of iterations. Incomplete factorization methods tend to be black box methods to generate these preconditioners but may fail for a number of reasons. These reasons include numerical issues that require searching for adequate scaling, shifting, and fill-in while utilizing a difficult to parallelize algorithm. With a move towards heterogeneous computing, many sparse applications find GPUs that are optimized for dense tensor applications like training neural networks being underutilized. In this work, we demonstrate that a simple artificial neural network trained either at compile time or in parallel to the running application on a GPU can provide an incomplete sparse Cholesky factorization that can be used as a preconditioner. This generated preconditioner is as good or better in terms of reduction of iterations than the one found using multiple preconditioning techniques such as scaling and shifting. Moreover, the generated method also works and never fails to produce a preconditioner that does not reduce the iteration count.'}, {'Abstract': \"Abstract:Hundreds of millions of people now interact with language models, with uses ranging from serving as a writing aid to informing hiring decisions. Yet these language models are known to perpetuate systematic racial prejudices, making their judgments biased in problematic ways about groups like African Americans. While prior research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice: we extend research showing that Americans hold raciolinguistic stereotypes about speakers of African American English and find that language models have the same prejudice, exhibiting covert stereotypes that are more negative than any human stereotypes about African Americans ever experimentally recorded, although closest to the ones from before the civil rights movement. By contrast, the language models' overt stereotypes about African Americans are much more positive. We demonstrate that dialect prejudice has the potential for harmful consequences by asking language models to make hypothetical decisions about people, based only on how they speak. Language models are more likely to suggest that speakers of African American English be assigned less prestigious jobs, be convicted of crimes, and be sentenced to death. Finally, we show that existing methods for alleviating racial bias in language models such as human feedback training do not mitigate the dialect prejudice, but can exacerbate the discrepancy between covert and overt stereotypes, by teaching language models to superficially conceal the racism that they maintain on a deeper level. Our findings have far-reaching implications for the fair and safe employment of language technology.\"}, {'Abstract': \"Abstract:Satisfiability solving has been used to tackle a range of long-standing open math problems in recent years. We add another success by solving a geometry problem that originated a century ago. In the 1930s, Esther Klein's exploration of unavoidable shapes in planar point sets in general position showed that every set of five points includes four points in convex position. For a long time, it was open if an empty hexagon, i.e., six points in convex position without a point inside, can be avoided. In 2006, Gerken and Nicolás independently proved that the answer is no. We establish the exact bound: Every 30-point set in the plane in general position contains an empty hexagon. Our key contributions include an effective, compact encoding and a search-space partitioning strategy enabling linear-time speedups even when using thousands of cores.\"}, {'Abstract': \"Abstract:In this work, for the first time, we demonstrate that Mamba-based point cloud methods can outperform point-based methods. Mamba exhibits strong global modeling capabilities and linear computational complexity, making it highly attractive for point cloud analysis. To enable more effective processing of 3-D point cloud data by Mamba, we propose a novel Consistent Traverse Serialization to convert point clouds into 1-D point sequences while ensuring that neighboring points in the sequence are also spatially adjacent. Consistent Traverse Serialization yields six variants by permuting the order of x, y, and z coordinates, and the synergistic use of these variants aids Mamba in comprehensively observing point cloud data. Furthermore, to assist Mamba in handling point sequences with different orders more effectively, we introduce point prompts to inform Mamba of the sequence's arrangement rules. Finally, we propose positional encoding based on spatial coordinate mapping to inject positional information into point cloud sequences better. Based on these improvements, we construct a point cloud network named Point Cloud Mamba, which combines local and global modeling. Point Cloud Mamba surpasses the SOTA point-based method PointNeXt and achieves new SOTA performance on the ScanObjectNN, ModelNet40, and ShapeNetPart datasets.\"}, {'Abstract': 'Abstract:While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the \"reversal curse\". It is a typical example that the model knows \"A\\'s father is B\", but is unable to reason \"B\\'s child is A\". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models\\' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may disrupt complete phrases or entities, thereby posing challenges for the model to comprehend and learn from training data. To address this issue, we propose Semantic-aware Permutation Training (SPT), which addresses this issue by segmenting the training sentences into semantic units (i.e., entities or phrases) with an assistant language model and permuting these units before feeding into the model. Extensive experiments demonstrate that SPT effectively mitigates the reversal curse since the performance on reversed questions approximates that on the forward ones, and significantly advances the performance of existing works.'}, {'Abstract': 'Abstract:Low-latency video streaming over 5G has become rapidly popular over the last few years due to its increased usage in hosting virtual events, online education, webinars, and all-hands meetings. Our work aims to address the absence of studies that reveal the real-world behavior of low-latency video streaming. To that end, we provide an experimental methodology and measurements, collected in a US metropolitan area over a commercial 5G network, that correlates application-level QoE and lower-layer metrics on the devices, such as RSRP, RSRQ, handover records, etc., under both static and mobility scenarios. We find that RAN-side information, which is readily available on every cellular device, has the potential to enhance throughput estimation modules of video streaming clients, ultimately making low-latency streaming more resilient against network perturbations and handover events.'}, {'Abstract': 'Abstract:Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.'}, {'Abstract': 'Abstract:The solution of a sparse system of linear equations is ubiquitous in scientific applications. Iterative methods, such as the Preconditioned Conjugate Gradient method (PCG), are normally chosen over direct methods due to memory and computational complexity constraints. However, the efficiency of these methods depends on the preconditioner utilized. The development of the preconditioner normally requires some insight into the sparse linear system and the desired trade-off of generating the preconditioner and the reduction in the number of iterations. Incomplete factorization methods tend to be black box methods to generate these preconditioners but may fail for a number of reasons. These reasons include numerical issues that require searching for adequate scaling, shifting, and fill-in while utilizing a difficult to parallelize algorithm. With a move towards heterogeneous computing, many sparse applications find GPUs that are optimized for dense tensor applications like training neural networks being underutilized. In this work, we demonstrate that a simple artificial neural network trained either at compile time or in parallel to the running application on a GPU can provide an incomplete sparse Cholesky factorization that can be used as a preconditioner. This generated preconditioner is as good or better in terms of reduction of iterations than the one found using multiple preconditioning techniques such as scaling and shifting. Moreover, the generated method also works and never fails to produce a preconditioner that does not reduce the iteration count.'}, {'Abstract': \"Abstract:Hundreds of millions of people now interact with language models, with uses ranging from serving as a writing aid to informing hiring decisions. Yet these language models are known to perpetuate systematic racial prejudices, making their judgments biased in problematic ways about groups like African Americans. While prior research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice: we extend research showing that Americans hold raciolinguistic stereotypes about speakers of African American English and find that language models have the same prejudice, exhibiting covert stereotypes that are more negative than any human stereotypes about African Americans ever experimentally recorded, although closest to the ones from before the civil rights movement. By contrast, the language models' overt stereotypes about African Americans are much more positive. We demonstrate that dialect prejudice has the potential for harmful consequences by asking language models to make hypothetical decisions about people, based only on how they speak. Language models are more likely to suggest that speakers of African American English be assigned less prestigious jobs, be convicted of crimes, and be sentenced to death. Finally, we show that existing methods for alleviating racial bias in language models such as human feedback training do not mitigate the dialect prejudice, but can exacerbate the discrepancy between covert and overt stereotypes, by teaching language models to superficially conceal the racism that they maintain on a deeper level. Our findings have far-reaching implications for the fair and safe employment of language technology.\"}, {'Abstract': \"Abstract:Satisfiability solving has been used to tackle a range of long-standing open math problems in recent years. We add another success by solving a geometry problem that originated a century ago. In the 1930s, Esther Klein's exploration of unavoidable shapes in planar point sets in general position showed that every set of five points includes four points in convex position. For a long time, it was open if an empty hexagon, i.e., six points in convex position without a point inside, can be avoided. In 2006, Gerken and Nicolás independently proved that the answer is no. We establish the exact bound: Every 30-point set in the plane in general position contains an empty hexagon. Our key contributions include an effective, compact encoding and a search-space partitioning strategy enabling linear-time speedups even when using thousands of cores.\"}, {'Abstract': \"Abstract:In this work, for the first time, we demonstrate that Mamba-based point cloud methods can outperform point-based methods. Mamba exhibits strong global modeling capabilities and linear computational complexity, making it highly attractive for point cloud analysis. To enable more effective processing of 3-D point cloud data by Mamba, we propose a novel Consistent Traverse Serialization to convert point clouds into 1-D point sequences while ensuring that neighboring points in the sequence are also spatially adjacent. Consistent Traverse Serialization yields six variants by permuting the order of x, y, and z coordinates, and the synergistic use of these variants aids Mamba in comprehensively observing point cloud data. Furthermore, to assist Mamba in handling point sequences with different orders more effectively, we introduce point prompts to inform Mamba of the sequence's arrangement rules. Finally, we propose positional encoding based on spatial coordinate mapping to inject positional information into point cloud sequences better. Based on these improvements, we construct a point cloud network named Point Cloud Mamba, which combines local and global modeling. Point Cloud Mamba surpasses the SOTA point-based method PointNeXt and achieves new SOTA performance on the ScanObjectNN, ModelNet40, and ShapeNetPart datasets.\"}, {'Abstract': 'Abstract:While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the \"reversal curse\". It is a typical example that the model knows \"A\\'s father is B\", but is unable to reason \"B\\'s child is A\". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models\\' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may disrupt complete phrases or entities, thereby posing challenges for the model to comprehend and learn from training data. To address this issue, we propose Semantic-aware Permutation Training (SPT), which addresses this issue by segmenting the training sentences into semantic units (i.e., entities or phrases) with an assistant language model and permuting these units before feeding into the model. Extensive experiments demonstrate that SPT effectively mitigates the reversal curse since the performance on reversed questions approximates that on the forward ones, and significantly advances the performance of existing works.'}, {'Abstract': 'Abstract:Low-latency video streaming over 5G has become rapidly popular over the last few years due to its increased usage in hosting virtual events, online education, webinars, and all-hands meetings. Our work aims to address the absence of studies that reveal the real-world behavior of low-latency video streaming. To that end, we provide an experimental methodology and measurements, collected in a US metropolitan area over a commercial 5G network, that correlates application-level QoE and lower-layer metrics on the devices, such as RSRP, RSRQ, handover records, etc., under both static and mobility scenarios. We find that RAN-side information, which is readily available on every cellular device, has the potential to enhance throughput estimation modules of video streaming clients, ultimately making low-latency streaming more resilient against network perturbations and handover events.'}, {'Abstract': 'Abstract:Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.'}, {'Abstract': 'Abstract:The solution of a sparse system of linear equations is ubiquitous in scientific applications. Iterative methods, such as the Preconditioned Conjugate Gradient method (PCG), are normally chosen over direct methods due to memory and computational complexity constraints. However, the efficiency of these methods depends on the preconditioner utilized. The development of the preconditioner normally requires some insight into the sparse linear system and the desired trade-off of generating the preconditioner and the reduction in the number of iterations. Incomplete factorization methods tend to be black box methods to generate these preconditioners but may fail for a number of reasons. These reasons include numerical issues that require searching for adequate scaling, shifting, and fill-in while utilizing a difficult to parallelize algorithm. With a move towards heterogeneous computing, many sparse applications find GPUs that are optimized for dense tensor applications like training neural networks being underutilized. In this work, we demonstrate that a simple artificial neural network trained either at compile time or in parallel to the running application on a GPU can provide an incomplete sparse Cholesky factorization that can be used as a preconditioner. This generated preconditioner is as good or better in terms of reduction of iterations than the one found using multiple preconditioning techniques such as scaling and shifting. Moreover, the generated method also works and never fails to produce a preconditioner that does not reduce the iteration count.'}, {'Abstract': \"Abstract:Hundreds of millions of people now interact with language models, with uses ranging from serving as a writing aid to informing hiring decisions. Yet these language models are known to perpetuate systematic racial prejudices, making their judgments biased in problematic ways about groups like African Americans. While prior research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice: we extend research showing that Americans hold raciolinguistic stereotypes about speakers of African American English and find that language models have the same prejudice, exhibiting covert stereotypes that are more negative than any human stereotypes about African Americans ever experimentally recorded, although closest to the ones from before the civil rights movement. By contrast, the language models' overt stereotypes about African Americans are much more positive. We demonstrate that dialect prejudice has the potential for harmful consequences by asking language models to make hypothetical decisions about people, based only on how they speak. Language models are more likely to suggest that speakers of African American English be assigned less prestigious jobs, be convicted of crimes, and be sentenced to death. Finally, we show that existing methods for alleviating racial bias in language models such as human feedback training do not mitigate the dialect prejudice, but can exacerbate the discrepancy between covert and overt stereotypes, by teaching language models to superficially conceal the racism that they maintain on a deeper level. Our findings have far-reaching implications for the fair and safe employment of language technology.\"}, {'Abstract': \"Abstract:Satisfiability solving has been used to tackle a range of long-standing open math problems in recent years. We add another success by solving a geometry problem that originated a century ago. In the 1930s, Esther Klein's exploration of unavoidable shapes in planar point sets in general position showed that every set of five points includes four points in convex position. For a long time, it was open if an empty hexagon, i.e., six points in convex position without a point inside, can be avoided. In 2006, Gerken and Nicolás independently proved that the answer is no. We establish the exact bound: Every 30-point set in the plane in general position contains an empty hexagon. Our key contributions include an effective, compact encoding and a search-space partitioning strategy enabling linear-time speedups even when using thousands of cores.\"}]\n"
     ]
    }
   ],
   "source": [
    "print(papers_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4b4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
