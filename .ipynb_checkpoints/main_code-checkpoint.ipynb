{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c3f90a9-5b52-4dd2-a4ed-cc129fa1a4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests  # send request\n",
    "from bs4 import BeautifulSoup  # parse web pages\n",
    "import pandas as pd  # csv\n",
    "from time import sleep  # wait\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca483445-7836-40b9-919b-ef31221fc491",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a request header (to prevent anti-scraping)\n",
    "headers = {\n",
    "    'authority': 'curlconverter.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'if-modified-since': 'Fri, 15 Jul 2022 21:44:42 GMT',\n",
    "    'if-none-match': 'W/\"62d1dfca-3a58\"',\n",
    "    'referer': 'https://curlconverter.com/',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Microsoft Edge\";v=\"102\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Linux\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'cross-site',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.30',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7633b5-ee38-433a-9118-3a2562a2e51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc297bf6-096f-4af0-9e46-ea8138d4ad51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43murl\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m page\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m soup\n",
      "\u001b[1;31mNameError\u001b[0m: name 'url' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "468a4def-3f9f-4b3f-ba84-41739d7912d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = 'https://arxiv.org/list/cs/pastweek?skip=0&show=100'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Assuming each entry is contained in a <dd> and <dt> pair\n",
    "dt_entries = soup.find_all('dt')\n",
    "dd_entries = soup.find_all('dd')\n",
    "\n",
    "# Open a CSV file to write the data\n",
    "with open('arxiv_data.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['arXiv ID', 'Title', 'Authors', 'Subjects', 'Abstract'])\n",
    "    \n",
    "    # Loop through each pair of <dt> and <dd> to extract information\n",
    "    for dt, dd in zip(dt_entries, dd_entries):\n",
    "        arxiv_id = dt.find('span', class_='list-identifier').text.strip()\n",
    "        title = dd.find('div', class_='list-title mathjax').text.strip()\n",
    "        authors = ', '.join([author.text for author in dd.find_all('a')])\n",
    "        subjects = dd.find('span', class_='primary-subject').text.strip()\n",
    "        # Replace 'abstract_placeholder' with the actual abstract extraction code\n",
    "        abstract_placeholder = 'Abstract not shown in the image'\n",
    "        \n",
    "        # Write the extracted information to the CSV file\n",
    "        writer.writerow([arxiv_id, title, authors, subjects, abstract_placeholder])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "407e4af7-bcc6-4812-baf4-d5d59e9a717a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Read the data from the original CSV file\n",
    "with open('arxiv_data.csv', 'r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    data = list(reader)\n",
    "\n",
    "# Modify the 'arXiv ID' and 'Title' fields in the data\n",
    "for i, row in enumerate(data):\n",
    "    if i == 0:  # skip the header\n",
    "        continue\n",
    "    # Remove 'arXiv:' from the arXiv ID and '[pdf, other]' from the end\n",
    "    row[0] = row[0].replace('arXiv:', '').strip()\n",
    "    row[0] = row[0].split('[')[0].strip()\n",
    "    # Remove 'Title:' from the beginning of the Title\n",
    "    row[1] = row[1].replace('Title:', '').strip()\n",
    "\n",
    "# Write the modified data back to a new CSV file\n",
    "with open('modified_arxiv_data.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14edcadf-bbb8-41cf-8eb9-3f7f6b46e6f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Function to get the abstract text given an arXiv ID\n",
    "def get_abstract(arxiv_id):\n",
    "    # Construct the URL for the paper's abstract page\n",
    "    url = f'https://arxiv.org/abs/{arxiv_id}'\n",
    "    # Fetch the page content\n",
    "    response = requests.get(url)\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Extract the abstract text\n",
    "    abstract = soup.find('blockquote', class_='abstract').text.strip()\n",
    "    # Remove \"Abstract:\" from the beginning of the abstract text\n",
    "    abstract = abstract.replace('Abstract:', '').strip()\n",
    "    return abstract\n",
    "\n",
    "# Read the data from the CSV file\n",
    "with open('modified_arxiv_data.csv', 'r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    data = [row for row in reader]\n",
    "\n",
    "# Update the data with abstracts\n",
    "for row in data:\n",
    "    try:\n",
    "        row['Abstract'] = get_abstract(row['arXiv ID'])\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get abstract for arXiv ID {row['arXiv ID']}: {e}\")\n",
    "\n",
    "# Write the updated data back to the CSV file\n",
    "with open('updated_arxiv_data.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    fieldnames = ['arXiv ID', 'Title', 'Authors', 'Subjects', 'Abstract']\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c8cf23-80c5-49e0-bd1f-50422fdd3f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pachong",
   "language": "python",
   "name": "pachong"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
