{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7925ca46",
   "metadata": {},
   "source": [
    "# STA326 Exercise 2\n",
    "Touching the first 100 papers under CS topic of ArXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8c5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests  # send request\n",
    "from bs4 import BeautifulSoup  # parse web pages\n",
    "import pandas as pd  # csv\n",
    "from time import sleep  # wait\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070d43c",
   "metadata": {},
   "source": [
    "### Web Scrape and Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecdb379",
   "metadata": {},
   "source": [
    "Firstly, obtain the arxiv numbers of the top 100 papers in the CS field. The URL of the article is https://arxiv.org/abs/{arxiv_number}. And then use these URL to obtain the title, subjects, authors, and abstract of these 100 papers respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7b237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://arxiv.org/list/cs/pastweek?skip=0&show=100'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e531ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv = soup.find_all('span', {'class': 'list-identifier'})\n",
    "arxiv_list = [x.text[6:16] for x in arxiv]\n",
    "# print(arxiv_list)\n",
    "\n",
    "url_list = [f\"https://arxiv.org/abs/{arxiv_list[i]}\" for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e55acd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "subjects_list = []\n",
    "authors_list = []\n",
    "abstract_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    url = url_list[i]\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text,'html.parser')\n",
    "    \n",
    "    title_list.append(soup.find('h1', {'class':'title mathjax'}).text[6:])\n",
    "    subjects_list.append(soup.find('td', {'class':'tablecell subjects'}).text.replace('\\n',''))\n",
    "    authors_list.append(soup.find('div', {'class':'authors'}).text.replace('Authors:',''))\n",
    "    abstract_list.append(soup.find('blockquote', {'class':'abstract mathjax'}).text.replace('\\n','').replace('Abstract:','').rstrip())\n",
    "    \n",
    "    seconds = random.randint(3, 6)\n",
    "    sleep(seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac0340a",
   "metadata": {},
   "source": [
    "### Collecting into a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70482819",
   "metadata": {},
   "source": [
    "Create a dataframe `paper_df` and add the data from the lists above to it. Then store the DataFrame in `'./Papers.csv'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3158d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_df = pd.DataFrame({'title' : title_list, \n",
    "                        'subjects,' : subjects_list,\n",
    "                        'authors' : authors_list,\n",
    "                        'abstract' : abstract_list})\n",
    "\n",
    "paper_df.to_csv('./Papers.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
