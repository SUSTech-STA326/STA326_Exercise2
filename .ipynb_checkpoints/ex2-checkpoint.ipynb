{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d09229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d625a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:Point Could Mamba: Point Cloud Learning via State Space Model\n",
      "Authors: ['Tao Zhang', 'Xiangtai Li', 'Haobo Yuan', 'Shunping Ji', 'Shuicheng Yan']\n",
      "Subjects: Computer Vision and Pattern Recognition (cs.CV)\n",
      "Abstract:In this work, for the first time, we demonstrate that Mamba-based point cloud methods can outperform point-based methods. Mamba exhibits strong global modeling capabilities and linear computational complexity, making it highly attractive for point cloud analysis. To enable more effective processing of 3-D point cloud data by Mamba, we propose a novel Consistent Traverse Serialization to convert point clouds into 1-D point sequences while ensuring that neighboring points in the sequence are also spatially adjacent. Consistent Traverse Serialization yields six variants by permuting the order of x, y, and z coordinates, and the synergistic use of these variants aids Mamba in comprehensively observing point cloud data. Furthermore, to assist Mamba in handling point sequences with different orders more effectively, we introduce point prompts to inform Mamba of the sequence's arrangement rules. Finally, we propose positional encoding based on spatial coordinate mapping to inject positional information into point cloud sequences better. Based on these improvements, we construct a point cloud network named Point Cloud Mamba, which combines local and global modeling. Point Cloud Mamba surpasses the SOTA point-based method PointNeXt and achieves new SOTA performance on the ScanObjectNN, ModelNet40, and ShapeNetPart datasets.\n"
     ]
    }
   ],
   "source": [
    "def scrape_arxiv_paper_details(paper_id):\n",
    "    paper_link = f\"https://arxiv.org/abs/{paper_id}\"\n",
    "    response = requests.get(paper_link)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extracting details\n",
    "    title = soup.find('h1', class_='title mathjax').text.strip() if soup.find('h1', class_='title mathjax') else \"N/A\"\n",
    "    authors = [author.text.strip() for author in soup.find_all('div', class_='authors')[0].find_all('a')] if soup.find_all('div', class_='authors') else [\"N/A\"]\n",
    "    subjects_tag = soup.find('span', class_='primary-subject') if soup.find('span', class_='primary-subject') else None\n",
    "    subjects = subjects_tag.text.strip() if subjects_tag else \"N/A\"\n",
    "    abstract = soup.find('blockquote', class_='abstract mathjax').text.strip() if soup.find('blockquote', class_='abstract mathjax') else \"N/A\"\n",
    "\n",
    "    # Print the details\n",
    "    print(title)\n",
    "    print(\"Authors:\", authors)\n",
    "    print(\"Subjects:\", subjects)\n",
    "    print(abstract)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    paper_id = \"2403.00762\"\n",
    "    scrape_arxiv_paper_details(paper_id)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1eb385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page: https://arxiv.org/list/cs/recent?show=25&skip=0\n",
      "Fetching page: https://arxiv.org/list/cs/recent?show=25&skip=25\n",
      "Fetching page: https://arxiv.org/list/cs/recent?show=25&skip=50\n",
      "Fetching page: https://arxiv.org/list/cs/recent?show=25&skip=75\n",
      "['2403.00762', '2403.00758', '2403.00752', '2403.00745', '2403.00743', '2403.00742', '2403.00737', '2403.00729', '2403.00725', '2403.00724', '2403.00720', '2403.00717', '2403.00715', '2403.00712', '2403.00704', '2403.00696', '2403.00691', '2403.00690', '2403.00689', '2403.00686', '2403.00685', '2403.00682', '2403.00680', '2403.00675', '2403.00674', '2403.00673', '2403.00669', '2403.00668', '2403.00665', '2403.00663', '2403.00662', '2403.00646', '2403.00645', '2403.00644', '2403.00643', '2403.00642', '2403.00641', '2403.00633', '2403.00632', '2403.00631', '2403.00628', '2403.00625', '2403.00623', '2403.00622', '2403.00621', '2403.00613', '2403.00611', '2403.00607', '2403.00606', '2403.00598', '2403.00594', '2403.00592', '2403.00591', '2403.00590', '2403.00587', '2403.00586', '2403.00585', '2403.00584', '2403.00582', '2403.00579', '2403.00578', '2403.00574', '2403.00573', '2403.00571', '2403.00570', '2403.00567', '2403.00566', '2403.00565', '2403.00564', '2403.00563', '2403.00561', '2403.00558', '2403.00556', '2403.00554', '2403.00553', '2403.00550', '2403.00546', '2403.00543', '2403.00542', '2403.00540', '2403.00539', '2403.00536', '2403.00529', '2403.00528', '2403.00527', '2403.00526', '2403.00522', '2403.00520', '2403.00517', '2403.00515', '2403.00514', '2403.00510', '2403.00509', '2403.00506', '2403.00504', '2403.00499', '2403.00497', '2403.00491', '2403.00489', '2403.00486']\n"
     ]
    }
   ],
   "source": [
    "def get_arxiv_paper_ids():\n",
    "    base_url = \"https://arxiv.org/list/cs/recent?show={}&skip={}\"\n",
    "    paper_ids = []\n",
    "\n",
    "    # Iterate through the pages to collect paper IDs\n",
    "    for page_number in range(0, 100, 25):\n",
    "        url = base_url.format(25, page_number)\n",
    "        print(\"Fetching page:\", url)  # Debugging statement\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the list of papers\n",
    "        papers_list = soup.find('div', {'id': 'dlpage'})\n",
    "\n",
    "        # Loop through the list of papers and extract their IDs\n",
    "        for dt_tag in papers_list.find_all('dt'):\n",
    "            paper_link = dt_tag.find('a', {'title': 'Abstract'}).get('href')\n",
    "            paper_id = paper_link.split('/')[-1]\n",
    "            paper_ids.append(paper_id)\n",
    "\n",
    "            if len(paper_ids) == 100:\n",
    "                return paper_ids  # Return if 100 paper IDs are collected\n",
    "\n",
    "    return paper_ids\n",
    "\n",
    "def main():\n",
    "    paper_ids = get_arxiv_paper_ids()\n",
    "    #print(\"First 100 Paper IDs:\")\n",
    "    print(paper_ids)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed2f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids = ['2403.00762', '2403.00758', '2403.00752', '2403.00745', '2403.00743', '2403.00742', '2403.00737', '2403.00729', '2403.00725', '2403.00724', '2403.00720', '2403.00717', '2403.00715', '2403.00712', '2403.00704', '2403.00696', '2403.00691', '2403.00690', '2403.00689', '2403.00686', '2403.00685', '2403.00682', '2403.00680', '2403.00675', '2403.00674', '2403.00673', '2403.00669', '2403.00668', '2403.00665', '2403.00663', '2403.00662', '2403.00646', '2403.00645', '2403.00644', '2403.00643', '2403.00642', '2403.00641', '2403.00633', '2403.00632', '2403.00631', '2403.00628', '2403.00625', '2403.00623', '2403.00622', '2403.00621', '2403.00613', '2403.00611', '2403.00607', '2403.00606', '2403.00598', '2403.00594', '2403.00592', '2403.00591', '2403.00590', '2403.00587', '2403.00586', '2403.00585', '2403.00584', '2403.00582', '2403.00579', '2403.00578', '2403.00574', '2403.00573', '2403.00571', '2403.00570', '2403.00567', '2403.00566', '2403.00565', '2403.00564', '2403.00563', '2403.00561', '2403.00558', '2403.00556', '2403.00554', '2403.00553', '2403.00550', '2403.00546', '2403.00543', '2403.00542', '2403.00540', '2403.00539', '2403.00536', '2403.00529', '2403.00528', '2403.00527', '2403.00526', '2403.00522', '2403.00520', '2403.00517', '2403.00515', '2403.00514', '2403.00510', '2403.00509', '2403.00506', '2403.00504', '2403.00499', '2403.00497', '2403.00491', '2403.00489', '2403.00486']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de233ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping paper: https://arxiv.org/abs/2403.00762\n",
      "response: <Response [200]>\n",
      "Scraping paper: https://arxiv.org/abs/2403.00758\n",
      "response: <Response [200]>\n",
      "Scraping paper: https://arxiv.org/abs/2403.00752\n",
      "response: <Response [200]>\n",
      "Scraping paper: https://arxiv.org/abs/2403.00745\n",
      "response: <Response [200]>\n",
      "Scraping paper: https://arxiv.org/abs/2403.00743\n",
      "response: <Response [200]>\n",
      "Scraping paper: https://arxiv.org/abs/2403.00742\n",
      "response: <Response [200]>\n",
      "Scraping paper: https://arxiv.org/abs/2403.00737\n"
     ]
    }
   ],
   "source": [
    "def scrape_arxiv_paper_details(paper_id):\n",
    "    paper_link = f\"https://arxiv.org/abs/{paper_id}\"\n",
    "    print(\"Scraping paper:\", paper_link)  # Debugging statement\n",
    "    response = requests.get(paper_link)\n",
    "    print(\"response:\", response)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extracting details\n",
    "    title = soup.find('h1', class_='title mathjax').text.strip() if soup.find('h1', class_='title mathjax') else \"N/A\"\n",
    "    authors = [author.text.strip() for author in soup.find_all('div', class_='authors')[0].find_all('a')] if soup.find_all('div', class_='authors') else [\"N/A\"]\n",
    "    subjects_tag = soup.find('span', class_='primary-subject') if soup.find('span', class_='primary-subject') else None\n",
    "    subjects = subjects_tag.text.strip() if subjects_tag else \"N/A\"\n",
    "    abstract = soup.find('blockquote', class_='abstract mathjax').text.strip() if soup.find('blockquote', class_='abstract mathjax') else \"N/A\"\n",
    "\n",
    "    return {'Title': title, 'Authors': authors, 'Subjects': subjects, 'Abstract': abstract}\n",
    "\n",
    "def save_to_csv(papers):\n",
    "    with open('arxiv_cs_papers.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Title', 'Authors', 'Subjects', 'Abstract']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for paper_id in papers:\n",
    "            paper_details = scrape_arxiv_paper_details(paper_id)\n",
    "            writer.writerow(paper_details)\n",
    "\n",
    "def main():\n",
    "    #paper_ids = get_arxiv_paper_ids()\n",
    "    #print(\"Paper IDs collected:\", paper_ids)  # Debugging statement\n",
    "    save_to_csv(paper_ids)\n",
    "    print(\"Scraping and saving completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612da456",
   "metadata": {},
   "outputs": [],
   "source": [
    "不要连续的请求 修改head 增加一些slip （过几秒再爬）\n",
    "设置一个超时的机制，跳过该个爬取下一个"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
