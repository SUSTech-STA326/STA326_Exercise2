{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests  # send request\n",
    "from bs4 import BeautifulSoup  # parse web pages\n",
    "import pandas as pd  # csv\n",
    "from time import sleep  # wait\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a request header (to prevent anti-scraping)\n",
    "headers = {\n",
    "    'authority': 'curlconverter.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'if-modified-since': 'Fri, 15 Jul 2022 21:44:42 GMT',\n",
    "    'if-none-match': 'W/\"62d1dfca-3a58\"',\n",
    "    'referer': 'https://curlconverter.com/',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Microsoft Edge\";v=\"102\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Linux\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'cross-site',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.30',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url  = \"https://arxiv.org/list/cs/pastweek?skip=0&show=100\"\n",
    "page = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_lst   = []\n",
    "subject_lst  = []\n",
    "title_lst    = []\n",
    "abstract_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of author_lst is: 100\n",
      "author_lst[0]: Tao Zhang, Xiangtai Li, Haobo Yuan, Shunping Ji, Shuicheng Yan\n"
     ]
    }
   ],
   "source": [
    "for element in soup.find_all('div', {'class': 'list-authors'}):\n",
    "    author = []\n",
    "    for sub_element in element.find_all('a'):\n",
    "        author.append(sub_element.get_text())\n",
    "    author_lst.append(', '.join(author))\n",
    "print(f'Length of author_lst is: {len(author_lst)}')\n",
    "print(f'author_lst[0]: {author_lst[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of subject_lst is: 100\n",
      "subject_lst[0]: Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)\n"
     ]
    }
   ],
   "source": [
    "for element in soup.find_all('div', {'class': 'list-subjects'}):\n",
    "    subject_lst.append(element.get_text()[11:].strip('\\n'))\n",
    "print(f'Length of subject_lst is: {len(subject_lst)}')\n",
    "print(f'subject_lst[0]: {subject_lst[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of title_lst is: 100\n",
      "title_lst[-1]: Selective-Stereo: Adaptive Frequency Information Selection for Stereo  Matching\n"
     ]
    }
   ],
   "source": [
    "for element in soup.find_all('div', {'class': 'list-title mathjax'}):\n",
    "    title_lst.append(element.get_text()[7:].strip())\n",
    "print(f'Length of title_lst is: {len(title_lst)}')\n",
    "print(f'title_lst[-1]: {title_lst[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:24<00:00,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of abstract_lst is: 100\n",
      "abstract_lst[0]: In this work, for the first time, we demonstrate that Mamba-based point cloud methods can outperform point-based methods. Mamba exhibits strong global modeling capabilities and linear computational complexity, making it highly attractive for point cloud analysis. To enable more effective processing of 3-D point cloud data by Mamba, we propose a novel Consistent Traverse Serialization to convert point clouds into 1-D point sequences while ensuring that neighboring points in the sequence are also spatially adjacent. Consistent Traverse Serialization yields six variants by permuting the order of x, y, and z coordinates, and the synergistic use of these variants aids Mamba in comprehensively observing point cloud data. Furthermore, to assist Mamba in handling point sequences with different orders more effectively, we introduce point prompts to inform Mamba of the sequence's arrangement rules. Finally, we propose positional encoding based on spatial coordinate mapping to inject positional information into point cloud sequences better. Based on these improvements, we construct a point cloud network named Point Cloud Mamba, which combines local and global modeling. Point Cloud Mamba surpasses the SOTA point-based method PointNeXt and achieves new SOTA performance on the ScanObjectNN, ModelNet40, and ShapeNetPart datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for element in tqdm(soup.find_all('span', {'class': 'list-identifier'})):\n",
    "    sub_element = element.find('a', {'title': 'Abstract'})\n",
    "    sub_url = \"https://arxiv.org\" + sub_element['href']\n",
    "    sub_page = requests.get(sub_url, headers=headers)\n",
    "    sub_soup = BeautifulSoup(sub_page.content, 'html.parser')\n",
    "    sleep(1)\n",
    "    ab_element = sub_soup.find('blockquote', {'class': 'abstract mathjax'})\n",
    "    abstract_lst.append(ab_element.get_text()[10:].strip().strip('\\n'))\n",
    "print(f'Length of abstract_lst is: {len(abstract_lst)}')\n",
    "print(f'abstract_lst[0]: {abstract_lst[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'title': title_lst, 'subjects': subject_lst, 'authors': author_lst, 'abstract': abstract_lst})\n",
    "df.to_csv('./arxiv_cs_top100.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of all data: 100\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of all data: {len(df)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
